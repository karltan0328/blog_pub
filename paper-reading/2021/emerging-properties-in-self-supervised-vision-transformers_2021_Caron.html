<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="https://img.032802.xyz/logo.webp">
  <link rel="icon" type="image/png" sizes="32x32" href="https://img.032802.xyz/logo.webp">
  <link rel="icon" type="image/png" sizes="16x16" href="https://img.032802.xyz/logo.webp">
  <link rel="mask-icon" href="https://img.032802.xyz/logo.webp" color="#222">
  <meta name="google-site-verification" content="4aWmB8Q57Phm14T7Z2Y6_LbdCwonYdcWwSWVn9VKoHY">
  <meta name="msvalidate.01" content="90E5A0CCE16329AE72C18C4332F541B0">
  <meta name="baidu-site-verification" content="codeva-7IL5gMIbni">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"blog.032802.xyz","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.1","exturl":true,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":true,"height":500},"bookmark":{"enable":true,"color":"#222","save":"manual"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting s">
<meta property="og:type" content="blog">
<meta property="og:title" content="【论文笔记】Emerging Properties in Self-Supervised Vision Transformers">
<meta property="og:url" content="https://blog.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron.html">
<meta property="og:site_name" content="Karl的博客">
<meta property="og:description" content="In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting s">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-01-20T09:03:13.000Z">
<meta property="article:modified_time" content="2025-01-25T08:40:05.000Z">
<meta property="article:author" content="Karl">
<meta property="article:tag" content="2021ICCV">
<meta property="article:tag" content="Attention">
<meta property="article:tag" content="ViT">
<meta property="article:tag" content="DINO">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://blog.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron.html">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://blog.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron.html","path":"paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron.html","title":"【论文笔记】Emerging Properties in Self-Supervised Vision Transformers"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>【论文笔记】Emerging Properties in Self-Supervised Vision Transformers | Karl的博客</title>
  



  <script data-pjax defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{&quot;token&quot;: &quot;eba0e9933f39438c90a3a5417bdc88f5&quot;}'></script>

  <script>
    (function(c,l,a,r,i,t,y){
        c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
        t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
        y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
    })(window, document, "clarity", "script", "q43mw72e69");
</script>


  <script async defer data-website-id="36e39f74-37bc-447c-ac21-0d8bc8e87bfc" src="https://umami.032802.xyz/script.js" data-host-url="https://umami.032802.xyz"></script>

<link rel="dns-prefetch" href="https://waline.032802.xyz">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Karl的博客" type="application/atom+xml">
<link rel="alternate" href="/rss2.xml" title="Karl的博客" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Karl的博客</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-comments"><a href="/comments/" rel="section"><i class="fa fa-comments fa-fw"></i>留言板</a></li><li class="menu-item menu-item-links"><a href="/links/" rel="section"><i class="fa fa-link fa-fw"></i>友链</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">50</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">8</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">64</span></a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-travellings"><span class="exturl" data-url="aHR0cHM6Ly93d3cudHJhdmVsbGluZ3MuY24vZ28uaHRtbA=="><i class="fa fa-train-subway fa-fw"></i>开往</span></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#emerging-properties-in-self-supervised-vision-transformers"><span class="nav-text">Emerging
Properties in Self-Supervised Vision Transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#abstract"><span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction"><span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#related-work"><span class="nav-text">2. Related Work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#approach"><span class="nav-text">3. Approach</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ssl-with-knowledge-distillation"><span class="nav-text">3.1. SSL with Knowledge
Distillation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#implementation-and-evaluation-protocols"><span class="nav-text">3.2. Implementation and
evaluation protocols</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#main-results"><span class="nav-text">4. Main Results</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#comparing-with-ssl-frameworks-on-imagenet"><span class="nav-text">4.1. Comparing with
SSL frameworks on ImageNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#properties-of-vit-trained-with-ssl"><span class="nav-text">4.2. Properties of ViT
trained with SSL</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#nearest-neighbor-retrieval-with-dino-vit"><span class="nav-text">4.2.1 Nearest neighbor
retrieval with DINO ViT</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#discovering-the-semantic-layout-of-scenes"><span class="nav-text">4.2.2 Discovering the
semantic layout of scenes</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#transfer-learning-on-downstream-tasks"><span class="nav-text">4.2.3 Transfer learning
on downstream tasks</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ablation-study-of-dino"><span class="nav-text">5. Ablation Study of DINO</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#importance-of-the-different-components"><span class="nav-text">5.1. Importance of the
Different Components</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#impact-of-the-choice-of-teacher-network"><span class="nav-text">5.2. Impact of the
choice of Teacher Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#avoiding-collapse"><span class="nav-text">5.3. Avoiding collapse</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#compute-requirements"><span class="nav-text">5.4. Compute requirements</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#training-with-small-batches"><span class="nav-text">5.5. Training with small
batches</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#appendix"><span class="nav-text">Appendix</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#a.-additional-results"><span class="nav-text">A. Additional Results</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#b.-methodology-comparison"><span class="nav-text">B. Methodology Comparison</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#c.-projection-head"><span class="nav-text">C. Projection Head</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#d.-additional-ablations"><span class="nav-text">D. Additional Ablations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#e.-multi-crop"><span class="nav-text">E. Multi-crop</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#f.-evaluation-protocols"><span class="nav-text">F. Evaluation Protocols</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#f.1-k-nn-classification"><span class="nav-text">F.1 \(k\)-NN classification</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#f.2-linear-classification"><span class="nav-text">F.2 Linear classification</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#g.-self-attention-visualizations"><span class="nav-text">G. Self-Attention
Visualizations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#h.-class-representation"><span class="nav-text">H. Class Representation</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Karl"
      src="https://img.032802.xyz/profile.webp">
  <p class="site-author-name" itemprop="name">Karl</p>
  <div class="site-description" itemprop="description">不积跬步无以至千里</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">50</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">64</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2thcmx0YW4wMzI4" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;karltan0328"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOmFkbWluQDAzMjgwMi54eXo=" title="E-Mail → mailto:admin@032802.xyz"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly91bWFtaS4wMzI4MDIueHl6L3NoYXJlL2Fab21QNGpkZzAyb1NDZFEvYmxvZy4wMzI4MDIueHl6" title="Umami → https:&#x2F;&#x2F;umami.032802.xyz&#x2F;share&#x2F;aZomP4jdg02oSCdQ&#x2F;blog.032802.xyz"><i class="fa fa-chart-column fa-fw"></i>Umami</span>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml" rel="noopener me"><i class="fa fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpoLWhhbnM="><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></span>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <span class="exturl" data-url="aHR0cHM6Ly9wb3J0YWwucnVucm9hZC5jbG91ZC8=" title="https:&#x2F;&#x2F;portal.runroad.cloud&#x2F;">乐子云</span>
            </li>
            <li class="links-of-blogroll-item">
              <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9kb2NzL2dldHRpbmctc3RhcnRlZC8=" title="https:&#x2F;&#x2F;theme-next.js.org&#x2F;docs&#x2F;getting-started&#x2F;">NexT Docs</span>
            </li>
            <li class="links-of-blogroll-item">
              <span class="exturl" data-url="aHR0cHM6Ly9wYXBlcmNvcGlsb3QuY29tLw==" title="https:&#x2F;&#x2F;papercopilot.com&#x2F;">Paper Copilot</span>
            </li>
            <li class="links-of-blogroll-item">
              <span class="exturl" data-url="aHR0cHM6Ly93d3cuZGVlcC1tbC5jb20v" title="https:&#x2F;&#x2F;www.deep-ml.com&#x2F;">Deep-ML</span>
            </li>
        </ul>
      </div>
    </div>
        <div class="pjax">
        <div class="sidebar-inner sidebar-post-related">
          <div class="animated">
              <div class="links-of-blogroll-title"><i class="fa fa-signs-post fa-fw"></i>
    相关文章
  </div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/paper-reading/2020/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale_2021_Dosovitskiy.html" rel="bookmark">
        <time class="popular-posts-time">2025-01-20</time>
        <br>
      【论文笔记】An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/paper-reading/2017/attention-is-all-you-need_2023_Vaswani.html" rel="bookmark">
        <time class="popular-posts-time">2025-01-20</time>
        <br>
      【论文笔记】Attention Is All You Need
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/paper-reading/2024/instance-adaptive-and-geometric-aware-keypoint-learning-for-category-level-6d-object-pose-estimation_2024_Lin.html" rel="bookmark">
        <time class="popular-posts-time">2025-02-19</time>
        <br>
      【论文笔记】Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/paper-reading/2024/unopose-unseen-object-pose-estimation-with-an-unposed-rgb-d-reference-image_2025_Liu.html" rel="bookmark">
        <time class="popular-posts-time">2025-05-19</time>
        <br>
      【论文笔记】UNOPose: Unseen Object Pose Estimation with  an Unposed RGB-D Reference Image
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/paper-reading/paper-reading-records.html" rel="bookmark">
        <time class="popular-posts-time">2025-02-22</time>
        <br>
      论文阅读记录
      </a>
    </li>
  </ul>

          </div>
        </div>
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://blog.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://img.032802.xyz/profile.webp">
      <meta itemprop="name" content="Karl">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Karl的博客">
      <meta itemprop="description" content="不积跬步无以至千里">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="【论文笔记】Emerging Properties in Self-Supervised Vision Transformers | Karl的博客">
      <meta itemprop="description" content="In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent $k$-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【论文笔记】Emerging Properties in Self-Supervised Vision Transformers
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
  
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-01-20 17:03:13" itemprop="dateCreated datePublished" datetime="2025-01-20T17:03:13+08:00">2025-01-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-25 16:40:05" itemprop="dateModified" datetime="2025-01-25T16:40:05+08:00">2025-01-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B8%87%E5%8D%B7%E4%B9%A6/" itemprop="url" rel="index"><span itemprop="name">读万卷书</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline：</span>
  
    <a title="waline" href="/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron.html#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>16k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>59 分钟</span>
    </span>
</div>

            <div class="post-description">In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent $k$-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1
id="emerging-properties-in-self-supervised-vision-transformers">Emerging
Properties in Self-Supervised Vision Transformers</h1>
<figure>
<img
src="https://img.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron/attn6.webp"
alt="Figure 1: Self-attention from a Vision Transformer with 8 \times 8 patches trained with no supervision. We look at the self-attention of the [CLS] token on the heads of the last layer. This token is not attached to any label nor supervision. These maps show that the model automatically learns class-specific features leading to unsupervised object segmentations." />
<figcaption aria-hidden="true">Figure 1: Self-attention from a Vision
Transformer with <span class="math inline">\(8 \times 8\)</span> patches
trained with no supervision. We look at the self-attention of the [CLS]
token on the heads of the last layer. This token is not attached to any
label nor supervision. These maps show that the model automatically
learns class-specific features leading to unsupervised object
segmentations.</figcaption>
</figure>
<h2 id="abstract">Abstract</h2>
<h2 id="introduction">1. Introduction</h2>
<h2 id="related-work">2. Related Work</h2>
<h2 id="approach">3. Approach</h2>
<blockquote>
<p>"Talk is cheap. Show me the code."</p>
<p>― Linus Torvalds</p>
</blockquote>
<p>DINO的<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ZhY2Vib29rcmVzZWFyY2gvZGlubw==">原仓库<i class="fa fa-external-link-alt"></i></span>给出了一个<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ZhY2Vib29rcmVzZWFyY2gvZGluby9ibG9iL21haW4vdmlzdWFsaXplX2F0dGVudGlvbi5weQ==">demo<i class="fa fa-external-link-alt"></i></span>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Copyright (c) Facebook, Inc. and its affiliates.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment"># you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment"># You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"># See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> colorsys</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> skimage.io</span><br><span class="line"><span class="keyword">from</span> skimage.measure <span class="keyword">import</span> find_contours</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.patches <span class="keyword">import</span> Polygon</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> pth_transforms</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">import</span> vision_transformer <span class="keyword">as</span> vits</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_mask</span>(<span class="params">image, mask, color, alpha=<span class="number">0.5</span></span>):</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        image[:, :, c] = image[:, :, c] * (<span class="number">1</span> - alpha * mask) + alpha * mask * color[c] * <span class="number">255</span></span><br><span class="line">    <span class="keyword">return</span> image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_colors</span>(<span class="params">N, bright=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Generate random colors.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    brightness = <span class="number">1.0</span> <span class="keyword">if</span> bright <span class="keyword">else</span> <span class="number">0.7</span></span><br><span class="line">    hsv = [(i / N, <span class="number">1</span>, brightness) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line">    colors = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> c: colorsys.hsv_to_rgb(*c), hsv))</span><br><span class="line">    random.shuffle(colors)</span><br><span class="line">    <span class="keyword">return</span> colors</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">display_instances</span>(<span class="params">image, mask, fname=<span class="string">&quot;test&quot;</span>, figsize=(<span class="params"><span class="number">5</span>, <span class="number">5</span></span>), blur=<span class="literal">False</span>, contour=<span class="literal">True</span>, alpha=<span class="number">0.5</span></span>):</span><br><span class="line">    fig = plt.figure(figsize=figsize, frameon=<span class="literal">False</span>)</span><br><span class="line">    ax = plt.Axes(fig, [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line">    ax.set_axis_off()</span><br><span class="line">    fig.add_axes(ax)</span><br><span class="line">    ax = plt.gca()</span><br><span class="line"></span><br><span class="line">    N = <span class="number">1</span></span><br><span class="line">    mask = mask[<span class="literal">None</span>, :, :]</span><br><span class="line">    <span class="comment"># Generate random colors</span></span><br><span class="line">    colors = random_colors(N)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Show area outside image boundaries.</span></span><br><span class="line">    height, width = image.shape[:<span class="number">2</span>]</span><br><span class="line">    margin = <span class="number">0</span></span><br><span class="line">    ax.set_ylim(height + margin, -margin)</span><br><span class="line">    ax.set_xlim(-margin, width + margin)</span><br><span class="line">    ax.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    masked_image = image.astype(np.uint32).copy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        color = colors[i]</span><br><span class="line">        _mask = mask[i]</span><br><span class="line">        <span class="keyword">if</span> blur:</span><br><span class="line">            _mask = cv2.blur(_mask,(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">        <span class="comment"># Mask</span></span><br><span class="line">        masked_image = apply_mask(masked_image, _mask, color, alpha)</span><br><span class="line">        <span class="comment"># Mask Polygon</span></span><br><span class="line">        <span class="comment"># Pad to ensure proper polygons for masks that touch image edges.</span></span><br><span class="line">        <span class="keyword">if</span> contour:</span><br><span class="line">            padded_mask = np.zeros((_mask.shape[<span class="number">0</span>] + <span class="number">2</span>, _mask.shape[<span class="number">1</span>] + <span class="number">2</span>))</span><br><span class="line">            padded_mask[<span class="number">1</span>:-<span class="number">1</span>, <span class="number">1</span>:-<span class="number">1</span>] = _mask</span><br><span class="line">            contours = find_contours(padded_mask, <span class="number">0.5</span>)</span><br><span class="line">            <span class="keyword">for</span> verts <span class="keyword">in</span> contours:</span><br><span class="line">                <span class="comment"># Subtract the padding and flip (y, x) to (x, y)</span></span><br><span class="line">                verts = np.fliplr(verts) - <span class="number">1</span></span><br><span class="line">                p = Polygon(verts, facecolor=<span class="string">&quot;none&quot;</span>, edgecolor=color)</span><br><span class="line">                ax.add_patch(p)</span><br><span class="line">    ax.imshow(masked_image.astype(np.uint8), aspect=<span class="string">&#x27;auto&#x27;</span>)</span><br><span class="line">    fig.savefig(fname)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;fname&#125;</span> saved.&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = argparse.ArgumentParser(<span class="string">&#x27;Visualize Self-Attention maps&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--arch&#x27;</span>, default=<span class="string">&#x27;vit_small&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">        choices=[<span class="string">&#x27;vit_tiny&#x27;</span>, <span class="string">&#x27;vit_small&#x27;</span>, <span class="string">&#x27;vit_base&#x27;</span>], <span class="built_in">help</span>=<span class="string">&#x27;Architecture (support only ViT atm).&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--patch_size&#x27;</span>, default=<span class="number">8</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, <span class="built_in">help</span>=<span class="string">&#x27;Patch resolution of the model.&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--pretrained_weights&#x27;</span>, default=<span class="string">&#x27;&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;Path to pretrained weights to load.&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--checkpoint_key&quot;</span>, default=<span class="string">&quot;teacher&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&#x27;Key to use in the checkpoint (example: &quot;teacher&quot;)&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--image_path&quot;</span>, default=<span class="literal">None</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, <span class="built_in">help</span>=<span class="string">&quot;Path of the image to load.&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--image_size&quot;</span>, default=(<span class="number">480</span>, <span class="number">480</span>), <span class="built_in">type</span>=<span class="built_in">int</span>, nargs=<span class="string">&quot;+&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;Resize image.&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--output_dir&#x27;</span>, default=<span class="string">&#x27;.&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;Path where to save visualizations.&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--threshold&quot;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="literal">None</span>, <span class="built_in">help</span>=<span class="string">&quot;&quot;&quot;We visualize masks</span></span><br><span class="line"><span class="string">        obtained by thresholding the self-attention maps to keep xx% of the mass.&quot;&quot;&quot;</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    <span class="comment"># build model</span></span><br><span class="line">    model = vits.__dict__[args.arch](patch_size=args.patch_size, num_classes=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        p.requires_grad = <span class="literal">False</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    model.to(device)</span><br><span class="line">    <span class="keyword">if</span> os.path.isfile(args.pretrained_weights):</span><br><span class="line">        state_dict = torch.load(args.pretrained_weights, map_location=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> args.checkpoint_key <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> args.checkpoint_key <span class="keyword">in</span> state_dict:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Take key <span class="subst">&#123;args.checkpoint_key&#125;</span> in provided checkpoint dict&quot;</span>)</span><br><span class="line">            state_dict = state_dict[args.checkpoint_key]</span><br><span class="line">        <span class="comment"># remove `module.` prefix</span></span><br><span class="line">        state_dict = &#123;k.replace(<span class="string">&quot;module.&quot;</span>, <span class="string">&quot;&quot;</span>): v <span class="keyword">for</span> k, v <span class="keyword">in</span> state_dict.items()&#125;</span><br><span class="line">        <span class="comment"># remove `backbone.` prefix induced by multicrop wrapper</span></span><br><span class="line">        state_dict = &#123;k.replace(<span class="string">&quot;backbone.&quot;</span>, <span class="string">&quot;&quot;</span>): v <span class="keyword">for</span> k, v <span class="keyword">in</span> state_dict.items()&#125;</span><br><span class="line">        msg = model.load_state_dict(state_dict, strict=<span class="literal">False</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Pretrained weights found at &#123;&#125; and loaded with msg: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(args.pretrained_weights, msg))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.&quot;</span>)</span><br><span class="line">        url = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> args.arch == <span class="string">&quot;vit_small&quot;</span> <span class="keyword">and</span> args.patch_size == <span class="number">16</span>:</span><br><span class="line">            url = <span class="string">&quot;dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth&quot;</span></span><br><span class="line">        <span class="keyword">elif</span> args.arch == <span class="string">&quot;vit_small&quot;</span> <span class="keyword">and</span> args.patch_size == <span class="number">8</span>:</span><br><span class="line">            url = <span class="string">&quot;dino_deitsmall8_300ep_pretrain/dino_deitsmall8_300ep_pretrain.pth&quot;</span>  <span class="comment"># model used for visualizations in our paper</span></span><br><span class="line">        <span class="keyword">elif</span> args.arch == <span class="string">&quot;vit_base&quot;</span> <span class="keyword">and</span> args.patch_size == <span class="number">16</span>:</span><br><span class="line">            url = <span class="string">&quot;dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth&quot;</span></span><br><span class="line">        <span class="keyword">elif</span> args.arch == <span class="string">&quot;vit_base&quot;</span> <span class="keyword">and</span> args.patch_size == <span class="number">8</span>:</span><br><span class="line">            url = <span class="string">&quot;dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth&quot;</span></span><br><span class="line">        <span class="keyword">if</span> url <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Since no pretrained weights have been provided, we load the reference pretrained DINO weights.&quot;</span>)</span><br><span class="line">            state_dict = torch.hub.load_state_dict_from_url(url=<span class="string">&quot;https://dl.fbaipublicfiles.com/dino/&quot;</span> + url)</span><br><span class="line">            model.load_state_dict(state_dict, strict=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;There is no reference weights available for this model =&gt; We use random weights.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># open image</span></span><br><span class="line">    <span class="keyword">if</span> args.image_path <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># user has not specified any image - we use our own image</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Please use the `--image_path` argument to indicate the path of the image you wish to visualize.&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Since no image path have been provided, we take the first image in our paper.&quot;</span>)</span><br><span class="line">        response = requests.get(<span class="string">&quot;https://dl.fbaipublicfiles.com/dino/img.png&quot;</span>)</span><br><span class="line">        img = Image.<span class="built_in">open</span>(BytesIO(response.content))</span><br><span class="line">        img = img.convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> os.path.isfile(args.image_path):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(args.image_path, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            img = Image.<span class="built_in">open</span>(f)</span><br><span class="line">            img = img.convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Provided image path <span class="subst">&#123;args.image_path&#125;</span> is non valid.&quot;</span>)</span><br><span class="line">        sys.exit(<span class="number">1</span>)</span><br><span class="line">    transform = pth_transforms.Compose([</span><br><span class="line">        pth_transforms.Resize(args.image_size),</span><br><span class="line">        pth_transforms.ToTensor(),</span><br><span class="line">        pth_transforms.Normalize((<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>), (<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>)),</span><br><span class="line">    ])</span><br><span class="line">    img = transform(img)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># make the image divisible by the patch size</span></span><br><span class="line">    w, h = img.shape[<span class="number">1</span>] - img.shape[<span class="number">1</span>] % args.patch_size, img.shape[<span class="number">2</span>] - img.shape[<span class="number">2</span>] % args.patch_size</span><br><span class="line">    img = img[:, :w, :h].unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    w_featmap = img.shape[-<span class="number">2</span>] // args.patch_size</span><br><span class="line">    h_featmap = img.shape[-<span class="number">1</span>] // args.patch_size</span><br><span class="line"></span><br><span class="line">    attentions = model.get_last_selfattention(img.to(device))</span><br><span class="line"></span><br><span class="line">    nh = attentions.shape[<span class="number">1</span>] <span class="comment"># number of head</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># we keep only the output patch attention</span></span><br><span class="line">    attentions = attentions[<span class="number">0</span>, :, <span class="number">0</span>, <span class="number">1</span>:].reshape(nh, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.threshold <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># we keep only a certain percentage of the mass</span></span><br><span class="line">        val, idx = torch.sort(attentions)</span><br><span class="line">        val /= torch.<span class="built_in">sum</span>(val, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        cumval = torch.cumsum(val, dim=<span class="number">1</span>)</span><br><span class="line">        th_attn = cumval &gt; (<span class="number">1</span> - args.threshold)</span><br><span class="line">        idx2 = torch.argsort(idx)</span><br><span class="line">        <span class="keyword">for</span> head <span class="keyword">in</span> <span class="built_in">range</span>(nh):</span><br><span class="line">            th_attn[head] = th_attn[head][idx2[head]]</span><br><span class="line">        th_attn = th_attn.reshape(nh, w_featmap, h_featmap).<span class="built_in">float</span>()</span><br><span class="line">        <span class="comment"># interpolate</span></span><br><span class="line">        th_attn = nn.functional.interpolate(th_attn.unsqueeze(<span class="number">0</span>), scale_factor=args.patch_size, mode=<span class="string">&quot;nearest&quot;</span>)[<span class="number">0</span>].cpu().numpy()</span><br><span class="line"></span><br><span class="line">    attentions = attentions.reshape(nh, w_featmap, h_featmap)</span><br><span class="line">    attentions = nn.functional.interpolate(attentions.unsqueeze(<span class="number">0</span>), scale_factor=args.patch_size, mode=<span class="string">&quot;nearest&quot;</span>)[<span class="number">0</span>].cpu().numpy()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># save attentions heatmaps</span></span><br><span class="line">    os.makedirs(args.output_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    torchvision.utils.save_image(torchvision.utils.make_grid(img, normalize=<span class="literal">True</span>, scale_each=<span class="literal">True</span>), os.path.join(args.output_dir, <span class="string">&quot;img.png&quot;</span>))</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(nh):</span><br><span class="line">        fname = os.path.join(args.output_dir, <span class="string">&quot;attn-head&quot;</span> + <span class="built_in">str</span>(j) + <span class="string">&quot;.png&quot;</span>)</span><br><span class="line">        plt.imsave(fname=fname, arr=attentions[j], <span class="built_in">format</span>=<span class="string">&#x27;png&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;fname&#125;</span> saved.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.threshold <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        image = skimage.io.imread(os.path.join(args.output_dir, <span class="string">&quot;img.png&quot;</span>))</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(nh):</span><br><span class="line">            display_instances(image, th_attn[j], fname=os.path.join(args.output_dir, <span class="string">&quot;mask_th&quot;</span> + <span class="built_in">str</span>(args.threshold) + <span class="string">&quot;_head&quot;</span> + <span class="built_in">str</span>(j) +<span class="string">&quot;.png&quot;</span>), blur=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>同时也给出了<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ZhY2Vib29rcmVzZWFyY2gvZGlubz90YWI9cmVhZG1lLW92LWZpbGUjc2VsZi1hdHRlbnRpb24tdmlzdWFsaXphdGlvbg==">这段代码的运行方式<i class="fa fa-external-link-alt"></i></span>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python visualize_attention.py</span><br></pre></td></tr></table></figure>
<p>我们不妨直接运行，其结果如下：</p>
<figure>
<img
src="https://img.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron/demo-res.webp"
alt="demo-res" />
<figcaption aria-hidden="true">demo-res</figcaption>
</figure>
<p>其中：</p>
<ul>
<li><code>demo</code>：表示输入图像；</li>
<li><code>img</code>：表示输入图像经过<code>torchvision.utils.make_grid(img, normalize=True, scale_each=True)</code>函数处理后的图像；</li>
<li><code>attn-head0 - attn-head5</code>：表示第0个注意力头到第5个注意力头的注意力图。</li>
</ul>
<p>接下来我们逐行分析<code>visualize_attention.py</code>的代码：</p>
<ul>
<li><p>1-35行：引入包；</p></li>
<li><p>38-41行：定义<code>apply_mask</code>函数，该函数用于将彩色遮罩叠加到原始图像上；</p></li>
<li><p>44-52行：定义<code>random_colors</code>函数，该函数用于生成随机颜色，使用HSV颜色空间来确保生成的颜色具有良好的视觉区分度；</p></li>
<li><p>55-95行：定义<code>display_instances</code>函数，该函数用于显示和保存带有遮罩的图像；</p></li>
<li><p>98-213行：<code>main</code>函数，接下来会详细解释<code>main</code>函数：</p>
<ul>
<li>99-148行：接收参数、加载模型；</li>
<li>151-170行：读取图像，并对图像进行预处理；</li>
<li>173-174行：make the image divisible by the patch size；</li>
<li>176-177行：获取注意力图的高和宽；</li>
<li><strong>179行：获取所有注意力头的注意力图；</strong></li>
<li>181行：获取注意力头的数量；</li>
<li>184行：在所有注意力头上获取类别标签[CLS]对图像所有patch的注意力向量；</li>
<li>186-197行：如果传入了<code>--threshold</code>参数，则只保留累积注意力值达到阈值的区域这里在运行demo时没有设置<code>--threshold</code>参数，所以没有执行这一段代码，<code>th_attn</code>为空；</li>
<li>199-200行：在所有注意力头上将类别标签[CLS]对图像所有patch的注意力向量reshape成2维图像，假设有<span
class="math inline">\(n\)</span>个注意力头，那么此时有<span
class="math inline">\(n\)</span>个2维图像，在reshape后，使用最近邻<code>nearest</code>插值将注意力图的分辨率恢复到输入图像的分辨率；</li>
<li>203-208行：保存所有注意力头的注意力图；</li>
<li>210-213行：保存186-197行处理后的注意力图。</li>
</ul></li>
</ul>
<p>显然，179行的<strong><code>attentions = model.get_last_selfattention(img.to(device))</code></strong>是整段代码的关键。</p>
<p><code>get_last_selfattention</code>函数的定义在<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ZhY2Vib29rcmVzZWFyY2gvZGluby9ibG9iL21haW4vdmlzaW9uX3RyYW5zZm9ybWVyLnB5I0wyMTY=">vision_transformer.py第216行<i class="fa fa-external-link-alt"></i></span>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VisionTransformer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Vision Transformer &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># ... existing code ...</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_last_selfattention</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.prepare_tokens(x)</span><br><span class="line">        <span class="comment"># ... existing code ...</span></span><br><span class="line">    <span class="comment"># ... existing code ...</span></span><br></pre></td></tr></table></figure>
<p><code>get_last_selfattention</code>函数在执行时，首先会调用<code>prepare_tokens</code>函数，<code>prepare_tokens</code>函数的定义在<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ZhY2Vib29rcmVzZWFyY2gvZGluby9ibG9iL21haW4vdmlzaW9uX3RyYW5zZm9ybWVyLnB5I0wxOTY=">vision_transformer.py第196行<i class="fa fa-external-link-alt"></i></span>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VisionTransformer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Vision Transformer &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># ... existing code ...</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">prepare_tokens</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, nc, w, h = x.shape</span><br><span class="line">        x = <span class="variable language_">self</span>.patch_embed(x)  <span class="comment"># patch linear embedding</span></span><br><span class="line">        <span class="comment"># ... existing code ...</span></span><br><span class="line">    <span class="comment"># ... existing code ...</span></span><br></pre></td></tr></table></figure>
<p><code>prepare_tokens</code>函数在执行时，会继续调用<code>patch_embed</code>，<code>patch_embed</code>是<code>PatchEmbed</code>类的实例，<code>PatchEmbed</code>类的定义在<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ZhY2Vib29rcmVzZWFyY2gvZGluby9ibG9iL21haW4vdmlzaW9uX3RyYW5zZm9ybWVyLnB5I0wxMTY=">vision_transformer.py第116行<i class="fa fa-external-link-alt"></i></span>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Image to Patch Embedding</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, embed_dim=<span class="number">768</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        num_patches = (img_size // patch_size) * (img_size // patch_size)</span><br><span class="line">        <span class="variable language_">self</span>.img_size = img_size</span><br><span class="line">        <span class="variable language_">self</span>.patch_size = patch_size</span><br><span class="line">        <span class="variable language_">self</span>.num_patches = num_patches</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        x = <span class="variable language_">self</span>.proj(x).flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p><code>PatchEmbed</code>类继承于<code>nn.Module</code>，所以先看<code>forward</code>函数：</p>
<ul>
<li><p><code>B, C, H, W = x.shape</code>是获取了图像的batch_size
B、通道数C、图像高度H和图像宽度W；</p></li>
<li><p><code>x = self.proj(x).flatten(2).transpose(1, 2)</code>等价于顺序执行下面三步：</p>
<ul>
<li><p><code>x = self.proj(x)</code>，<code>x = x.flatten(2)</code>，<code>x = x.transpose(1, 2)</code>；</p></li>
<li><p>其中：</p>
<ul>
<li><code>x = self.proj(x)</code>表示使用卷积核<code>nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</code>对输入图像进行了处理，<code>in_chans</code>为输入通道数，<code>embed_dim</code>为输出通道数，<code>kernel_size</code>为卷积核大小，<code>stride</code>为步长。这里的<code>embed_dim</code>就是<span
class="math inline">\(D\)</span>，<code>patch_size</code>就是<span
class="math inline">\(P\)</span>，且该卷积操作的卷积核大小和步长大小相同，所以将输入图片经过这个卷积操作后，每一个patch都会被映射成一个<span
class="math inline">\(D\)</span>维的向量，且patch和patch之间没有重合部分，那么最后输出的维度是<span
class="math inline">\(\mathbb{R}^{B \times D \times \frac{H}{P} \times
\frac{W}{P}}\)</span>；</li>
<li><code>x = x.flatten(2)</code>表示从<code>x</code>的第2维开始展平，保持维度0和1不变，将维度2和3合并，令<span
class="math inline">\(N = \frac{H}{P} \times
\frac{W}{P}\)</span>为patch的数量，那么最后输出的维度是<span
class="math inline">\(\mathbb{R}^{B \times D \times N}\)</span>；</li>
<li><code>x = x.transpose(1, 2)</code>表示将维度1和维度2交换，那么最后输出的维度是<span
class="math inline">\(\mathbb{R}^{B \times N \times
D}\)</span>，用卷积核的参数表示，输出的维度为<span
class="math inline">\(\mathbb{R}^{B \times
\frac{\text{img_size}^2}{\text{patch_size}^2} \times
\text{embed_dim}}\)</span>。</li>
</ul></li>
</ul></li>
</ul>
<p><code>patch_embed</code>类结束，回到<code>prepare_tokens</code>函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VisionTransformer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Vision Transformer &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=[<span class="number">224</span>], patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, num_classes=<span class="number">0</span>, embed_dim=<span class="number">768</span>, depth=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">                 num_heads=<span class="number">12</span>, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">False</span>, qk_scale=<span class="literal">None</span>, drop_rate=<span class="number">0.</span>, attn_drop_rate=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 drop_path_rate=<span class="number">0.</span>, norm_layer=nn.LayerNorm, **kwargs</span>):</span><br><span class="line">        <span class="comment"># ... existing code ...</span></span><br><span class="line">        <span class="variable language_">self</span>.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, embed_dim))</span><br><span class="line">        <span class="comment"># ... existing code ...</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_drop = nn.Dropout(p=drop_rate)</span><br><span class="line">        <span class="comment"># ... existing code ...</span></span><br><span class="line">        trunc_normal_(<span class="variable language_">self</span>.cls_token, std=<span class="number">.02</span>)</span><br><span class="line">        <span class="comment"># ... existing code ...</span></span><br><span class="line">    <span class="comment"># ... existing code ...</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">prepare_tokens</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, nc, w, h = x.shape</span><br><span class="line">        x = <span class="variable language_">self</span>.patch_embed(x)  <span class="comment"># patch linear embedding</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># add the [CLS] token to the embed patch tokens</span></span><br><span class="line">        cls_tokens = <span class="variable language_">self</span>.cls_token.expand(B, -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># add positional encoding to each token</span></span><br><span class="line">        x = x + <span class="variable language_">self</span>.interpolate_pos_encoding(x, w, h)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.pos_drop(x)</span><br><span class="line">    <span class="comment"># ... existing code ...</span></span><br></pre></td></tr></table></figure>
<p>在执行完<code>x = self.patch_embed(x)</code>后，<code>x</code>的维度为<span
class="math inline">\(\mathbb{R}^{B \times
\frac{\text{img_size}^2}{\text{patch_size}^2} \times
\text{embed_dim}}\)</span>，<code>self.cls_token</code>的维度为<span
class="math inline">\(\mathbb{R}^{1 \times 1 \times
\text{embed_dim}}\)</span>，所以在<code>expand(B, -1, -1)</code>之后，<code>cls_tokens</code>的维度为<span
class="math inline">\(\mathbb{R}^{B \times 1 \times
\text{embed_dim}}\)</span>，<code>x = torch.cat((cls_tokens, x), dim=1)</code>表示将<code>cls_tokens</code>和<code>x</code>在第1维上拼接，那么最后<code>x</code>的维度为<span
class="math inline">\(\mathbb{R}^{B \times
\left(\frac{\text{img_size}^2}{\text{patch_size}^2} + 1\right) \times
\text{embed_dim}}\)</span>。</p>
<p>reshape做完，线性映射完，[CLS]拼接完，接下来是位置编码，即<code>x = x + self.interpolate_pos_encoding(x, w, h)</code>，<code>interpolate_pos_encoding</code>函数的定义在<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ZhY2Vib29rcmVzZWFyY2gvZGluby9ibG9iL21haW4vdmlzaW9uX3RyYW5zZm9ybWVyLnB5I0wxNzQ=">vision_transformer.py第174行<i class="fa fa-external-link-alt"></i></span>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VisionTransformer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Vision Transformer &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># ... existing code ...</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=[<span class="number">224</span>], patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, num_classes=<span class="number">0</span>, embed_dim=<span class="number">768</span>, depth=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">                 num_heads=<span class="number">12</span>, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">False</span>, qk_scale=<span class="literal">None</span>, drop_rate=<span class="number">0.</span>, attn_drop_rate=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 drop_path_rate=<span class="number">0.</span>, norm_layer=nn.LayerNorm, **kwargs</span>):</span><br><span class="line">        <span class="comment"># ... existing code ...</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_embed = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches + <span class="number">1</span>, embed_dim))</span><br><span class="line">        <span class="comment"># ... existing code ...</span></span><br><span class="line">        trunc_normal_(<span class="variable language_">self</span>.pos_embed, std=<span class="number">.02</span>)</span><br><span class="line">        <span class="comment"># ... existing code ...</span></span><br><span class="line">    <span class="comment"># ... existing code ...</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">interpolate_pos_encoding</span>(<span class="params">self, x, w, h</span>):</span><br><span class="line">        npatch = x.shape[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        N = <span class="variable language_">self</span>.pos_embed.shape[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> npatch == N <span class="keyword">and</span> w == h:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.pos_embed</span><br><span class="line">        class_pos_embed = <span class="variable language_">self</span>.pos_embed[:, <span class="number">0</span>]</span><br><span class="line">        patch_pos_embed = <span class="variable language_">self</span>.pos_embed[:, <span class="number">1</span>:]</span><br><span class="line">        dim = x.shape[-<span class="number">1</span>]</span><br><span class="line">        w0 = w // <span class="variable language_">self</span>.patch_embed.patch_size</span><br><span class="line">        h0 = h // <span class="variable language_">self</span>.patch_embed.patch_size</span><br><span class="line">        <span class="comment"># we add a small number to avoid floating point error in the interpolation</span></span><br><span class="line">        <span class="comment"># see discussion at https://github.com/facebookresearch/dino/issues/8</span></span><br><span class="line">        w0, h0 = w0 + <span class="number">0.1</span>, h0 + <span class="number">0.1</span></span><br><span class="line">        patch_pos_embed = nn.functional.interpolate(</span><br><span class="line">            patch_pos_embed.reshape(<span class="number">1</span>, <span class="built_in">int</span>(math.sqrt(N)), <span class="built_in">int</span>(math.sqrt(N)), dim).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),</span><br><span class="line">            mode=<span class="string">&#x27;bicubic&#x27;</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">int</span>(w0) == patch_pos_embed.shape[-<span class="number">2</span>] <span class="keyword">and</span> <span class="built_in">int</span>(h0) == patch_pos_embed.shape[-<span class="number">1</span>]</span><br><span class="line">        patch_pos_embed = patch_pos_embed.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).view(<span class="number">1</span>, -<span class="number">1</span>, dim)</span><br><span class="line">        <span class="keyword">return</span> torch.cat((class_pos_embed.unsqueeze(<span class="number">0</span>), patch_pos_embed), dim=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># ... existing code ...</span></span><br></pre></td></tr></table></figure>
<p>简单来说，这个函数能够基于初始化生成的位置编码，用插值的方法为不同分辨率的输入图像生成对应的位置编码，以让模型能够处理不同尺寸的输入图像。</p>
<p>最后，返回<code>self.pos_drop(x)</code>，<code>pos_drop</code>函数就是一个简单的<code>nn.Dropout</code>，所以最后<code>x</code>的维度为<span
class="math inline">\(\mathbb{R}^{B \times
\left(\frac{\text{img_size}^2}{\text{patch_size}^2} + 1\right) \times
\text{embed_dim}}\)</span>。</p>
<p><code>prepare_tokens</code>函数结束，回到<code>get_last_selfattention</code>函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VisionTransformer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Vision Transformer &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># ... existing code ...</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_last_selfattention</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.prepare_tokens(x)</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.blocks):</span><br><span class="line">            <span class="keyword">if</span> i &lt; <span class="built_in">len</span>(<span class="variable language_">self</span>.blocks) - <span class="number">1</span>:</span><br><span class="line">                x = blk(x)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># return attention of the last block</span></span><br><span class="line">                <span class="keyword">return</span> blk(x, return_attention=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># ... existing code ...</span></span><br></pre></td></tr></table></figure>
<p>简单来说，在准备完tokens即<code>x = self.prepare_tokens(x)</code>执行结束后，让<code>x</code>逐个通过所有的<code>self.blocks</code>，当到达最后一个block时，返回最后一个block的注意力图。</p>
<p>那么现在需要知道<code>self.blocks</code>是什么，<code>self.blocks</code>是一个<code>nn.ModuleList</code>，其中包含depth个<code>Block</code>类，<code>Block</code>类的定义在<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ZhY2Vib29rcmVzZWFyY2gvZGluby9ibG9iL21haW4vdmlzaW9uX3RyYW5zZm9ybWVyLnB5I0w5NQ==">vision_transformer.py第95行<i class="fa fa-external-link-alt"></i></span>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">False</span>, qk_scale=<span class="literal">None</span>, drop=<span class="number">0.</span>, attn_drop=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 drop_path=<span class="number">0.</span>, act_layer=nn.GELU, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="comment"># ... existing code ...</span></span><br><span class="line">        <span class="variable language_">self</span>.attn = Attention(</span><br><span class="line">            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)</span><br><span class="line">        <span class="comment"># ... existing code ...</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, return_attention=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="comment"># ... existing code ...</span></span><br></pre></td></tr></table></figure>
<p><code>Block</code>类的核心部分便是<code>Attention</code>类，而<code>Attention</code>类的定义在<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ZhY2Vib29rcmVzZWFyY2gvZGluby9ibG9iL21haW4vdmlzaW9uX3RyYW5zZm9ybWVyLnB5I0w2OA==">vision_transformer.py第68行<i class="fa fa-external-link-alt"></i></span>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads=<span class="number">8</span>, qkv_bias=<span class="literal">False</span>, qk_scale=<span class="literal">None</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        <span class="variable language_">self</span>.scale = qk_scale <span class="keyword">or</span> head_dim ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        <span class="variable language_">self</span>.proj = nn.Linear(dim, dim)</span><br><span class="line">        <span class="variable language_">self</span>.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, N, C = x.shape</span><br><span class="line">        qkv = <span class="variable language_">self</span>.qkv(x).reshape(B, N, <span class="number">3</span>, <span class="variable language_">self</span>.num_heads, C // <span class="variable language_">self</span>.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        q, k, v = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) * <span class="variable language_">self</span>.scale</span><br><span class="line">        attn = attn.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        attn = <span class="variable language_">self</span>.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B, N, C)</span><br><span class="line">        x = <span class="variable language_">self</span>.proj(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x, attn</span><br></pre></td></tr></table></figure>
<p>在这个<code>Attention</code>类中，同时实现了单头注意力和多头注意力，逐行进行分析：</p>
<ul>
<li><p><code>def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):</code></p>
<ol type="1">
<li><code>super().__init__()</code>：调用父类<code>nn.Module</code>的构造函数；</li>
<li><code>self.num_heads = num_heads</code>：设置注意力头的数量；</li>
<li><code>head_dim = dim // num_heads</code>：设置每个注意力头所需要处理的特征维度；</li>
<li><code>self.scale = qk_scale or head_dim ** -0.5</code>：设置注意力分数的缩放因子，如果<code>qk_scale</code>不为空，则使用<code>qk_scale</code>，否则使用<code>head_dim</code>的负0.5次方作为缩放因子，即<span
class="math inline">\(\frac{1}{\sqrt{\text{head_dim}}}\)</span>，和原版Transformer中的设置一致；</li>
<li><code>self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)</code>：定义一个线性层，将原本的特征维度从<code>C</code>变为<code>3 * C</code>，以便于在后续切分为<code>q</code>,
<code>k</code>, <code>v</code>；</li>
<li><code>self.attn_drop = nn.Dropout(attn_drop)</code>：注意力Dropout；</li>
<li><code>self.proj = nn.Linear(dim, dim)</code>：一个线性层；</li>
<li><code>self.proj_drop = nn.Dropout(proj_drop)</code>：投影Dropout；</li>
</ol></li>
<li><p><code>def forward(self, x):</code></p>
<ol type="1">
<li><p><code>B, N, C = x.shape</code>：获取输入张量的维度，<code>B</code>表示批量大小，<code>N</code>表示序列长度，<code>C</code>表示特征维度，也就是说<span
class="math inline">\(\mathbf{x} \in \mathbb{R}^{B \times N \times
C}\)</span>；</p></li>
<li><p><code>qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)</code>：等价于顺序执行下面三步：</p>
<ul>
<li><p><code>qkv = self.qkv(x)</code>，<code>qkv = qkv.reshape(B, N, 3, self.num_heads, C // self.num_heads)</code>，<code>qkv = qkv.permute(2, 0, 3, 1, 4)</code>；</p></li>
<li><p>其中：</p>
<ul>
<li><code>qkv = self.qkv(x)</code>：<span
class="math inline">\(\mathbf{qkv} \in \mathbb{R}^{B \times N \times
3C}\)</span>；</li>
<li><code>qkv = qkv.reshape(B, N, 3, self.num_heads, C // self.num_heads)</code>：<span
class="math inline">\(\mathbf{qkv} \in \mathbb{R}^{B \times N \times 3
\times \text{num_heads} \times
\frac{C}{\text{num_heads}}}\)</span>；</li>
<li><code>qkv = qkv.permute(2, 0, 3, 1, 4)</code>：<span
class="math inline">\(\mathbf{qkv} \in \mathbb{R}^{3 \times B \times
\text{num_heads} \times N \times
\frac{C}{\text{num_heads}}}\)</span>；</li>
</ul></li>
</ul></li>
<li><p><code>q, k, v = qkv[0], qkv[1], qkv[2]</code>：</p>
<ul>
<li><span class="math inline">\(\mathbf{q} \in \mathbb{R}^{B \times
\text{num_heads} \times N \times
\frac{C}{\text{num_heads}}}\)</span>；</li>
<li><span class="math inline">\(\mathbf{k} \in \mathbb{R}^{B \times
\text{num_heads} \times N \times
\frac{C}{\text{num_heads}}}\)</span>；</li>
<li><span class="math inline">\(\mathbf{v} \in \mathbb{R}^{B \times
\text{num_heads} \times N \times
\frac{C}{\text{num_heads}}}\)</span>；</li>
</ul></li>
<li><p><code>attn = (q @ k.transpose(-2, -1)) * self.scale</code>：<span
class="math inline">\(\mathbf{attn} \in \mathbb{R}^{B \times
\text{num_heads} \times N \times N}\)</span>；</p></li>
<li><p><code>attn = attn.softmax(dim=-1)</code>：softmax归一化；</p></li>
<li><p><code>attn = self.attn_drop(attn)</code>：注意力Dropout；</p></li>
<li><p><code>x = (attn @ v).transpose(1, 2).reshape(B, N, C)</code>：等价于顺序执行下面三步：</p>
<ul>
<li><p><code>x = (attn @ v)</code>，<code>x = x.transpose(1, 2)</code>，<code>x = x.reshape(B, N, C)</code>；</p></li>
<li><p>其中：</p>
<ul>
<li><code>x = (attn @ v)</code>：<span class="math inline">\(\mathbf{x}
\in \mathbb{R}^{B \times \text{num_heads} \times N \times
\frac{C}{\text{num_heads}}}\)</span>；</li>
<li><code>x = x.transpose(1, 2)</code>：<span
class="math inline">\(\mathbf{x} \in \mathbb{R}^{B \times N \times
\text{num_heads} \times \frac{C}{\text{num_heads}}}\)</span>；</li>
<li><code>x = x.reshape(B, N, C)</code>：<span
class="math inline">\(\mathbf{x} \in \mathbb{R}^{B \times N \times
C}\)</span>；</li>
</ul></li>
</ul></li>
<li><p><code>x = self.proj(x)</code>：<span
class="math inline">\(\mathbf{x} \in \mathbb{R}^{B \times N \times
C}\)</span>；</p></li>
<li><p><code>x = self.proj_drop(x)</code>：投影Dropout；</p></li>
<li><p><code>return x, attn</code>：</p>
<ul>
<li><span class="math inline">\(\mathbf{x} \in \mathbb{R}^{B \times N
\times C}\)</span>；</li>
<li><span class="math inline">\(\mathbf{attn} \in \mathbb{R}^{B \times
\text{num_heads} \times N \times N}\)</span>。</li>
</ul></li>
</ol></li>
</ul>
<p><code>Attention</code>类结束，回到<code>Block</code>类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">False</span>, qk_scale=<span class="literal">None</span>, drop=<span class="number">0.</span>, attn_drop=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 drop_path=<span class="number">0.</span>, act_layer=nn.GELU, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.norm1 = norm_layer(dim)</span><br><span class="line">        <span class="variable language_">self</span>.attn = Attention(</span><br><span class="line">            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)</span><br><span class="line">        <span class="variable language_">self</span>.drop_path = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        <span class="variable language_">self</span>.norm2 = norm_layer(dim)</span><br><span class="line">        mlp_hidden_dim = <span class="built_in">int</span>(dim * mlp_ratio)</span><br><span class="line">        <span class="variable language_">self</span>.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, return_attention=<span class="literal">False</span></span>):</span><br><span class="line">        y, attn = <span class="variable language_">self</span>.attn(<span class="variable language_">self</span>.norm1(x))</span><br><span class="line">        <span class="keyword">if</span> return_attention:</span><br><span class="line">            <span class="keyword">return</span> attn</span><br><span class="line">        x = x + <span class="variable language_">self</span>.drop_path(y)</span><br><span class="line">        x = x + <span class="variable language_">self</span>.drop_path(<span class="variable language_">self</span>.mlp(<span class="variable language_">self</span>.norm2(x)))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p><code>Block</code>类中关键的就只有<code>self.attn</code>了，所以不详细介绍了，要注意的是在<code>forward</code>中，当<code>return_attention=True</code>时，返回的是注意力图，否则返回的是输入经过注意力机制后的结果。</p>
<p><code>Block</code>类结束，回到<code>get_last_selfattention</code>函数，<code>get_last_selfattention</code>函数中只是<code>Block</code>类的堆叠，所以<code>get_last_selfattention</code>函数也结束。</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ZhY2Vib29rcmVzZWFyY2gvZGluby9ibG9iL21haW4vdmlzaW9uX3RyYW5zZm9ybWVyLnB5">vision_transformer.py<i class="fa fa-external-link-alt"></i></span>文件中还有下面几个部分没有提到：</p>
<ul>
<li><p><code>drop_path</code>函数：</p>
<ul>
<li><p>该函数值得展开，函数定义在<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ZhY2Vib29rcmVzZWFyY2gvZGluby9ibG9iL21haW4vdmlzaW9uX3RyYW5zZm9ybWVyLnB5I0wyNw==">vision_transformer.py第27行<i class="fa fa-external-link-alt"></i></span>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">drop_path</span>(<span class="params">x, drop_prob: <span class="built_in">float</span> = <span class="number">0.</span>, training: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> drop_prob == <span class="number">0.</span> <span class="keyword">or</span> <span class="keyword">not</span> training:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    keep_prob = <span class="number">1</span> - drop_prob</span><br><span class="line">    shape = (x.shape[<span class="number">0</span>],) + (<span class="number">1</span>,) * (x.ndim - <span class="number">1</span>)  <span class="comment"># work with diff dim tensors, not just 2D ConvNets</span></span><br><span class="line">    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)</span><br><span class="line">    random_tensor.floor_()  <span class="comment"># binarize</span></span><br><span class="line">    output = x.div(keep_prob) * random_tensor</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<ul>
<li><p><code>drop_path</code>，顾名思义，就是丢弃一整条路径上的所有值，但是实际上，<code>drop_path</code>是丢弃输入向量中的若干分量：</p>
<ul>
<li><code>if drop_prob == 0. or not training: return x</code>：如果<code>drop_prob</code>为0，或者训练模式为<code>False</code>，则直接返回输入向量；</li>
<li><code>keep_prob = 1 - drop_prob</code>：计算保留的概率；</li>
<li><code>shape = (x.shape[0],) + (1,) * (x.ndim - 1)</code>：创建一个与输入张量兼容的广播形状，保持第一维（批次维）不变，其他维度都设为1；</li>
<li><code>random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)</code>：生成一个与输入张量兼容的随机张量，其值在<code>[0 + keep_prob, 1 + keep_prob]</code>之间；</li>
<li><code>random_tensor.floor_()</code>：将随机张量的值向下取整为0或1，相当于二值化；</li>
<li><code>output = x.div(keep_prob) * random_tensor</code>：将输入张量除以保留概率，然后与二值化后的随机张量相乘，得到输出张量；</li>
<li><code>return output</code>：返回输出张量；</li>
<li><code>DropPath</code>与<code>Dropout</code>相比，能够通过缩放保持期望值不变，提供了更好的正则化效果，帮助网络学习更鲁棒的特征；</li>
</ul></li>
</ul></li>
</ul></li>
<li><p><code>DropPath</code>类：使用<code>drop_path</code>函数处理输入的向量；</p></li>
<li><p><code>Mlp</code>类：在<code>Block</code>类中使用到的<code>MLP</code>层；</p></li>
<li><p><code>vit_tiny</code>函数：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vit_tiny</span>(<span class="params">patch_size=<span class="number">16</span>, **kwargs</span>):</span><br><span class="line">    model = VisionTransformer(</span><br><span class="line">        patch_size=patch_size, embed_dim=<span class="number">192</span>, depth=<span class="number">12</span>, num_heads=<span class="number">3</span>, mlp_ratio=<span class="number">4</span>,</span><br><span class="line">        qkv_bias=<span class="literal">True</span>, norm_layer=partial(nn.LayerNorm, eps=<span class="number">1e-6</span>), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>mlp_ratio=4</code>：MLP隐藏层维度是嵌入维度的4倍，比如这里MLP隐藏层的维度就是192
* 4 = 768；</li>
</ul></li>
<li><p><code>vit_small</code>函数：</p>
<ul>
<li><code>embed_dim=384</code>；</li>
<li><code>num_heads=6</code>；</li>
</ul></li>
<li><p><code>vit_base</code>函数：</p>
<ul>
<li><code>embed_dim=768</code>；</li>
<li><code>num_heads=12</code>；</li>
</ul></li>
<li><p><code>DINOHead</code>类：DINO自监督学习中的特征投影，用于计算学生和教师网络输出的相似度。</p></li>
</ul>
<p>还没完，<code>VisionTransformer</code>类中还有下面几个函数没有提到：</p>
<ul>
<li><code>_init_weights</code>函数：针对不同的层采取不同的初始化方案；</li>
<li><code>forward</code>函数：只输出[CLS] token，用于分类任务；</li>
<li><code>get_intermediate_layers</code>函数：用于获取Vision
Transformer最后n个块的中间层特征。</li>
</ul>
<h3 id="ssl-with-knowledge-distillation">3.1. SSL with Knowledge
Distillation</h3>
<figure>
<img
src="https://img.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron/test.webp"
alt="Figure 2: Self-distillation with no labels. We illustrate DINO in the case of one single pair of views (x_1, x_2) for simplicity. The model passes two different random transformations of an input image to the student and teacher networks. Both networks have the same architecture but different parameters. The output of the teacher network is centered with a mean computed over the batch. Each networks outputs a K dimensional feature that is normalized with a temperature softmax over the feature dimension. Their similarity is then measured with a cross-entropy loss. We apply a stop-gradient (sg) operator on the teacher to propagate gradients only through the student. The teacher parameters are updated with an exponential moving average (ema) of the student parameters." />
<figcaption aria-hidden="true">Figure 2: Self-distillation with no
labels. We illustrate DINO in the case of one single pair of views <span
class="math inline">\((x_1, x_2)\)</span> for simplicity. The model
passes two different random transformations of an input image to the
student and teacher networks. Both networks have the same architecture
but different parameters. The output of the teacher network is centered
with a mean computed over the batch. Each networks outputs a <span
class="math inline">\(K\)</span> dimensional feature that is normalized
with a temperature softmax over the feature dimension. Their similarity
is then measured with a cross-entropy loss. We apply a stop-gradient
(sg) operator on the teacher to propagate gradients only through the
student. The teacher parameters are updated with an exponential moving
average (ema) of the student parameters.</figcaption>
</figure>
<div
style="border-top: 2px solid black; border-bottom: 2px solid black; background-color: #f8f9fa;">
<pre style="margin: 0; background-color: #f8f9fa; color: black;"><span style="font-weight: bold;">Algorithm 1</span> DINO PyTorch pseudocode w/o multi-crop.
<hr style="border: none; border-top: 1px solid black; margin: 2px 0; width: 100%;">
<span style="color: #408080;"># gs, gt: student and teacher networks
# C: center (K)
# tps, tpt: student and teacher temperatures
# l, m: network and center momentum rates</span>
gt.params = gs.params
for x in loader: <span style="color: #408080;"># load a minibatch x with n samples</span>
    x1, x2 = augment(x), augment(x) <span style="color: #408080;"># random views</span>

    s1, s2 = gs(x1), gs(x2) <span style="color: #408080;"># student output n-by-K</span>
    t1, t2 = gt(x1), gt(x2) <span style="color: #408080;"># teacher output n-by-K</span>

    loss = H(t1, s2)/2 + H(t2, s1)/2
    loss.backward() <span style="color: #408080;"># back-propagate</span>

    <span style="color: #408080;"># student, teacher and center updates</span>
    update(gs) <span style="color: #408080;"># SGD</span>
    gt.params = l*gt.params + (1-l)*gs.params
    C = m*C + (1-m)*cat([t1, t2]).mean(dim=0)

def H(t, s):
    t = t.detach() <span style="color: #408080;"># stop gradient</span>
    s = softmax(s / tps, dim=1)
    t = softmax((t - C) / tpt, dim=1) <span style="color: #408080;"># center + sharpen</span>
    return - (t * log(s)).sum(dim=1).mean()</pre>
</div>
<h3 id="implementation-and-evaluation-protocols">3.2. Implementation and
evaluation protocols</h3>
<table>
<caption style="text-align: left;">
Table 1: Networks configuration. "Blocks" is the number of Transformer
blocks, "dim" is channel dimension and "heads" is the number of heads in
multi-head attention. "# tokens" is the length of the token sequence
when considering <span class="math inline">\(224^2\)</span> resolution
inputs, "# params" is the total number of parameters (without counting
the projection head) and "im/s" is the inference time on a NVIDIA V100
GPU with 128 samples per forward.
</caption>
<thead>
<tr>
<th style="border-top: 2px solid black; border-bottom: 1px solid black;">
model
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
blocks
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
dim
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
heads
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
#tokens
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
#params
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
im/s
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
ResNet-50
</td>
<td style="text-align: center;">
-
</td>
<td style="text-align: center;">
2048
</td>
<td style="text-align: center;">
-
</td>
<td style="text-align: center;">
-
</td>
<td style="text-align: center;">
23M
</td>
<td style="text-align: center;">
1237
</td>
</tr>
<tr>
<td>
ViT-S/16
</td>
<td style="text-align: center;">
12
</td>
<td style="text-align: center;">
384
</td>
<td style="text-align: center;">
6
</td>
<td style="text-align: center;">
197
</td>
<td style="text-align: center;">
21M
</td>
<td style="text-align: center;">
1007
</td>
</tr>
<tr>
<td>
ViT-S/8
</td>
<td style="text-align: center;">
12
</td>
<td style="text-align: center;">
384
</td>
<td style="text-align: center;">
6
</td>
<td style="text-align: center;">
785
</td>
<td style="text-align: center;">
21M
</td>
<td style="text-align: center;">
180
</td>
</tr>
<tr>
<td>
ViT-B/16
</td>
<td style="text-align: center;">
12
</td>
<td style="text-align: center;">
768
</td>
<td style="text-align: center;">
12
</td>
<td style="text-align: center;">
197
</td>
<td style="text-align: center;">
85M
</td>
<td style="text-align: center;">
312
</td>
</tr>
<tr>
<td style="border-bottom: 2px solid black;">
ViT-B/8
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
12
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
768
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
12
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
785
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
85M
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
63
</td>
</tr>
</tbody>
</table>
<h2 id="main-results">4. Main Results</h2>
<h3 id="comparing-with-ssl-frameworks-on-imagenet">4.1. Comparing with
SSL frameworks on ImageNet</h3>
<table>
<caption style="text-align: left;">
Table 2: Linear and <span class="math inline">\(k\)</span>-NN
classification on ImageNet. We report top-1 accuracy for linear and
<span class="math inline">\(k\)</span>-NN evaluations on the validation
set of ImageNet for different self-supervised methods. We focus on
ResNet-50 and ViT-small architectures, but also report the best results
obtained across architectures. <sup>∗</sup> are run by us. We run the
<span class="math inline">\(k\)</span>-NN evaluation for models with
official released weights. The throughput (im/s) is calculated on a
NVIDIA V100 GPU with 128 samples per forward. Parameters (M) are of the
feature extractor.
</caption>
<thead>
<tr>
<th style="border-top: 2px solid black; border-bottom: 1px solid black;">
Method
</th>
<th style="border-top: 2px solid black; border-bottom: 1px solid black;">
Arch.
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Param.
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
im/s
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Linear
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
<span class="math inline">\(k\)</span>-NN
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="color: #808080;">
Supervised
</td>
<td style="color: #808080;">
RN50
</td>
<td style="text-align: center; color: #808080;">
23
</td>
<td style="text-align: center; color: #808080;">
1237
</td>
<td style="text-align: center; color: #808080;">
79.3
</td>
<td style="text-align: center; color: #808080;">
79.3
</td>
</tr>
<tr>
<td>
SCLR [12]
</td>
<td>
RN50
</td>
<td style="text-align: center;">
23
</td>
<td style="text-align: center;">
1237
</td>
<td style="text-align: center;">
69.1
</td>
<td style="text-align: center;">
60.7
</td>
</tr>
<tr>
<td>
MoCov2 [15]
</td>
<td>
RN50
</td>
<td style="text-align: center;">
23
</td>
<td style="text-align: center;">
1237
</td>
<td style="text-align: center;">
71.1
</td>
<td style="text-align: center;">
61.9
</td>
</tr>
<tr>
<td>
InfoMin [67]
</td>
<td>
RN50
</td>
<td style="text-align: center;">
23
</td>
<td style="text-align: center;">
1237
</td>
<td style="text-align: center;">
73.0
</td>
<td style="text-align: center;">
65.3
</td>
</tr>
<tr>
<td>
BarlowT [81]
</td>
<td>
RN50
</td>
<td style="text-align: center;">
23
</td>
<td style="text-align: center;">
1237
</td>
<td style="text-align: center;">
73.2
</td>
<td style="text-align: center;">
66.0
</td>
</tr>
<tr>
<td>
OBoW [27]
</td>
<td>
RN50
</td>
<td style="text-align: center;">
23
</td>
<td style="text-align: center;">
1237
</td>
<td style="text-align: center;">
73.8
</td>
<td style="text-align: center;">
61.9
</td>
</tr>
<tr>
<td>
BYOL [30]
</td>
<td>
RN50
</td>
<td style="text-align: center;">
23
</td>
<td style="text-align: center;">
1237
</td>
<td style="text-align: center;">
74.4
</td>
<td style="text-align: center;">
64.8
</td>
</tr>
<tr>
<td>
DCv2 [10]
</td>
<td>
RN50
</td>
<td style="text-align: center;">
23
</td>
<td style="text-align: center;">
1237
</td>
<td style="text-align: center;">
75.2
</td>
<td style="text-align: center;">
67.1
</td>
</tr>
<tr>
<td>
SwAV [10]
</td>
<td>
RN50
</td>
<td style="text-align: center;">
23
</td>
<td style="text-align: center;">
1237
</td>
<td style="text-align: center;">
<strong>75.3</strong>
</td>
<td style="text-align: center;">
65.7
</td>
</tr>
<tr style="background-color: #f7e0d5;">
<td style="border-bottom: 1px solid black;">
DINO
</td>
<td style="border-bottom: 1px solid black;">
RN50
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
23
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
1237
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
<strong>75.3</strong>
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
<strong>67.5</strong>
</td>
</tr>
<tr>
<td style="color: #808080;">
Supervised
</td>
<td style="color: #808080;">
ViT-S
</td>
<td style="text-align: center; color: #808080;">
21
</td>
<td style="text-align: center; color: #808080;">
1007
</td>
<td style="text-align: center; color: #808080;">
79.8
</td>
<td style="text-align: center; color: #808080;">
79.8
</td>
</tr>
<tr>
<td>
BYOL<sup>∗</sup> [30]
</td>
<td>
ViT-S
</td>
<td style="text-align: center;">
21
</td>
<td style="text-align: center;">
1007
</td>
<td style="text-align: center;">
71.4
</td>
<td style="text-align: center;">
66.6
</td>
</tr>
<tr>
<td>
MoCov2<sup>∗</sup> [15]
</td>
<td>
ViT-S
</td>
<td style="text-align: center;">
21
</td>
<td style="text-align: center;">
1007
</td>
<td style="text-align: center;">
72.7
</td>
<td style="text-align: center;">
64.4
</td>
</tr>
<tr>
<td>
SwAV<sup>∗</sup> [10]
</td>
<td>
ViT-S
</td>
<td style="text-align: center;">
21
</td>
<td style="text-align: center;">
1007
</td>
<td style="text-align: center;">
73.5
</td>
<td style="text-align: center;">
66.3
</td>
</tr>
<tr style="background-color: #f7e0d5;">
<td style="border-bottom: 1px solid black;">
DINO
</td>
<td style="border-bottom: 1px solid black;">
ViT-S
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
21
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
1007
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
<strong>77.0</strong>
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
<strong>74.5</strong>
</td>
</tr>
<tr>
<td colspan="6" style="border-bottom: 1px solid black; height: 1px;">
</td>
</tr>
<tr>
<td colspan="6" style="font-style: italic;">
Comparison across architectures
</td>
</tr>
<tr>
<td>
SCLR [12]
</td>
<td>
RN50w4
</td>
<td style="text-align: center;">
375
</td>
<td style="text-align: center;">
117
</td>
<td style="text-align: center;">
76.8
</td>
<td style="text-align: center;">
69.3
</td>
</tr>
<tr>
<td>
SwAV [10]
</td>
<td>
RN50w2
</td>
<td style="text-align: center;">
93
</td>
<td style="text-align: center;">
384
</td>
<td style="text-align: center;">
77.3
</td>
<td style="text-align: center;">
67.3
</td>
</tr>
<tr>
<td>
BYOL [30]
</td>
<td>
RN50w2
</td>
<td style="text-align: center;">
93
</td>
<td style="text-align: center;">
384
</td>
<td style="text-align: center;">
77.4
</td>
<td style="text-align: center;">
-
</td>
</tr>
<tr style="background-color: #f7e0d5;">
<td>
DINO
</td>
<td>
ViT-B/16
</td>
<td style="text-align: center;">
85
</td>
<td style="text-align: center;">
312
</td>
<td style="text-align: center;">
78.2
</td>
<td style="text-align: center;">
76.1
</td>
</tr>
<tr>
<td>
SwAV [10]
</td>
<td>
RN50w5
</td>
<td style="text-align: center;">
586
</td>
<td style="text-align: center;">
76
</td>
<td style="text-align: center;">
78.5
</td>
<td style="text-align: center;">
67.1
</td>
</tr>
<tr>
<td>
BYOL [30]
</td>
<td>
RN50w4
</td>
<td style="text-align: center;">
375
</td>
<td style="text-align: center;">
117
</td>
<td style="text-align: center;">
78.6
</td>
<td style="text-align: center;">
-
</td>
</tr>
<tr>
<td>
BYOL [30]
</td>
<td>
RN200w2
</td>
<td style="text-align: center;">
250
</td>
<td style="text-align: center;">
123
</td>
<td style="text-align: center;">
79.6
</td>
<td style="text-align: center;">
73.9
</td>
</tr>
<tr style="background-color: #f7e0d5;">
<td>
DINO
</td>
<td>
ViT-S/8
</td>
<td style="text-align: center;">
21
</td>
<td style="text-align: center;">
180
</td>
<td style="text-align: center;">
79.7
</td>
<td style="text-align: center;">
<strong>78.3</strong>
</td>
</tr>
<tr>
<td>
SCLRv2 [13]
</td>
<td>
RN152w3+SK
</td>
<td style="text-align: center;">
794
</td>
<td style="text-align: center;">
46
</td>
<td style="text-align: center;">
79.8
</td>
<td style="text-align: center;">
73.1
</td>
</tr>
<tr style="background-color: #f7e0d5;">
<td style="border-bottom: 2px solid black;">
DINO
</td>
<td style="border-bottom: 2px solid black;">
ViT-B/8
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
85
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
63
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
<strong>80.1</strong>
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
77.4
</td>
</tr>
</tbody>
</table>
<h3 id="properties-of-vit-trained-with-ssl">4.2. Properties of ViT
trained with SSL</h3>
<h4 id="nearest-neighbor-retrieval-with-dino-vit">4.2.1 Nearest neighbor
retrieval with DINO ViT</h4>
<figure>
<img
src="https://img.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron/fig3.webp"
alt="Figure 3: Attention maps from multiple heads. We consider the heads from the last layer of a ViT-S/8 trained with DINO and display the self-attention for [CLS] token query. Different heads, materialized by different colors, focus on different locations that represents different objects or parts (more examples in Appendix)." />
<figcaption aria-hidden="true">Figure 3: Attention maps from multiple
heads. We consider the heads from the last layer of a ViT-S/8 trained
with DINO and display the self-attention for [CLS] token query.
Different heads, materialized by different colors, focus on different
locations that represents different objects or parts (more examples in
Appendix).</figcaption>
</figure>
<table>
<caption style="text-align: left;">
Table 3: Image retrieval. We compare the performance in retrieval of
off-the-shelf features pretrained with supervision or with DINO on
ImageNet and Google Landmarks v2 (GLDv2) dataset. We report mAP on
revisited Oxford and Paris. Pretraining with DINO on a landmark dataset
performs particularly well. For reference, we also report the best
retrieval method with off-the-shelf features [57].
</caption>
<thead>
<tr>
<th rowspan="2" style="border-top: 2px solid black; border-bottom: 1px solid black;">
Pretrain
</th>
<th rowspan="2" style="border-top: 2px solid black; border-bottom: 1px solid black;">
Arch.
</th>
<th rowspan="2" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Pretrain
</th>
<th colspan="2" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
<span class="math inline">\(\mathcal{R}\)</span>Ox
</th>
<th colspan="2" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
<span class="math inline">\(\mathcal{R}\)</span>Par
</th>
</tr>
<tr>
<th style="text-align: center; border-bottom: 1px solid black;">
M
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
H
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
M
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
H
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="border-bottom: 1px solid black;">
Sup. [57]
</td>
<td style="border-bottom: 1px solid black;">
RN101+R-MAC
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
ImNet
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
49.8
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
18.5
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
74.0
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
<strong>52.1</strong>
</td>
</tr>
<tr>
<td>
Sup.
</td>
<td>
ViT-S/16
</td>
<td style="text-align: center;">
ImNet
</td>
<td style="text-align: center;">
33.5
</td>
<td style="text-align: center;">
8.9
</td>
<td style="text-align: center;">
63.0
</td>
<td style="text-align: center;">
37.2
</td>
</tr>
<tr style="background-color: #f7e0d5;">
<td>
DINO
</td>
<td>
ResNet-50
</td>
<td style="text-align: center;">
ImNet
</td>
<td style="text-align: center;">
35.4
</td>
<td style="text-align: center;">
11.1
</td>
<td style="text-align: center;">
55.9
</td>
<td style="text-align: center;">
27.5
</td>
</tr>
<tr style="background-color: #f7e0d5;">
<td>
DINO
</td>
<td>
ViT-S/16
</td>
<td style="text-align: center;">
ImNet
</td>
<td style="text-align: center;">
41.8
</td>
<td style="text-align: center;">
13.7
</td>
<td style="text-align: center;">
63.1
</td>
<td style="text-align: center;">
34.4
</td>
</tr>
<tr style="background-color: #f7e0d5;">
<td style="border-bottom: 2px solid black;">
DINO
</td>
<td style="border-bottom: 2px solid black;">
ViT-S/16
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
GLDv2
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
<strong>51.5</strong>
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
<strong>24.3</strong>
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
<strong>75.3</strong>
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
51.6
</td>
</tr>
</tbody>
</table>
<table style="width: 100%; border-collapse: collapse;">
<caption style="text-align: left;">
Table 4: Copy detection. We report the mAP performance in copy detection
on Copydays "strong" subset [21]. For reference, we also report the
performance of the multigrain model [5], trained specifically for
particular object retrieval.
</caption>
<thead>
<tr>
<th style="border-top: 2px solid black; border-bottom: 1px solid black;">
Method
</th>
<th style="border-top: 2px solid black; border-bottom: 1px solid black;">
Arch.
</th>
<th style="border-top: 2px solid black; border-bottom: 1px solid black;">
Dim.
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Resolution
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
mAP
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Multigrain [5]
</td>
<td>
ResNet-50
</td>
<td>
2048
</td>
<td style="text-align: center;">
224<sup>2</sup>
</td>
<td style="text-align: center;">
75.1
</td>
</tr>
<tr>
<td style="border-bottom: 1px solid black;">
Multigrain [5]
</td>
<td style="border-bottom: 1px solid black;">
ResNet-50
</td>
<td style="border-bottom: 1px solid black;">
2048
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
largest side 800
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
82.5
</td>
</tr>
<tr>
<td>
Supervised [69]
</td>
<td>
ViT-B/16
</td>
<td>
1536
</td>
<td style="text-align: center;">
224<sup>2</sup>
</td>
<td style="text-align: center;">
76.4
</td>
</tr>
<tr style="background-color: #f7e0d5;">
<td>
DINO
</td>
<td>
ViT-B/16
</td>
<td>
1536
</td>
<td style="text-align: center;">
224<sup>2</sup>
</td>
<td style="text-align: center;">
81.7
</td>
</tr>
<tr style="background-color: #f7e0d5;">
<td style="border-bottom: 2px solid black;">
DINO
</td>
<td style="border-bottom: 2px solid black;">
ViT-B/8
</td>
<td style="border-bottom: 2px solid black;">
1536
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
320<sup>2</sup>
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
<strong>85.5</strong>
</td>
</tr>
</tbody>
</table>
<table style="width: 100%; border-collapse: collapse;">
<caption style="text-align: left;">
Table 5: DAVIS 2017 Video object segmentation. We evaluate the quality
of frozen features on video instance tracking. We report mean region
similarity <span class="math inline">\(\mathcal{J}_m\)</span> and mean
contour-based accuracy <span
class="math inline">\(\mathcal{F}_m\)</span>. We compare with existing
self-supervised methods and a supervised ViT-S/8 trained on ImageNet.
Image resolution is 480p.
</caption>
<thead>
<tr>
<th style="border-top: 2px solid black; border-bottom: 1px solid black;">
Method
</th>
<th style="border-top: 2px solid black; border-bottom: 1px solid black;">
Data
</th>
<th style="border-top: 2px solid black; border-bottom: 1px solid black;">
Arch.
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
<span class="math inline">\((\mathcal{J}\&amp;\mathcal{F})_m\)</span>
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
<span class="math inline">\(\mathcal{J}_m\)</span>
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
<span class="math inline">\(\mathcal{F}_m\)</span>
</th>
</tr>
</thead>
<tbody>
<tr style="font-style: italic;">
<td colspan="6">
Supervised
</td>
</tr>
<tr>
<td>
ImageNet
</td>
<td>
INet
</td>
<td>
ViT-S/8
</td>
<td style="text-align: center;">
66.0
</td>
<td style="text-align: center;">
63.9
</td>
<td style="text-align: center;">
68.1
</td>
</tr>
<tr>
<td style="border-bottom: 1px solid black;">
STM [48]
</td>
<td style="border-bottom: 1px solid black;">
I/D/Y
</td>
<td style="border-bottom: 1px solid black;">
RN50
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
81.8
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
79.2
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
84.3
</td>
</tr>
<tr style="font-style: italic;">
<td colspan="6" style="border-top: 1px solid black;">
Self-supervised
</td>
</tr>
<tr>
<td>
CT [71]
</td>
<td>
VLOG
</td>
<td>
RN50
</td>
<td style="text-align: center;">
48.7
</td>
<td style="text-align: center;">
46.4
</td>
<td style="text-align: center;">
50.0
</td>
</tr>
<tr>
<td>
MAST [40]
</td>
<td>
YT-VOS
</td>
<td>
RN18
</td>
<td style="text-align: center;">
65.5
</td>
<td style="text-align: center;">
63.3
</td>
<td style="text-align: center;">
67.6
</td>
</tr>
<tr>
<td>
STC [37]
</td>
<td>
Kinetics
</td>
<td>
RN18
</td>
<td style="text-align: center;">
67.6
</td>
<td style="text-align: center;">
64.8
</td>
<td style="text-align: center;">
70.2
</td>
</tr>
<tr style="background-color: #f7e0d5;">
<td>
DINO
</td>
<td>
INet
</td>
<td>
ViT-S/16
</td>
<td style="text-align: center;">
61.8
</td>
<td style="text-align: center;">
60.2
</td>
<td style="text-align: center;">
63.4
</td>
</tr>
<tr style="background-color: #f7e0d5;">
<td>
DINO
</td>
<td>
INet
</td>
<td>
ViT-B/16
</td>
<td style="text-align: center;">
62.3
</td>
<td style="text-align: center;">
60.7
</td>
<td style="text-align: center;">
63.9
</td>
</tr>
<tr style="background-color: #f7e0d5;">
<td>
DINO
</td>
<td>
INet
</td>
<td>
ViT-S/8
</td>
<td style="text-align: center;">
<strong>69.9</strong>
</td>
<td style="text-align: center;">
<strong>66.6</strong>
</td>
<td style="text-align: center;">
<strong>73.1</strong>
</td>
</tr>
<tr style="background-color: #f7e0d5;">
<td style="border-bottom: 2px solid black;">
DINO
</td>
<td style="border-bottom: 2px solid black;">
INet
</td>
<td style="border-bottom: 2px solid black;">
ViT-B/8
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
<strong>71.4</strong>
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
<strong>67.9</strong>
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
<strong>74.9</strong>
</td>
</tr>
</tbody>
</table>
<h4 id="discovering-the-semantic-layout-of-scenes">4.2.2 Discovering the
semantic layout of scenes</h4>
<h4 id="transfer-learning-on-downstream-tasks">4.2.3 Transfer learning
on downstream tasks</h4>
<h2 id="ablation-study-of-dino">5. Ablation Study of DINO</h2>
<h3 id="importance-of-the-different-components">5.1. Importance of the
Different Components</h3>
<p><img
src="https://img.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron/fig4.webp" /></p>
<table>
<tr>
<th style="border-top: 2px solid black; border-bottom: 1px solid black;">
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Random
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Supervised
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
DINO
</th>
</tr>
<tr>
<td>
ViT-S/16
</td>
<td style="text-align: center;">
22.0
</td>
<td style="text-align: center;">
27.3
</td>
<td style="text-align: center; background-color: #f7e0d5;">
45.9
</td>
</tr>
<tr>
<td style="border-bottom: 2px solid black;">
ViT-S/8
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
21.8
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
23.7
</td>
<td style="text-align: center; border-bottom: 2px solid black; background-color: #f7e0d5;">
44.7
</td>
</tr>
<caption style="caption-side: bottom; text-align: left;">
Figure 4: Segmentations from supervised versus DINO. We visualize masks
obtained by thresholding the self-attention maps to keep 60% of the
mass. On top, we show the resulting masks for a ViT-S/8 trained with
supervision and DINO. We show the best head for both models. The table
at the bottom compares the Jaccard similarity between the ground truth
and these masks on the validation images of PASCAL VOC12 dataset.
</caption>
</table>
<table>
<caption style="text-align: left;">
Table 6: Transfer learning by finetuning pretrained models on different
datasets. We report top-1 accuracy. Self-supervised pretraining with
DINO transfers better than supervised pretraining.
</caption>
<thead>
<tr>
<th style="border-top: 2px solid black; border-bottom: 1px solid black;">
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Cifar<sub>10</sub>
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Cifar<sub>100</sub>
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
INat<sub>18</sub>
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
INat<sub>19</sub>
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Flwrs
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Cars
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
INet
</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="8" style="font-style: italic; padding-top: 10px;">
ViT-S/16
</td>
</tr>
<tr>
<td>
Sup. [69]
</td>
<td style="text-align: center;">
<strong>99.0</strong>
</td>
<td style="text-align: center;">
89.5
</td>
<td style="text-align: center;">
70.7
</td>
<td style="text-align: center;">
76.6
</td>
<td style="text-align: center;">
98.2
</td>
<td style="text-align: center;">
92.1
</td>
<td style="text-align: center;">
79.9
</td>
</tr>
<tr style="background-color: #f7e0d5;">
<td style="border-bottom: 1px solid black;">
DINO
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
<strong>99.0</strong>
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
<strong>90.5</strong>
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
<strong>72.0</strong>
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
<strong>78.2</strong>
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
<strong>98.5</strong>
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
<strong>93.0</strong>
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
<strong>81.5</strong>
</td>
</tr>
<tr>
<td colspan="8" style="font-style: italic; padding-top: 10px; border-top: 1px solid black;">
ViT-B/16
</td>
</tr>
<tr>
<td>
Sup. [69]
</td>
<td style="text-align: center;">
99.0
</td>
<td style="text-align: center;">
90.8
</td>
<td style="text-align: center;">
<strong>73.2</strong>
</td>
<td style="text-align: center;">
77.7
</td>
<td style="text-align: center;">
98.4
</td>
<td style="text-align: center;">
92.1
</td>
<td style="text-align: center;">
81.8
</td>
</tr>
<tr style="background-color: #f7e0d5;">
<td style="border-bottom: 2px solid black;">
DINO
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
<strong>99.1</strong>
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
<strong>91.7</strong>
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
72.6
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
<strong>78.6</strong>
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
<strong>98.8</strong>
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
<strong>93.0</strong>
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
<strong>82.8</strong>
</td>
</tr>
</tbody>
</table>
<table>
<caption style="text-align: left;">
Table 7: Important component for self-supervised ViT pre-training.
Models are trained for 300 epochs with ViT-S/16. We study the different
components that matter for the <span class="math inline">\(k\)</span>-NN
and linear ("Lin.") evaluations. For the different variants, we
highlight the differences from the default DINO setting. The best
combination is the momentum encoder with the multicrop augmentation and
the cross-entropy loss. We also report results with BYOL [30], MoCo-v2
[15] and SwAV [10].
</caption>
<thead>
<tr>
<th style="border-top: 2px solid black; border-bottom: 1px solid black;">
</th>
<th style="border-top: 2px solid black; border-bottom: 1px solid black;">
Method
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Mom.
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
SK
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
MC
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Loss
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Pred.
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
<span class="math inline">\(k\)</span>-NN
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Lin.
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
DINO
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
✗
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
CE
</td>
<td style="text-align: center;">
✗
</td>
<td style="text-align: center;">
72.8
</td>
<td style="text-align: center;">
76.1
</td>
</tr>
<tr>
<td>
2
</td>
<td>
</td>
<td style="text-align: center; background-color: #f7e0d5;">
✗
</td>
<td style="text-align: center;">
✗
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
CE
</td>
<td style="text-align: center;">
✗
</td>
<td style="text-align: center;">
0.1
</td>
<td style="text-align: center;">
0.1
</td>
</tr>
<tr>
<td>
3
</td>
<td>
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center; background-color: #f7e0d5;">
✓
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
CE
</td>
<td style="text-align: center;">
✗
</td>
<td style="text-align: center;">
72.2
</td>
<td style="text-align: center;">
76.0
</td>
</tr>
<tr>
<td>
4
</td>
<td>
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
✗
</td>
<td style="text-align: center; background-color: #f7e0d5;">
✗
</td>
<td style="text-align: center;">
CE
</td>
<td style="text-align: center;">
✗
</td>
<td style="text-align: center;">
67.9
</td>
<td style="text-align: center;">
72.5
</td>
</tr>
<tr>
<td>
5
</td>
<td>
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
✗
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center; background-color: #f7e0d5;">
MSE
</td>
<td style="text-align: center;">
✗
</td>
<td style="text-align: center;">
52.6
</td>
<td style="text-align: center;">
62.4
</td>
</tr>
<tr>
<td style="border-bottom: 1px solid black;">
6
</td>
<td style="border-bottom: 1px solid black;">
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
✓
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
✗
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
✓
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
CE
</td>
<td style="text-align: center; border-bottom: 1px solid black; background-color: #f7e0d5;">
✓
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
71.8
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
75.6
</td>
</tr>
<tr>
<td>
7
</td>
<td>
BYOL
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
✗
</td>
<td style="text-align: center;">
✗
</td>
<td style="text-align: center;">
MSE
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
66.6
</td>
<td style="text-align: center;">
71.4
</td>
</tr>
<tr>
<td>
8
</td>
<td>
MoCov2
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
✗
</td>
<td style="text-align: center;">
✗
</td>
<td style="text-align: center;">
INCE
</td>
<td style="text-align: center;">
✗
</td>
<td style="text-align: center;">
62.0
</td>
<td style="text-align: center;">
71.6
</td>
</tr>
<tr>
<td style="border-bottom: 2px solid black;">
9
</td>
<td style="border-bottom: 2px solid black;">
SwAV
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
✗
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
✓
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
✓
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
CE
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
✗
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
64.7
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
71.8
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td colspan="9" style="text-align: center;">
SK: Sinkhorn-Knopp, MC: Multi-Crop, Pred.: Predictor<br>CE:
Cross-Entropy, MSE: Mean Square Error, INCE: InfoNCE
</td>
</tr>
</tfoot>
</table>
<figure>
<img
src="https://img.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron/figure_throughput_knn_our.webp"
alt="Figure 5: Effect of Patch Size. k-NN evaluation as a function of the throughputs for different input patch sizes with ViT-B and ViT-S. Models are trained for 300 epochs." />
<figcaption aria-hidden="true">Figure 5: Effect of Patch Size. <span
class="math inline">\(k\)</span>-NN evaluation as a function of the
throughputs for different input patch sizes with ViT-B and ViT-S. Models
are trained for 300 epochs.</figcaption>
</figure>
<h3 id="impact-of-the-choice-of-teacher-network">5.2. Impact of the
choice of Teacher Network</h3>
<p><img
src="https://img.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron/figure_mom.webp" /></p>
<table>
<caption style="caption-side: bottom; text-align: center;">
Figure 6: Top-1 accuracy on ImageNet validation with <span
class="math inline">\(k\)</span>-NN classifier. (top) Comparison between
the performance of the momentum teacher and the student during training.
(bottom) Comparison between different types of teacher network. The
momentum encoder leads to the best performance but is not the only
viable option.
</caption>
<thead>
<tr>
<th style="border-top: 2px solid black; border-bottom: 1px solid black;">
Teacher
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Top-1
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Student copy
</td>
<td style="text-align: center;">
0.1
</td>
</tr>
<tr>
<td>
Previous iter
</td>
<td style="text-align: center;">
0.1
</td>
</tr>
<tr>
<td>
Previous epoch
</td>
<td style="text-align: center;">
66.6
</td>
</tr>
<tr>
<td style="border-bottom: 2px solid black;">
Momentum
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
72.8
</td>
</tr>
</tbody>
</table>
<h3 id="avoiding-collapse">5.3. Avoiding collapse</h3>
<figure>
<img
src="https://img.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron/figure_collapse.webp"
alt="Figure 7: Collapse study. (left): evolution of the teacher&#39;s target entropy along training epochs; (right): evolution of KL divergence between teacher and student outputs." />
<figcaption aria-hidden="true">Figure 7: Collapse study. (left):
evolution of the teacher's target entropy along training epochs;
(right): evolution of KL divergence between teacher and student
outputs.</figcaption>
</figure>
<table>
<caption style="text-align: left;">
Table 8: Time and memory requirements. We show total running time and
peak memory per GPU ("mem.") when running ViT-S/16 DINO models on two
8-GPU machines. We report top-1 ImageNet val acc with linear evaluation
for several variants of multi-crop, each having a different level of
compute requirement.
</caption>
<thead>
<tr>
<th rowspan="2" style="border-top: 2px solid black; border-bottom: 1px solid black;">
multi-crop
</th>
<th colspan="2" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
100 epochs
</th>
<th colspan="2" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
300 epochs
</th>
<th rowspan="2" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
mem.
</th>
</tr>
<tr>
<th style="text-align: center; border-bottom: 1px solid black; background-color: #e6e6e6;">
top-1
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
time
</th>
<th style="text-align: center; border-bottom: 1px solid black; background-color: #e6e6e6;">
top-1
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
time
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">
2 × 224<sup>2</sup>
</td>
<td style="text-align: center; background-color: #e6e6e6;">
67.8
</td>
<td style="text-align: center;">
15.3h
</td>
<td style="text-align: center; background-color: #e6e6e6;">
72.5
</td>
<td style="text-align: center;">
45.9h
</td>
<td style="text-align: center;">
9.3G
</td>
</tr>
<tr>
<td style="text-align: left;">
2 × 224<sup>2</sup> + 2 × 96<sup>2</sup>
</td>
<td style="text-align: center; background-color: #e6e6e6;">
71.5
</td>
<td style="text-align: center;">
17.0h
</td>
<td style="text-align: center; background-color: #e6e6e6;">
74.5
</td>
<td style="text-align: center;">
51.0h
</td>
<td style="text-align: center;">
10.5G
</td>
</tr>
<tr>
<td style="text-align: left;">
2 × 224<sup>2</sup> + 6 × 96<sup>2</sup>
</td>
<td style="text-align: center; background-color: #e6e6e6;">
73.8
</td>
<td style="text-align: center;">
20.3h
</td>
<td style="text-align: center; background-color: #e6e6e6;">
75.9
</td>
<td style="text-align: center;">
60.9h
</td>
<td style="text-align: center;">
12.9G
</td>
</tr>
<tr>
<td style="text-align: left; border-bottom: 2px solid black;">
2 × 224<sup>2</sup> + 10 × 96<sup>2</sup>
</td>
<td style="text-align: center; border-bottom: 2px solid black; background-color: #e6e6e6;">
74.6
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
24.2h
</td>
<td style="text-align: center; border-bottom: 2px solid black; background-color: #e6e6e6;">
76.1
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
72.6h
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
15.4G
</td>
</tr>
</tbody>
</table>
<h3 id="compute-requirements">5.4. Compute requirements</h3>
<h3 id="training-with-small-batches">5.5. Training with small
batches</h3>
<table>
<caption style="text-align: left;">
Table 9: Effect of batch sizes. Top-1 with <span
class="math inline">\(k\)</span>-NN for models trained for 100 epochs
without multi-crop.
</caption>
<thead>
<tr>
<th style="border-top: 2px solid black; border-bottom: 1px solid black;">
bs
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
128
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
256
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
512
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
1024
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="border-bottom: 2px solid black;">
top-1
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
57.9
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
59.1
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
59.6
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
59.9
</td>
</tr>
</tbody>
</table>
<h2 id="appendix">Appendix</h2>
<h3 id="a.-additional-results">A. Additional Results</h3>
<table>
<caption style="text-align: left;">
Table 10: <span class="math inline">\(k\)</span>-NN and linear
evaluation for ViT-S/16 and ResNet-50 pre-trained with DINO. We use
ImageNet-1k [60] ("Inet"), Places205 [84], PASCAL VOC [24] and
Oxford-102 flowers ("FLOWERS") [46]. ViT trained with DINO provides
features that are particularly <span class="math inline">\(k\)</span>-NN
friendly.
</caption>
<thead>
<tr>
<th rowspan="2" style="text-align: left; border-top: 2px solid black; border-bottom: 1px solid black;">
</th>
<th colspan="3" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Logistic
</th>
<th colspan="3" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
<span class="math inline">\(k\)</span>-NN
</th>
</tr>
<tr>
<th style="text-align: center; border-bottom: 1px solid black;">
RN50
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
ViT-S
</th>
<th style="text-align: center; border-bottom: 1px solid black; background-color: #e6e6e6;">
<span class="math inline">\(\Delta\)</span>
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
RN50
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
ViT-S
</th>
<th style="text-align: center; border-bottom: 1px solid black; background-color: #e6e6e6;">
<span class="math inline">\(\Delta\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Inet 100%
</td>
<td style="text-align: center;">
72.1
</td>
<td style="text-align: center;">
75.7
</td>
<td style="text-align: center; background-color: #e6e6e6;">
3.6
</td>
<td style="text-align: center;">
67.5
</td>
<td style="text-align: center;">
74.5
</td>
<td style="text-align: center; background-color: #e6e6e6;">
7.0
</td>
</tr>
<tr>
<td>
Inet 10%
</td>
<td style="text-align: center;">
67.8
</td>
<td style="text-align: center;">
72.2
</td>
<td style="text-align: center; background-color: #e6e6e6;">
4.4
</td>
<td style="text-align: center;">
59.3
</td>
<td style="text-align: center;">
69.1
</td>
<td style="text-align: center; background-color: #e6e6e6;">
9.8
</td>
</tr>
<tr>
<td>
Inet 1%
</td>
<td style="text-align: center;">
55.1
</td>
<td style="text-align: center;">
64.5
</td>
<td style="text-align: center; background-color: #e6e6e6;">
9.4
</td>
<td style="text-align: center;">
47.2
</td>
<td style="text-align: center;">
61.3
</td>
<td style="text-align: center; background-color: #e6e6e6;">
14.1
</td>
</tr>
<tr>
<td>
Pl. 10%
</td>
<td style="text-align: center;">
53.4
</td>
<td style="text-align: center;">
52.1
</td>
<td style="text-align: center; background-color: #e6e6e6;">
-1.3
</td>
<td style="text-align: center;">
46.9
</td>
<td style="text-align: center;">
48.6
</td>
<td style="text-align: center; background-color: #e6e6e6;">
1.7
</td>
</tr>
<tr>
<td>
Pl. 1%
</td>
<td style="text-align: center;">
46.5
</td>
<td style="text-align: center;">
46.3
</td>
<td style="text-align: center; background-color: #e6e6e6;">
-0.2
</td>
<td style="text-align: center;">
39.2
</td>
<td style="text-align: center;">
41.3
</td>
<td style="text-align: center; background-color: #e6e6e6;">
2.1
</td>
</tr>
<tr>
<td>
VOC07
</td>
<td style="text-align: center;">
88.9
</td>
<td style="text-align: center;">
89.2
</td>
<td style="text-align: center; background-color: #e6e6e6;">
0.3
</td>
<td style="text-align: center;">
84.9
</td>
<td style="text-align: center;">
88.0
</td>
<td style="text-align: center; background-color: #e6e6e6;">
3.1
</td>
</tr>
<tr>
<td style="border-bottom: 1px solid black;">
FLOWERS
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
95.6
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
96.4
</td>
<td style="text-align: center; border-bottom: 1px solid black; background-color: #e6e6e6;">
0.8
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
87.9
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
89.1
</td>
<td style="text-align: center; border-bottom: 1px solid black; background-color: #e6e6e6;">
1.2
</td>
</tr>
<tr>
<td style="border-bottom: 2px solid black;">
Average <span class="math inline">\(\Delta\)</span>
</td>
<td style="border-bottom: 2px solid black;">
</td>
<td style="border-bottom: 2px solid black;">
</td>
<td style="text-align: center; border-bottom: 2px solid black; background-color: #e6e6e6;">
2.4
</td>
<td style="border-bottom: 2px solid black;">
</td>
<td style="border-bottom: 2px solid black;">
</td>
<td style="text-align: center; border-bottom: 2px solid black; background-color: #e6e6e6;">
<strong>5.6</strong>
</td>
</tr>
</tbody>
</table>
<table style="width: 100%; border-collapse: collapse;">
<caption style="text-align: left;">
Table 11: ImageNet classification with different pretraining. Top-1
accuracy on ImageNet for supervised ViT-B/16 models using different
pretrainings or using an additional pretrained convnet to guide the
training. The methods use different image resolution ("res.") and
training procedure ("tr. proc."), i.e., data augmentation and
optimization. "MPP" is Masked Patch Prediction.
</caption>
<thead>
<tr>
<th colspan="2" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Pretraining
</th>
<th rowspan="2" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
res.
</th>
<th rowspan="2" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
tr. proc.
</th>
<th rowspan="2" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Top-1
</th>
</tr>
<tr>
<th style="border-bottom: 1px solid black;">
method
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
data
</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="5" style="font-style: italic;">
Pretrain on additional data
</td>
</tr>
<tr>
<td>
MMP
</td>
<td style="text-align: center;">
JFT-300M
</td>
<td style="text-align: center;">
384
</td>
<td style="text-align: center;">
[19]
</td>
<td style="text-align: center;">
79.9
</td>
</tr>
<tr>
<td style="border-bottom: 1px solid black;">
Supervised
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
JFT-300M
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
384
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
[19]
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
84.2
</td>
</tr>
<tr>
<td colspan="5" style="font-style: italic; border-top: 1px solid black;">
Train with additional model
</td>
</tr>
<tr>
<td style="border-bottom: 1px solid black;">
Rand. init.
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
-
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
224
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
[69]
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
83.4
</td>
</tr>
<tr>
<td colspan="5" style="font-style: italic; border-top: 1px solid black;">
No additional data nor model
</td>
</tr>
<tr>
<td>
Rand. init.
</td>
<td style="text-align: center;">
-
</td>
<td style="text-align: center;">
224
</td>
<td style="text-align: center;">
[19]
</td>
<td style="text-align: center;">
77.9
</td>
</tr>
<tr>
<td>
Rand. init.
</td>
<td style="text-align: center;">
-
</td>
<td style="text-align: center;">
224
</td>
<td style="text-align: center;">
[69]
</td>
<td style="text-align: center;">
81.8
</td>
</tr>
<tr>
<td>
Supervised
</td>
<td style="text-align: center;">
ImNet
</td>
<td style="text-align: center;">
224
</td>
<td style="text-align: center;">
[69]
</td>
<td style="text-align: center;">
81.9
</td>
</tr>
<tr style="background-color: #f7e0d5;">
<td style="border-bottom: 2px solid black;">
DINO
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
ImNet
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
224
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
[69]
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
82.8
</td>
</tr>
</tbody>
</table>
<table>
<caption style="text-align: left;">
Table 12: Low-shot learning on ImageNet with frozen ViT features. We
train a logistic regression on frozen features (FROZEN). Note that this
FROZEN evaluation is performed without any fine-tuning nor data
augmentation. We report top-1 accuracy. For reference, we show
previously published results that uses finetuning and semi-supervised
learning.
</caption>
<thead>
<tr>
<th rowspan="2" style="text-align: left; border-top: 2px solid black; border-bottom: 1px solid black;">
Method
</th>
<th rowspan="2" style="text-align: left; border-top: 2px solid black; border-bottom: 1px solid black;">
Arch
</th>
<th rowspan="2" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Param.
</th>
<th colspan="2" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Top 1
</th>
</tr>
<tr>
<th style="text-align: center; border-bottom: 1px solid black;">
1%
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
10%
</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="5" style="font-style: italic;">
Self-supervised pretraining with finetuning
</td>
</tr>
<tr>
<td style="text-align: left;">
UDA [75]
</td>
<td style="text-align: left;">
RN50
</td>
<td style="text-align: center;">
23
</td>
<td style="text-align: center;">
-
</td>
<td style="text-align: center;">
68.1
</td>
</tr>
<tr>
<td style="text-align: left;">
SimCLRv2 [13]
</td>
<td style="text-align: left;">
RN50
</td>
<td style="text-align: center;">
23
</td>
<td style="text-align: center;">
57.9
</td>
<td style="text-align: center;">
68.4
</td>
</tr>
<tr>
<td style="text-align: left;">
BYOL [30]
</td>
<td style="text-align: left;">
RN50
</td>
<td style="text-align: center;">
23
</td>
<td style="text-align: center;">
53.2
</td>
<td style="text-align: center;">
68.8
</td>
</tr>
<tr>
<td style="text-align: left;">
SwAV [10]
</td>
<td style="text-align: left;">
RN50
</td>
<td style="text-align: center;">
23
</td>
<td style="text-align: center;">
53.9
</td>
<td style="text-align: center;">
70.2
</td>
</tr>
<tr>
<td style="text-align: left;">
SimCLRv2 [16]
</td>
<td style="text-align: left;">
RN50w4
</td>
<td style="text-align: center;">
375
</td>
<td style="text-align: center;">
63.0
</td>
<td style="text-align: center;">
74.4
</td>
</tr>
<tr>
<td style="text-align: left; border-bottom: 1px solid black;">
BYOL [30]
</td>
<td style="text-align: left; border-bottom: 1px solid black;">
RN200w2
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
250
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
71.2
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
77.7
</td>
</tr>
<tr>
<td colspan="5" style="font-style: italic; border-top: 1px solid black;">
Semi-supervised methods
</td>
</tr>
<tr>
<td style="text-align: left;">
SimCLRv2+KD [13]
</td>
<td style="text-align: left;">
RN50
</td>
<td style="text-align: center;">
23
</td>
<td style="text-align: center;">
60.0
</td>
<td style="text-align: center;">
70.5
</td>
</tr>
<tr>
<td style="text-align: left;">
SwAV+CT [3]
</td>
<td style="text-align: left;">
RN50
</td>
<td style="text-align: center;">
23
</td>
<td style="text-align: center;">
-
</td>
<td style="text-align: center;">
70.8
</td>
</tr>
<tr>
<td style="text-align: left;">
FixMatch [64]
</td>
<td style="text-align: left;">
RN50
</td>
<td style="text-align: center;">
23
</td>
<td style="text-align: center;">
-
</td>
<td style="text-align: center;">
71.5
</td>
</tr>
<tr>
<td style="text-align: left;">
MPL [49]
</td>
<td style="text-align: left;">
RN50
</td>
<td style="text-align: center;">
23
</td>
<td style="text-align: center;">
-
</td>
<td style="text-align: center;">
73.9
</td>
</tr>
<tr>
<td style="text-align: left; border-bottom: 1px solid black;">
SimCLRv2+KD [13]
</td>
<td style="text-align: left; border-bottom: 1px solid black;">
RN152w3+SK
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
794
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
76.6
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
80.9
</td>
</tr>
<tr>
<td colspan="5" style="font-style: italic; border-top: 1px solid black;">
Frozen self-supervised features
</td>
</tr>
<tr style="background-color: #f7e0d5;">
<td style="text-align: left; border-bottom: 2px solid black;">
DINO <sub>-FROZEN</sub>
</td>
<td style="text-align: left; border-bottom: 2px solid black;">
ViT-S/16
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
21
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
64.5
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
72.2
</td>
</tr>
</tbody>
</table>
<h3 id="b.-methodology-comparison">B. Methodology Comparison</h3>
<table>
<caption style="text-align: left;">
Table 13: Methodology comparison for DEIT-small and ResNet-50. We report
ImageNet linear and <span class="math inline">\(k\)</span>-NN
evaluations validation accuracy after 300 epochs pre-training. All
numbers are run by us and match or outperform published results.
</caption>
<thead>
<tr>
<th rowspan="2" style="border-top: 2px solid black; border-bottom: 1px solid black;">
Method
</th>
<th colspan="2" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
ResNet-50
</th>
<th colspan="2" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
ViT-small
</th>
</tr>
<tr>
<th style="text-align: center; border-bottom: 1px solid black;">
Linear
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
<span class="math inline">\(k\)</span>-NN
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
Linear
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
<span class="math inline">\(k\)</span>-NN
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
MoCo-v2
</td>
<td style="text-align: center;">
71.1
</td>
<td style="text-align: center;">
62.9
</td>
<td style="text-align: center;">
71.6
</td>
<td style="text-align: center;">
62.0
</td>
</tr>
<tr>
<td>
BYOL
</td>
<td style="text-align: center;">
72.7
</td>
<td style="text-align: center;">
65.4
</td>
<td style="text-align: center;">
71.4
</td>
<td style="text-align: center;">
66.6
</td>
</tr>
<tr>
<td style="border-bottom: 1px solid black;">
SwAV
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
74.1
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
65.4
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
71.8
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
64.7
</td>
</tr>
<tr>
<td style="border-bottom: 2px solid black;">
DINO
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
<strong>74.5</strong>
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
<strong>65.6</strong>
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
<strong>76.1</strong>
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
<strong>72.8</strong>
</td>
</tr>
</tbody>
</table>
<figure>
<img
src="https://img.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron/fig8.webp"
alt="Figure 8: Self-attention for a set of reference points. We visualize the self-attention module from the last block of a ViT-S/8 trained with DINO. The network is able to separate objects, though it has been trained with no supervision at all." />
<figcaption aria-hidden="true">Figure 8: Self-attention for a set of
reference points. We visualize the self-attention module from the last
block of a ViT-S/8 trained with DINO. The network is able to separate
objects, though it has been trained with no supervision at
all.</figcaption>
</figure>
<table>
<caption style="text-align: left;">
Table 14: Relation to MoCo-v2 and BYOL. We ablate the components that
differ between DINO, MoCo-v2 and BYOL: the loss function (cross-entropy,
CE, versus InfoNCE, INCE, versus mean-square error, MSE), the multi-crop
training, the centering operator, the batch normalization in the
projection heads and the student predictor. Models are run for 300
epochs with ViT-S/16. We report top-1 accuracy on ImageNet linear
evaluation.
</caption>
<thead>
<tr>
<th style="border-top: 2px solid black; border-bottom: 1px solid black;">
</th>
<th style="border-top: 2px solid black; border-bottom: 1px solid black;">
Method
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Loss
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
multi-crop
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Center.
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
BN
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Pred.
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Top-1
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
DINO
</td>
<td style="text-align: center;">
CE
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
76.1
</td>
</tr>
<tr>
<td>
2
</td>
<td>
-
</td>
<td style="text-align: center;">
MSE
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
62.4
</td>
</tr>
<tr>
<td>
3
</td>
<td>
-
</td>
<td style="text-align: center;">
CE
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
75.6
</td>
</tr>
<tr>
<td style="border-bottom: 1px solid black;">
4
</td>
<td style="border-bottom: 1px solid black;">
-
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
CE
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
✓
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
72.5
</td>
</tr>
<tr>
<td>
5
</td>
<td>
MoCov2
</td>
<td style="text-align: center;">
INCE
</td>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
71.4
</td>
</tr>
<tr>
<td style="border-bottom: 1px solid black;">
6
</td>
<td style="border-bottom: 1px solid black;">
-
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
INCE
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
✓
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
✓
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
73.4
</td>
</tr>
<tr>
<td>
7
</td>
<td>
BYOL
</td>
<td style="text-align: center;">
MSE
</td>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
71.4
</td>
</tr>
<tr>
<td>
8
</td>
<td>
-
</td>
<td style="text-align: center;">
MSE
</td>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
0.1
</td>
</tr>
<tr>
<td>
9
</td>
<td>
-
</td>
<td style="text-align: center;">
MSE
</td>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
52.6
</td>
</tr>
<tr>
<td style="border-bottom: 2px solid black;">
10
</td>
<td style="border-bottom: 2px solid black;">
-
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
MSE
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
✓
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
✓
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
✓
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
64.8
</td>
</tr>
</tbody>
</table>
<table>
<caption style="text-align: left;">
Table 15: Relation to SwAV. We vary the operation on the teacher output
between centering, a softmax applied over the batch dimension and the
Sinkhorn-Knopp algorithm. We also ablate the Momentum encoder by
replacing it with a hard copy of the student with a stop-gradient as in
SwAV. Models are run for 300 epochs with ViT-S/16. We report top-1
accuracy on ImageNet linear evaluation.
</caption>
<thead>
<tr>
<th style="border-top: 2px solid black; border-bottom: 1px solid black;">
</th>
<th style="border-top: 2px solid black; border-bottom: 1px solid black;">
Method
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Momentum
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Operation
</th>
<th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
Top-1
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
DINO
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
Centering
</td>
<td style="text-align: center;">
76.1
</td>
</tr>
<tr>
<td>
2
</td>
<td>
-
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
Softmax (batch)
</td>
<td style="text-align: center;">
75.8
</td>
</tr>
<tr>
<td>
3
</td>
<td>
-
</td>
<td style="text-align: center;">
✓
</td>
<td style="text-align: center;">
Sinkhorn-Knopp
</td>
<td style="text-align: center;">
76.0
</td>
</tr>
<tr>
<td>
4
</td>
<td>
-
</td>
<td style="text-align: center;">
</td>
<td style="text-align: center;">
Centering
</td>
<td style="text-align: center;">
0.1
</td>
</tr>
<tr>
<td>
5
</td>
<td>
-
</td>
<td>
</td>
<td style="text-align: center;">
Softmax (batch)
</td>
<td style="text-align: center;">
72.2
</td>
</tr>
<tr>
<td style="border-bottom: 2px solid black;">
6
</td>
<td style="border-bottom: 2px solid black;">
SwAV
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
Sinkhorn-Knopp
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
71.8
</td>
</tr>
</tbody>
</table>
<div
style="border-top: 2px solid black; border-bottom: 2px solid black; background-color: #f8f9fa;">
<pre style="margin: 0; background-color: #f8f9fa; color: black;"><span style="color: #408080;"># x is n-by-k
# tau is Sinkhorn regularization param</span>
x = exp(x / tau)
for _ in range(num_iters):  <span style="color: #408080;"># 1 iter of Sinkhorn</span>
    <span style="color: #408080;"># total weight per dimension (or cluster)</span>
    c = sum(x, dim=0, keepdim=True)
    x /= c

    <span style="color: #408080;"># total weight per sample</span>
    n = sum(x, dim=1, keepdim=True)
    <span style="color: #408080;"># x sums to 1 for each sample (assignment)</span>
    x /= n</pre>
</div>
<div
style="border-top: 2px solid black; border-bottom: 2px solid black; background-color: #f8f9fa;">
<pre style="margin: 0; background-color: #f8f9fa; color: black;">x = softmax(x / tau, dim=0)
x /= sum(x, dim=1, keepdim=True)</pre>
</div>
<h3 id="c.-projection-head">C. Projection Head</h3>
<table>
<thead>
<tr>
<th style="border-bottom: 1px solid black;">
ViT-S, 100 epochs
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
heads w/o BN
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
heads w/ BN
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<span class="math inline">\(k\)</span>-NN top-1
</td>
<td style="text-align: center; background-color: #f7e0d5;">
69.7
</td>
<td style="text-align: center;">
68.6
</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="border-bottom: 1px solid black;">
# proj. head linear layers
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
1
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
2
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
3
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
4
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
w/ l2-norm bottleneck
</td>
<td style="text-align: center;">
-
</td>
<td style="text-align: center;">
62.2
</td>
<td style="text-align: center;">
68.0
</td>
<td style="text-align: center; background-color: #f7e0d5;">
69.3
</td>
</tr>
<tr>
<td>
w/o l2-norm bottleneck
</td>
<td style="text-align: center;">
61.6
</td>
<td style="text-align: center;">
62.9
</td>
<td style="text-align: center;">
0.1
</td>
<td style="text-align: center;">
0.1
</td>
</tr>
</tbody>
</table>
<figure>
<img
src="https://img.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron/head_design.webp"
alt="Figure 9: Projection head design w/ or w/o l2-norm bottleneck." />
<figcaption aria-hidden="true">Figure 9: Projection head design w/ or
w/o l2-norm bottleneck.</figcaption>
</figure>
<table>
<thead>
<tr>
<th style="border-bottom: 1px solid black;">
<span class="math inline">\(K\)</span>
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
1024
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
4096
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
16384
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
65536
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
262144
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<span class="math inline">\(k\)</span>-NN top-1
</td>
<td style="text-align: center;">
67.8
</td>
<td style="text-align: center;">
69.3
</td>
<td style="text-align: center;">
69.2
</td>
<td style="text-align: center; background-color: #f7e0d5;">
69.7
</td>
<td style="text-align: center;">
69.1
</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="border-bottom: 1px solid black;">
ViT-S, 100 epochs
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
heads w/ GELU
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
heads w/ ReLU
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<span class="math inline">\(k\)</span>-NN top-1
</td>
<td style="text-align: center; background-color: #f7e0d5;">
69.7
</td>
<td style="text-align: center;">
68.9
</td>
</tr>
</tbody>
</table>
<h3 id="d.-additional-ablations">D. Additional Ablations</h3>
<table>
<thead>
<tr>
<th style="border-bottom: 1px solid black;">
<span class="math inline">\(m\)</span>
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
0
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
0.9
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
0.99
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
0.999
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<span class="math inline">\(k\)</span>-NN top-1
</td>
<td style="text-align: center;">
69.1
</td>
<td style="text-align: center; background-color: #f7e0d5;">
69.7
</td>
<td style="text-align: center;">
69.4
</td>
<td style="text-align: center;">
0.1
</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="border-bottom: 1px solid black;">
<span class="math inline">\(\tau_t\)</span>
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
0
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
0.02
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
0.04
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
0.06
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
0.08
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
0.04 <span class="math inline">\(\to\)</span> 0.07
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<span class="math inline">\(k\)</span>-NN top-1
</td>
<td style="text-align: center;">
43.9
</td>
<td style="text-align: center;">
66.7
</td>
<td style="text-align: center;">
69.6
</td>
<td style="text-align: center;">
68.7
</td>
<td style="text-align: center;">
0.1
</td>
<td style="text-align: center; background-color: #f7e0d5;">
69.7
</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="border-bottom: 1px solid black;">
DINO ViT-S
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
100-ep
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
300-ep
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
800-ep
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<span class="math inline">\(k\)</span>-NN top-1
</td>
<td style="text-align: center;">
70.9
</td>
<td style="text-align: center;">
72.8
</td>
<td style="text-align: center; background-color: #f7e0d5;">
74.5
</td>
</tr>
</tbody>
</table>
<p><img
src="https://img.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron/figure_mom_rn50.webp" /></p>
<table>
<thead>
<tr>
<th colspan="2" style="border-top: 2px solid black; border-bottom: 1px solid black;">
ViT-S/16 weights
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Random weights
</td>
<td style="text-align: center;">
22.0
</td>
</tr>
<tr>
<td style="border-bottom: 1px solid black;">
Supervised
</td>
<td style="text-align: center; border-bottom: 1px solid black;">
27.3
</td>
</tr>
<tr>
<td>
DINO
</td>
<td style="text-align: center;">
45.9
</td>
</tr>
<tr>
<td>
DINO w/o multicrop
</td>
<td style="text-align: center;">
45.1
</td>
</tr>
<tr>
<td>
MoCo-v2
</td>
<td style="text-align: center;">
46.3
</td>
</tr>
<tr>
<td>
BYOL
</td>
<td style="text-align: center;">
47.8
</td>
</tr>
<tr>
<td style="border-bottom: 2px solid black;">
SwAV
</td>
<td style="text-align: center; border-bottom: 2px solid black;">
46.8
</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="border-bottom: 1px solid black;">
# heads
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
dim
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
dim/head
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
# params
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
im/sec
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
<span class="math inline">\(k\)</span>-NN
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="background-color: #f7e0d5;">
6
</td>
<td style="text-align: center; background-color: #f7e0d5;">
384
</td>
<td style="text-align: center; background-color: #f7e0d5;">
64
</td>
<td style="text-align: center; background-color: #f7e0d5;">
21
</td>
<td style="text-align: center; background-color: #f7e0d5;">
1007
</td>
<td style="text-align: center; background-color: #f7e0d5;">
72.8
</td>
</tr>
<tr>
<td>
8
</td>
<td style="text-align: center;">
384
</td>
<td style="text-align: center;">
48
</td>
<td style="text-align: center;">
21
</td>
<td style="text-align: center;">
971
</td>
<td style="text-align: center;">
73.1
</td>
</tr>
<tr>
<td>
12
</td>
<td style="text-align: center;">
384
</td>
<td style="text-align: center;">
32
</td>
<td style="text-align: center;">
21
</td>
<td style="text-align: center;">
927
</td>
<td style="text-align: center;">
73.7
</td>
</tr>
<tr>
<td>
16
</td>
<td style="text-align: center;">
384
</td>
<td style="text-align: center;">
24
</td>
<td style="text-align: center;">
21
</td>
<td style="text-align: center;">
860
</td>
<td style="text-align: center;">
73.8
</td>
</tr>
</tbody>
</table>
<h3 id="e.-multi-crop">E. Multi-crop</h3>
<table>
<thead>
<tr>
<th style="border-bottom: 1px solid black;">
(0.05, s), (s, 1), s:
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
0.08
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
0.16
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
0.24
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
0.32
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
0.48
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<span class="math inline">\(k\)</span>-NN top-1
</td>
<td style="text-align: center;">
65.6
</td>
<td style="text-align: center;">
68.0
</td>
<td style="text-align: center;">
69.7
</td>
<td style="text-align: center;">
69.8
</td>
<td style="text-align: center;">
69.5
</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="border-top: 2px solid black;">
crops
</th>
<th colspan="2" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
2 × 224<sup>2</sup>
</th>
<th colspan="2" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">
2 × 224<sup>2</sup> + 6 × 96<sup>2
</th>
</tr>
<tr>
<th style="border-bottom: 1px solid black;">
eval
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
<span class="math inline">\(k\)</span>-NN
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
linear
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
<span class="math inline">\(k\)</span>-NN
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
linear
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
BYOL
</td>
<td style="text-align: center;">
66.6
</td>
<td style="text-align: center;">
71.4
</td>
<td style="text-align: center;">
59.8
</td>
<td style="text-align: center;">
64.8
</td>
</tr>
<tr>
<td>
SwAV
</td>
<td style="text-align: center;">
60.5
</td>
<td style="text-align: center;">
68.5
</td>
<td style="text-align: center;">
64.7
</td>
<td style="text-align: center;">
71.8
</td>
</tr>
<tr>
<td>
MoCo-v2
</td>
<td style="text-align: center;">
62.0
</td>
<td style="text-align: center;">
71.6
</td>
<td style="text-align: center;">
65.4
</td>
<td style="text-align: center;">
73.4
</td>
</tr>
<tr>
<td style="background-color: #f7e0d5; border-bottom: 2px solid black;">
DINO
</td>
<td style="text-align: center; background-color: #f7e0d5; border-bottom: 2px solid black;">
<strong>67.9</strong>
</td>
<td style="text-align: center; background-color: #f7e0d5; border-bottom: 2px solid black;">
<strong>72.5</strong>
</td>
<td style="text-align: center; background-color: #f7e0d5; border-bottom: 2px solid black;">
<strong>72.7</strong>
</td>
<td style="text-align: center; background-color: #f7e0d5; border-bottom: 2px solid black;">
<strong>75.9</strong>
</td>
</tr>
</tbody>
</table>
<p><img
src="https://img.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron/figure_byol.webp" /></p>
<h3 id="f.-evaluation-protocols">F. Evaluation Protocols</h3>
<h4 id="f.1-k-nn-classification">F.1 <span
class="math inline">\(k\)</span>-NN classification</h4>
<h4 id="f.2-linear-classification">F.2 Linear classification</h4>
<table>
<thead>
<tr>
<th style="border-bottom: 1px solid black;">
concatenate <span class="math inline">\(l\)</span> last layers
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
1
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
2
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
4
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
6
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
representation dim
</td>
<td style="text-align: center;">
384
</td>
<td style="text-align: center;">
768
</td>
<td style="text-align: center;">
1536
</td>
<td style="text-align: center;">
2304
</td>
</tr>
<tr>
<td>
ViT-S/16 linear eval
</td>
<td style="text-align: center;">
76.1
</td>
<td style="text-align: center;">
76.6
</td>
<td style="text-align: center; background-color: #f7e0d5;">
77.0
</td>
<td style="text-align: center;">
77.0
</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="border-bottom: 1px solid black;">
pooling strategy
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
[CLS] tok. only
</th>
<th style="text-align: center; border-bottom: 1px solid black;">
concatenate [CLS] tok. and avgpooled patch tok.
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
representation dim
</td>
<td style="text-align: center;">
768
</td>
<td style="text-align: center;">
1536
</td>
</tr>
<tr>
<td>
ViT-B/16 linear eval
</td>
<td style="text-align: center;">
78.0
</td>
<td style="text-align: center; background-color: #f7e0d5;">
78.2
</td>
</tr>
</tbody>
</table>
<h3 id="g.-self-attention-visualizations">G. Self-Attention
Visualizations</h3>
<h3 id="h.-class-representation">H. Class Representation</h3>
<figure>
<img
src="https://img.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron/fig10.webp"
alt="Figure 10: Self-attention heads from the last layer. We look at the attention map when using the [CLS] token as a query for the different heads in the last layer. Note that the [CLS] token is not attached to any label or supervision." />
<figcaption aria-hidden="true">Figure 10: Self-attention heads from the
last layer. We look at the attention map when using the [CLS] token as a
query for the different heads in the last layer. Note that the [CLS]
token is not attached to any label or supervision.</figcaption>
</figure>
<figure>
<img
src="https://img.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron/tsne-classes.webp"
alt="Figure 11: t-SNE visualization of ImageNet classes as represented using DINO. For each class, we obtain the embedding by taking the average feature for all images of that class in the validation set." />
<figcaption aria-hidden="true">Figure 11: t-SNE visualization of
ImageNet classes as represented using DINO. For each class, we obtain
the embedding by taking the average feature for all images of that class
in the validation set.</figcaption>
</figure>
<div class="pdf-container" data-target="https://arxiv.org/pdf/2104.14294" data-height="500px"></div>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>请我一杯咖啡吧！</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="https://img.032802.xyz/alipay.webp" alt="Karl 微信">
        <span>微信</span>
      </div>
      <div>
        <img src="https://img.032802.xyz/alipay.webp" alt="Karl 支付宝">
        <span>支付宝</span>
      </div>

  </div>
</div>

          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Karl
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://blog.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron.html" title="【论文笔记】Emerging Properties in Self-Supervised Vision Transformers">https://blog.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron.html</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpoLWhhbnM="><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/2021ICCV/" rel="tag"><i class="fa fa-tag"></i> 2021ICCV</a>
              <a href="/tags/Attention/" rel="tag"><i class="fa fa-tag"></i> Attention</a>
              <a href="/tags/ViT/" rel="tag"><i class="fa fa-tag"></i> ViT</a>
              <a href="/tags/DINO/" rel="tag"><i class="fa fa-tag"></i> DINO</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/paper-reading/2020/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale_2021_Dosovitskiy.html" rel="prev" title="【论文笔记】An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale">
                  <i class="fa fa-angle-left"></i> 【论文笔记】An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/installation/build-a-chain-proxy.html" rel="next" title="搭建链式代理">
                  搭建链式代理 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Karl</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">174k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">10:33</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZw==">NexT.Gemini</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.3.0/pdfobject.min.js","integrity":"sha256-JJZNsid68vnh3/zyj0lY9BN5ynxVX/12XgOa1TlaYN0="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.min.js","integrity":"sha256-G8ouPAnw4zzMbnAenHnVz6h9XpKbNdOkrqTh7AadyHs="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>

  <script class="next-config" data-name="wavedrom" type="application/json">{"enable":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/wavedrom/3.5.0/wavedrom.min.js","integrity":"sha256-INLAoJc6quTNfiMWkGZniYO2cxE8mHpddnLow1m6RFs="}}</script>
  <script class="next-config" data-name="wavedrom_skin" type="application/json">{"enable":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/wavedrom/3.5.0/skins/default.js","integrity":"sha256-fduc/Zszk5ezWws2uInY/ALWVmIrmV6VTgXbsYSReFI="}}</script>
  <script src="/js/third-party/tags/wavedrom.js"></script>

  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://blog.032802.xyz/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron.html"}</script>
  <script src="/js/third-party/quicklink.js"></script>
<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-CN","enable":true,"serverURL":"https://waline.032802.xyz","cssUrl":"https://unpkg.com/@waline/client@v3/dist/waline.css","commentCount":true,"pageview":false,"locale":{"placeholder":"请畅所欲言！"},"emoji":["https://unpkg.com/@waline/emojis@1.2.0/bmoji","https://unpkg.com/@waline/emojis@1.2.0/qq","https://unpkg.com/@waline/emojis@1.2.0/weibo","https://unpkg.com/@waline/emojis@1.2.0/bilibili","https://unpkg.com/@waline/emojis@1.2.0/alus","https://unpkg.com/@waline/emojis@1.2.0/tw-emoji","https://unpkg.com/@waline/emojis@1.2.0/tw-body","https://unpkg.com/@waline/emojis@1.2.0/tw-food","https://unpkg.com/@waline/emojis@1.2.0/tw-natural","https://unpkg.com/@waline/emojis@1.2.0/tw-object","https://unpkg.com/@waline/emojis@1.2.0/tw-symbol","https://unpkg.com/@waline/emojis@1.2.0/tw-people","https://unpkg.com/@waline/emojis@1.2.0/tw-sport","https://unpkg.com/@waline/emojis@1.2.0/tw-time","https://unpkg.com/@waline/emojis@1.2.0/tw-travel","https://unpkg.com/@waline/emojis@1.2.0/tw-weather","https://unpkg.com/@waline/emojis@1.2.0/tw-flag"],"meta":["nick","mail","link"],"requiredMeta":["nick","mail"],"login":"disable","pageSize":10,"el":"#waline","comment":true,"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","path":"/paper-reading/2021/emerging-properties-in-self-supervised-vision-transformers_2021_Caron.html"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v3/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>

</body>
</html>
