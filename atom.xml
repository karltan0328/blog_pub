<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Karl的博客</title>
  
  
  <link href="https://blog.032802.xyz/atom.xml" rel="self"/>
  
  <link href="https://blog.032802.xyz/"/>
  <updated>2025-05-31T14:03:17.000Z</updated>
  <id>https://blog.032802.xyz/</id>
  
  <author>
    <name>Karl</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【代码复现】UNOPose: Unseen Object Pose Estimation with  an Unposed RGB-D Reference Image</title>
    <link href="https://blog.032802.xyz/code-running/shanice-l_UNOPose.html"/>
    <id>https://blog.032802.xyz/code-running/shanice-l_UNOPose.html</id>
    <published>2025-05-31T14:03:17.000Z</published>
    <updated>2025-05-31T14:03:17.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装unopose环境">安装UNOPose环境</h2><ol type="1"><li><p>修改<code>requirements.txt</code>文件：</p><ol type="1"><li>第59行：注释<code>torch==2.2.0+cu118</code>；</li><li>第61行：注释<code>torchvision==0.17.0+cu118</code>。</li></ol></li><li><p>创建环境：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create --name unopose python=3.10.12</span><br></pre></td></tr></table></figure></p></li><li><p>激活环境：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate unopose</span><br></pre></td></tr></table></figure></p></li><li><p>安装依赖：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pip3 install torch==2.0.0 torchvision==0.15.0 --index-url https://download.pytorch.org/whl/cu117</span><br><span class="line">pip3 install mmcv==2.2.0 -f https://download.openmmlab.com/mmcv/dist/cu117/torch2.0/index.html</span><br><span class="line"></span><br><span class="line">pip3 install -r requirements.txt</span><br><span class="line"></span><br><span class="line">python -m pip install <span class="string">&#x27;git+https://github.com/facebookresearch/detectron2.git&#x27;</span></span><br></pre></td></tr></table></figure></p></li><li><p>安装bop_toolkit：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> third_party/bop_toolkit</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure></p></li><li><p>安装pointnet2：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> core/unopose/model/pointnet2/</span><br><span class="line">pip3 install -e .</span><br></pre></td></tr></table></figure></p></li></ol><h2 id="下载数据集">下载数据集</h2><ol type="1"><li><p>在代码对应的<span class="exturl" data-url="aHR0cHM6Ly9odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9zaGFuaWNlLWwvVU5PUG9zZV9kYXRhL3RyZWUvbWFpbg==">HuggingFace🤗数据集仓库<i class="fa fa-external-link-alt"></i></span>中下载数据集配对文件：</p><ul><li><span class="exturl" data-url="aHR0cHM6Ly9odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9zaGFuaWNlLWwvVU5PUG9zZV9kYXRhL3Jlc29sdmUvbWFpbi9DdXN0b21TYW1BdXRvbWF0aWNNYXNrR2VuZXJhdG9yT25lcmVmVGFyZ2V0c0Nyb3Nzc2NlbmVSb3Q1MFJlZnZpc2liX3ljYnYtdGVzdC5qc29u">https://huggingface.co/datasets/shanice-l/UNOPose_data/resolve/main/CustomSamAutomaticMaskGeneratorOnerefTargetsCrosssceneRot50Refvisib_ycbv-test.json<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9zaGFuaWNlLWwvVU5PUG9zZV9kYXRhL3Jlc29sdmUvbWFpbi9DdXN0b21TYW1BdXRvbWF0aWNNYXNrR2VuZXJhdG9yX3Rlc3Rfb25lcmVmX3RhcmdldHNfY3Jvc3NzY2VuZV9yb3Q1MF9yZWZ2aXNpYl95Y2J2Lmpzb24=">https://huggingface.co/datasets/shanice-l/UNOPose_data/resolve/main/CustomSamAutomaticMaskGenerator_test_oneref_targets_crossscene_rot50_refvisib_ycbv.json<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9zaGFuaWNlLWwvVU5PUG9zZV9kYXRhL3Jlc29sdmUvbWFpbi9tZWdhcG9zZV9nc29fZml4ZWRfb2JqX2lkX3RvX3Zpc2liMF84X3NjZW5lX2ltX2luc3RfaWRzLmpzb24=">https://huggingface.co/datasets/shanice-l/UNOPose_data/resolve/main/megapose_gso_fixed_obj_id_to_visib0_8_scene_im_inst_ids.json<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9zaGFuaWNlLWwvVU5PUG9zZV9kYXRhL3Jlc29sdmUvbWFpbi9tZWdhcG9zZV9nc29fZml4ZWRfdmFsaWRfaW5zdF9pZHMuanNvbg==">https://huggingface.co/datasets/shanice-l/UNOPose_data/resolve/main/megapose_gso_fixed_valid_inst_ids.json<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9zaGFuaWNlLWwvVU5PUG9zZV9kYXRhL3Jlc29sdmUvbWFpbi9tZWdhcG9zZV9zaGFwZW5ldGNvcmVfZml4ZWRfb2JqX2lkX3RvX3Zpc2liMF84X3NjZW5lX2ltX2luc3RfaWRzLmpzb24=">https://huggingface.co/datasets/shanice-l/UNOPose_data/resolve/main/megapose_shapenetcore_fixed_obj_id_to_visib0_8_scene_im_inst_ids.json<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9zaGFuaWNlLWwvVU5PUG9zZV9kYXRhL3Jlc29sdmUvbWFpbi9tZWdhcG9zZV9zaGFwZW5ldGNvcmVfZml4ZWRfdmFsaWRfaW5zdF9pZHMuanNvbg==">https://huggingface.co/datasets/shanice-l/UNOPose_data/resolve/main/megapose_shapenetcore_fixed_valid_inst_ids.json<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9zaGFuaWNlLWwvVU5PUG9zZV9kYXRhL3Jlc29sdmUvbWFpbi90ZXN0X3JlZl90YXJnZXRzX2Nyb3Nzc2NlbmVfcm90NTAuanNvbg==">https://huggingface.co/datasets/shanice-l/UNOPose_data/resolve/main/test_ref_targets_crossscene_rot50.json<i class="fa fa-external-link-alt"></i></span></li></ul></li><li><p>下载YCB-V数据集的部分数据：</p><ul><li><span class="exturl" data-url="aHR0cHM6Ly9odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9ib3AtYmVuY2htYXJrL3ljYnYvcmVzb2x2ZS9tYWluL3ljYnZfdGVzdF9hbGwuemlw">https://huggingface.co/datasets/bop-benchmark/ycbv/resolve/main/ycbv_test_all.zip<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9ib3AtYmVuY2htYXJrL3ljYnYvcmVzb2x2ZS9tYWluL3ljYnZfbW9kZWxzLnppcA==">https://huggingface.co/datasets/bop-benchmark/ycbv/resolve/main/ycbv_models.zip<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9ib3AtYmVuY2htYXJrL3ljYnYvcmVzb2x2ZS9tYWluL3ljYnZfYmFzZS56aXA=">https://huggingface.co/datasets/bop-benchmark/ycbv/resolve/main/ycbv_base.zip<i class="fa fa-external-link-alt"></i></span></li></ul></li></ol><h2 id="下载预训练模型">下载预训练模型</h2><p>执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python core/unopose/scripts/download_and_save_dinov2_ckpt.py</span><br></pre></td></tr></table></figure><p>下载的预训练模型如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">ls</span> checkpoints/</span><br><span class="line">timm_vit_base_patch14_reg4_dinov2_lvd142m.pth</span><br></pre></td></tr></table></figure><h2 id="运行代码">运行代码</h2><p>解压<code>ycbv_test_all.zip</code>，并将上述下载的7个配对文件组织如下：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">datasets</span><br><span class="line">├── BOP_DATASETS</span><br><span class="line">│   └── ycbv</span><br><span class="line">│       ├── models_eval -&gt; /data1/dataset/unopose/ycbv/models_eval/</span><br><span class="line">│       ├── test -&gt; /data1/dataset/unopose/ycbv/test/</span><br><span class="line">│       ├── test_ref_targets_crossscene_rot50.json -&gt; /data1/dataset/unopose/test_ref_targets_crossscene_rot50.json</span><br><span class="line">│       └── test_targets_bop19.json -&gt; /data1/dataset/unopose/ycbv/ycbv/test_targets_bop19.json</span><br><span class="line">├── MegaPose-Training-Data</span><br><span class="line">│   ├── megapose_gso_fixed_obj_id_to_visib0_8_scene_im_inst_ids.json -&gt; /data1/dataset/unopose/megapose_gso_fixed_obj_id_to_visib0_8_scene_im_inst_ids.json</span><br><span class="line">│   ├── megapose_gso_fixed_valid_inst_ids.json -&gt; /data1/dataset/unopose/megapose_gso_fixed_valid_inst_ids.json</span><br><span class="line">│   ├── megapose_shapenetcore_fixed_obj_id_to_visib0_8_scene_im_inst_ids.json -&gt; /data1/dataset/unopose/megapose_shapenetcore_fixed_obj_id_to_visib0_8_scene_im_inst_ids.json</span><br><span class="line">│   └── megapose_shapenetcore_fixed_valid_inst_ids.json -&gt; /data1/dataset/unopose/megapose_shapenetcore_fixed_valid_inst_ids.json</span><br><span class="line">└── segmentation</span><br><span class="line">    └── CustomSamAutomaticMaskGenerator_test_oneref_targets_crossscene_rot50_refvisib_ycbv.json -&gt; /data1/dataset/unopose/CustomSamAutomaticMaskGenerator_test_oneref_targets_crossscene_rot50_refvisib_ycbv.json</span><br><span class="line"></span><br><span class="line">6 directories, 7 files</span><br></pre></td></tr></table></figure><p>执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./core/unopose/save_unopose.sh configs/main_cfg.py &lt;gpu_ids&gt; checkpoints/timm_vit_base_patch14_reg4_dinov2_lvd142m.pth</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">复现UNOPose的代码，并进行测试。</summary>
    
    
    
    <category term="代码复现" scheme="https://blog.032802.xyz/categories/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/"/>
    
    
    <category term="Object Pose Estimation" scheme="https://blog.032802.xyz/tags/Object-Pose-Estimation/"/>
    
    <category term="代码复现" scheme="https://blog.032802.xyz/tags/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/"/>
    
    <category term="2025CVPR" scheme="https://blog.032802.xyz/tags/2025CVPR/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】UNOPose: Unseen Object Pose Estimation with  an Unposed RGB-D Reference Image</title>
    <link href="https://blog.032802.xyz/paper-reading/2024/unopose-unseen-object-pose-estimation-with-an-unposed-rgb-d-reference-image_2025_Liu.html"/>
    <id>https://blog.032802.xyz/paper-reading/2024/unopose-unseen-object-pose-estimation-with-an-unposed-rgb-d-reference-image_2025_Liu.html</id>
    <published>2025-05-19T02:20:35.000Z</published>
    <updated>2025-05-19T02:20:35.000Z</updated>
    
    <content type="html"><![CDATA[<h1id="unopose-unseen-object-pose-estimation-with-an-unposed-rgb-d-reference-image">UNOPose:Unseen Object Pose Estimation with an Unposed RGB-D Reference Image</h1><table><colgroup><col style="width: 10%" /><col style="width: 9%" /><col style="width: 12%" /><col style="width: 12%" /><col style="width: 43%" /><col style="width: 12%" /></colgroup><thead><tr><th style="text-align: center;">方法</th><th style="text-align: center;">类型</th><th style="text-align: center;">训练输入</th><th style="text-align: center;">推理输入</th><th style="text-align: center;">输出</th><th style="text-align: center;">pipeline</th></tr></thead><tbody><tr><td style="text-align: center;">UNOPose</td><td style="text-align: center;">任意级</td><td style="text-align: center;">RGBDs</td><td style="text-align: center;">RGBDs</td><td style="text-align: center;">相对<spanclass="math inline">\(\mathbf{R}, \mathbf{t}\)</span></td><td style="text-align: center;"></td></tr></tbody></table><ul><li>2025.05.21：两阶段方法，估计两RGBD图像之间的相对位姿。粗略阶段使用GeoTransformer和DINOv2分别提取点云特征和RGB特征，特征拼接后计算相关矩阵，根据矩阵得到若干匹配点对，从这些匹配点对中选择三个可生成一个刚体变换，即位姿假设，使用距离的倒数进行评分，将评分最高的设置为初始位姿；精细阶段在粗略阶段的基础上增加了局部点云信息和位置编码信息，同样得到相关矩阵后使用加权SVD计算位姿。</li></ul><h2 id="abstract">Abstract</h2><h2 id="introduction">1. Introduction</h2><figure><imgsrc="https://img.032802.xyz/paper-reading/2024/unopose-unseen-object-pose-estimation-with-an-unposed-rgb-d-reference-image_2025_Liu/teaser.webp"alt="Figure 1. Illustration of unseen object pose estimation. Given a query image presenting a target object unseen during training, we aim to estimate its segmentation and 6DoF pose w.r.t. a reference frame. While previous methods [43, 57, 77, 90] often rely on the CAD model or multiple RGB(-D) images for reference, we merely use one unposed RGB-D reference image." /><figcaption aria-hidden="true">Figure 1. Illustration of unseen objectpose estimation. Given a query image presenting a target object unseenduring training, we aim to estimate its segmentation and 6DoF posew.r.t. a reference frame. While previous methods [43, 57, 77, 90] oftenrely on the CAD model or multiple RGB(-D) images for reference, wemerely use one unposed RGB-D reference image.</figcaption></figure><p>Our contributions can be summarized as follows:</p><ul><li>To the best of our knowledge, we are the first to conduct unseenobject 6DoF pose estimation leveraging a single unposed RGB-Dreference.</li><li>Based on the BOP Challenge, we devise a new extensive benchmarktailored for unseen object segmentation and pose estimation with onereference. Additionally, we evaluate several traditional andlearning-based methods on this benchmark for completeness.</li><li>We introduce UNOPose, a network for learning relative transformationbetween reference and query objects. To achieve this, we propose the<span class="math inline">\(SE(3)\)</span>-invariant global and localreference frames, enabling standardized object representations despitevariations in pose and size. Furthermore, the network can automaticallyadjust the confidence of each correspondence by incorporating an overlappredictor.</li></ul><h2 id="related-work">2. Related Work</h2><h2 id="uno-object-segmentation-and-pose-estimation">3. UNO ObjectSegmentation and Pose Estimation</h2><h3 id="problem-formulation">3.1. Problem Formulation</h3><p>假设查询图像中存在一个任意的、未见过的刚性物体，我们的目标是利用一张展示目标物体且无严重遮挡或截断的带掩码RGBD参考图像，估计该物体的掩码<spanclass="math inline">\(M_q\)</span>及其6D相对位姿<spanclass="math inline">\(\Delta \mathbf{T} \inSE(3)\)</span>。如图1所示，输入包括：</p><ol type="1"><li><span class="math inline">\([I_q | D_q] \in \mathbb{R}^{H \times W\times 4}\)</span>：查询RGBD图像；</li><li><span class="math inline">\([I_p | D_p] \in \mathbb{R}^{H \times W\times 4}\)</span> 和 <span class="math inline">\(M_p \in \mathbb{R}^{H\times W}\)</span>：参考RGBD图像及指示目标物体的对应二值掩码；</li><li><span class="math inline">\(K_q \in \mathbb{R}^{3 \times 3}\)</span>和 <span class="math inline">\(K_p \in \mathbb{R}^{3 \times3}\)</span>：查询图像与参考图像的相机内参。</li></ol><p>可选地，如果参考物体在相机坐标系中的位姿<spanclass="math inline">\(\mathbf{T}_p \inSE(3)\)</span>已知，则查询物体的位姿<spanclass="math inline">\(\mathbf{T}_q \in SE(3)\)</span>可通过<spanclass="math inline">\(\mathbf{T}_q = \Delta \mathbf{T}\mathbf{T}_p\)</span>恢复。需要注意的是，从实用性角度出发，该方法不应依赖参考物体的绝对位姿，因为世界坐标系在不同应用中可能任意变化。我们仅在推理阶段对标准物体位姿数据集进行评估时使用参考物体的位姿。</p><h3 id="uno-object-segmentation">3.2. UNO Object Segmentation</h3><p>首先，我们需要从杂乱的背景中分割出查询图像中的目标物体。得益于视觉基础模型强大的泛化能力，像“ZeroPose:CAD-Prompted Zero-shot Object 6D Pose Estimation in ClutteredScenes”、“SAM-6D: Segment Anything Model Meets Zero-Shot 6D Object PoseEstimation”和“CNOS: A Strong Baseline for CAD-based Novel ObjectSegmentation”等方法能够利用CAD模型对新物体进行有效分割。然而，与以往从多个渲染视图生成多样描述符的方法不同，我们仅能获取单张参考图像。为应对这一挑战，我们采用SAM模型[40]从查询图像中预测所有可能的掩码提议，然后通过余弦相似度比较查询图像与参考图像的DINOv2[65]描述符，对每个掩码提议进行评分，从而筛选出最相似的掩码<spanclass="math inline">\(M_q\)</span>。关于UNO分割的更多细节请参见附录。</p><h3 id="uno-object-pose-estimation">3.3. UNO Object Pose Estimation</h3><h4 id="overview-of-unopose">3.3.1. Overview of UNOPose</h4><p>给定查询图像的预测掩码<spanclass="math inline">\(M_q\)</span>和参考图像的掩码<spanclass="math inline">\(M_p\)</span>，我们从深度图<spanclass="math inline">\(D_q\)</span>和<spanclass="math inline">\(D_p\)</span>中裁剪出目标物体，并将其反投影到相机空间，得到两个点集<spanclass="math inline">\(\mathbf{Q}_{cam} \in \mathbb{R}^{N^Q \times3}\)</span>和<span class="math inline">\(\mathbf{P}_{cam} \in\mathbb{R}^{N^P \times 3}\)</span>，其中<spanclass="math inline">\(N^Q\)</span>和<spanclass="math inline">\(N^P\)</span>分别表示查询点云和参考点云的点数。我们的目标是通过最小化对应点距离来恢复相对变换<spanclass="math inline">\(\Delta \mathbf{T} = \{\Delta \mathbf{R}, \Delta\mathbf{t}\}\)</span></p><p><span class="math display">\[\begin{equation}\label{eq1}    \min\sum_{(\mathbf{q}, \mathbf{p}) \in \mathbf{C}}\Vert\Delta\mathbf{R}\mathbf{q} + \Delta\mathbf{t} - \mathbf{p}\Vert_2,\end{equation}\]</span></p><p>其中<span class="math inline">\(\mathbf{C}\)</span>是预测的<spanclass="math inline">\(\mathbf{Q}_{cam}\)</span>与<spanclass="math inline">\(\mathbf{P}_{cam}\)</span>之间的对应点集。我们利用颜色和几何线索构建该对应关系，并遵循点云配准中广泛使用的由粗到细范式来求解公式<spanclass="math inline">\(\eqref{eq1}\)</span>。网络结构如图2所示。</p><figure><imgsrc="https://img.032802.xyz/paper-reading/2024/unopose-unseen-object-pose-estimation-with-an-unposed-rgb-d-reference-image_2025_Liu/network_v1.webp"alt="Figure 2. The network architecture of UNOPose. Given the query and reference point clouds \mathbf{Q}_{cam} and \mathbf{P}_{cam} in the camera frame, UNOPose first transforms them into the SE(3)-invariant global reference frame (GRF). Then feature descriptors are extracted from sparse point sets for constructing the coarse correlation matrix. For achieving precise correspondences, the fine pose estimation module exploits structural details using positional encoding and local reference frame (LRF) encoding." /><figcaption aria-hidden="true">Figure 2. The network architecture ofUNOPose. Given the query and reference point clouds <spanclass="math inline">\(\mathbf{Q}_{cam}\)</span> and <spanclass="math inline">\(\mathbf{P}_{cam}\)</span> in the camera frame,UNOPose first transforms them into the <spanclass="math inline">\(SE(3)\)</span>-invariant global reference frame(GRF). Then feature descriptors are extracted from sparse point sets forconstructing the coarse correlation matrix. For achieving precisecorrespondences, the fine pose estimation module exploits structuraldetails using positional encoding and local reference frame (LRF)encoding.</figcaption></figure><h4 id="coarse-to-fine-pose-estimation">3.3.2. Coarse-to-fine PoseEstimation</h4><p><strong>Constructing a Pose-invariant ReferenceFrame.</strong>：仅给定一张未标定位姿的参考图像时，待预测的相对位姿在<spanclass="math inline">\(SE(3)\)</span>空间中具有任意性，这对实现鲁棒的对应关系构成了重大挑战。因此，我们引入一个位姿无关全局参考框架（GRF），并将<spanclass="math inline">\(\mathbf{Q}_{cam}\)</span>和<spanclass="math inline">\(\mathbf{P}_{cam}\)</span>转换到GRF中，得到<spanclass="math inline">\(\mathbf{Q}_G\)</span>和<spanclass="math inline">\(\mathbf{P}_G\)</span>。</p><p>具体来说，以<spanclass="math inline">\(\mathbf{Q}_{cam}\)</span>为例，将点云转换为全局参考框架（GRF）涉及7D坐标变换<spanclass="math inline">\(\{\mathbf{R}_G \in SO(3), \mathbf{t}_G \in\mathbb{R}^3, s_G \in \mathbb{R}\}\)</span>：</p><p><span class="math display">\[\begin{equation}\label{eq2}    \mathbf{Q}_G = \{\frac{\mathbf{R}_G^T(\mathbf{q} -\mathbf{t}_G)}{s_G} \mid \mathbf{q} \in \mathbf{Q}_{cam}\}.\end{equation}\]</span></p><p>为实现平移不变性，GRF的原点位于物体中心<spanclass="math inline">\(\mathbf{c}_Q\)</span>；为实现尺寸不变性，点云半径会重新缩放为<spanclass="math inline">\(1\)</span>，计算方式为：</p><p><span class="math display">\[\begin{equation}\label{eq3}    \mathbf{t}_G = \mathbf{c}_Q, \quad s_G = \max_{\mathbf{q} \in\mathbf{Q}_{cam}} \Vert\mathbf{q} - \mathbf{c}_Q\Vert_2.\end{equation}\]</span></p><p>关键在于设计旋转矩阵<span class="math inline">\(\mathbf{R}_G =[\mathbf{r}_{Gx} | \mathbf{r}_{Gy} |\mathbf{r}_{Gz}]\)</span>，其中<spanclass="math inline">\(\mathbf{r}_{Gx}, \mathbf{r}_{Gy},\mathbf{r}_{Gz}\)</span>为<spanclass="math inline">\(\mathbf{R}_G\)</span>的三列，用于确保变换后点云的方向不变性。受前人工作“RecognizingObjects in Range Data Using Regional Point Descriptors”、“Structuralindexing: efficient 3-D object recognition”和“TOLDI: An effective androbust approach for 3D local shapedescription”的启发，我们将物体中心的法向量<spanclass="math inline">\(\mathbf{n}(\mathbf{c}_Q)\)</span>作为<spanclass="math inline">\(\mathbf{r}_{Gz}\)</span>，将<spanclass="math inline">\(\mathbf{Q}_{cam}\)</span>中所有点投影到<spanclass="math inline">\(\mathbf{c}_Q\)</span>的切平面上，通过统计投影向量确定<spanclass="math inline">\(\mathbf{r}_{Gx}\)</span>，然后通过叉乘<spanclass="math inline">\(\mathbf{r}_{Gy} = \mathbf{r}_{Gx} \times\mathbf{r}_{Gz}\)</span>得到<spanclass="math inline">\(\mathbf{r}_{Gy}\)</span>。这确保XY平面均匀分割点云，且x轴代表主投影方向。GRF具有<spanclass="math inline">\(SE(3)\)</span>无关性，因其可由点云的空间分布唯一确定，使变换后的点云对位姿和尺寸变化具有鲁棒性。</p><p>具体来说，我们对协方差矩阵<spanclass="math inline">\(\text{Cov}(\mathbf{Q}_{cam}) = \frac{1}{N_Q}\mathbf{Q}_{cam}^T \mathbf{Q}_{cam} - \mathbf{c}_Q\mathbf{c}_Q^T\)</span>进行奇异值分解（SVD），并使用与最小奇异值对应的奇异向量来确定<spanclass="math inline">\(\mathbf{r}_{Gz}\)</span>：</p><p><span class="math display">\[\begin{equation}\label{eq4}    \mathbf{r}_{Gz} = \begin{cases}        \mathbf{n}(\mathbf{c}_Q), &amp; \text{if }\mathbf{n}(\mathbf{c}_Q)^T\sum_{\mathbf{q} \in\mathbf{Q}_{cam}}(\mathbf{c}_Q - \mathbf{q}) &gt; 0 \\        -\mathbf{n}(\mathbf{c}_Q), &amp; \text{otherwise}    \end{cases}\end{equation}\]</span></p><p>随后，<span class="math inline">\(r_{Gx}\)</span>的计算公式为：</p><p><span class="math display">\[\begin{equation}\label{eq5}    \mathbf{r}_{Gx} = \sum_{\mathbf{q} \in\mathbf{Q}_{cam}}w_q((\mathbf{q} - \mathbf{c}_Q) -\mathbf{r}_{Gz}^T(\mathbf{q} - \mathbf{c}_Q)\mathbf{r}_{Gz}),\end{equation}\]</span></p><p>其中<span class="math inline">\(w_q\)</span>是关于点<spanclass="math inline">\(\mathbf{q}\)</span>与物体中心<spanclass="math inline">\(\mathbf{c}_Q\)</span>之间距离的权重（具体细节见附录）。</p><p>以往的工作“Leveraging SE(3) Equivariance for Self-SupervisedCategory-Level Object Pose Estimation”、“CPPF: Towards RobustCategory-Level 9D Pose Estimation in the Wild”和“Rotation-InvariantTransformer for Point CloudMatching”通常依赖复杂网络或计算成本高昂的PPF特征来实现<spanclass="math inline">\(SE(3)\)</span>无关性。相比之下，我们的变换方法计算效率高，并且能够无缝适配多种网络架构。</p><p><strong>Coarse Pose Estimation.</strong>：给定GRF中的点云<spanclass="math inline">\(\mathbf{Q}_G\)</span>和<spanclass="math inline">\(\mathbf{P}_G\)</span>，我们采样两个稀疏点集<spanclass="math inline">\(\mathbf{Q}_G^c \in \mathbb{R}^{N^c \times3}\)</span>和<span class="math inline">\(\mathbf{P}_G^c \in\mathbb{R}^{N^c \times 3}\)</span>以高效获取粗位姿初始化<spanclass="math inline">\(\Delta\mathbf{T}_{init}\)</span>。具体而言，我们利用几何编码器“GeoTransformer:Fast and Robust Point Cloud Registration with GeometricTransformer”和颜色编码器“DINOv2: Learning Robust Visual Features withoutSupervision”分别提取点云和RGB特征。特征进一步拼接为<spanclass="math inline">\(f_Q^c \in \mathbb{R}^{N^c \timesd}\)</span>和<span class="math inline">\(f_P^c \in \mathbb{R}^{N^c\times d}\)</span>，其中<spanclass="math inline">\(d\)</span>为嵌入维度。遵循“SAM-6D: SegmentAnything Model Meets Zero-Shot 6D Object PoseEstimation”，我们添加可学习的背景标记以分配非重叠点。这些嵌入表示为<spanclass="math inline">\(\hat{f}_Q^c \in \mathbb{R}^{(N^c + 1) \timesd}\)</span>和<span class="math inline">\(\hat{f}_P^c \in\mathbb{R}^{(N^c + 1) \times d}\)</span>，并通过三个堆叠的GeometricTransformer解码模块进行处理。</p><p>最后一个解码器的输出<span class="math inline">\(\hat{F}_Q^c \in\mathbb{R}^{(N^c + 1) \times D}\)</span>和<spanclass="math inline">\(\hat{F}_P^c \in \mathbb{R}^{(N^c + 1) \timesD}\)</span>是用于构建相关矩阵的逐点特征。然而，在我们的场景中，重叠率可能受到视点、遮挡或深度噪声等复杂因素的影响。为解决这一问题，网络额外预测重叠置信度<spanclass="math inline">\(\hat{O}_Q^c \in \mathbb{R}^{(N^c + 1) \times1}\)</span>和<span class="math inline">\(\hat{O}_P^c \in\mathbb{R}^{(N^c + 1) \times1}\)</span>，表示各点属于重叠区域的概率。因此，重叠感知相关矩阵<spanclass="math inline">\(\mathbf{X}_c \in \mathbb{R}^{(N^c + 1) \times (N^c+ 1)}\)</span>可通过下式计算：</p><p><span class="math display">\[\begin{equation}\label{eq6}    \mathbf{X}^c = \text{softmax}[(\hat{O}_Q^c \odot\hat{F}_Q^c)(\hat{O}_P^c \odot \hat{F}_P^c)^T].\end{equation}\]</span></p><p>这里<span class="math inline">\(\odot\)</span>表示元素级乘法。<spanclass="math inline">\(\mathbf{X}^c\)</span>中的每个元素表示<spanclass="math inline">\(\mathbf{Q}^c\)</span>与<spanclass="math inline">\(\mathbf{P}^c\)</span>中点对之间的相关分数。</p><p>在计算出相关矩阵<spanclass="math inline">\(\mathbf{X}^c\)</span>后，我们可以提取<spanclass="math inline">\(\mathbf{Q}^c\)</span>和<spanclass="math inline">\(\mathbf{P}^c\)</span>之间所有可能的对应点对及其相关分数，从而求解公式<spanclass="math inline">\(\eqref{eq1}\)</span>。具体来说，我们首先根据<spanclass="math inline">\(\mathbf{X}^c\)</span>的分布采样<spanclass="math inline">\(N_H\)</span>个点对三元组，生成位姿假设。然后，每个位姿假设<spanclass="math inline">\(\Delta \mathbf{T}_h = \{\Delta \mathbf{R}_h,\Delta \mathbf{t}_h\}\)</span>按照“SurfEmb: Dense and ContinuousCorrespondence Distributions for Object Pose Estimation with LearntSurface Embeddings”和“SAM-6D: Segment Anything Model Meets Zero-Shot 6DObject Pose Estimation”中的方法，通过距离<spanclass="math inline">\(D_h\)</span>的倒数进行评分：</p><p><span class="math display">\[\begin{equation}\label{eq7}    \begin{aligned}        D_h &amp;= \frac{1}{N^c}\sum_{\mathbf{p}^c \in\mathbf{P}^c}\min_{\mathbf{q}^c \in\mathbf{Q}^c}\Vert\Delta\mathbf{R}_h^T(\mathbf{q}^c -\Delta\mathbf{t}_h) - \mathbf{p}^c\Vert_2, \\        \xi_h &amp;= \frac{1}{D_h}, \quad h = 1, 2, \cdots, N_H.    \end{aligned}\end{equation}\]</span></p><p>得分<spanclass="math inline">\(\xi_h\)</span>最高的位姿假设被选为初始位姿预测值<spanclass="math inline">\(\Delta \mathbf{T}_{init} = \{\Delta\mathbf{R}_{init}, \Delta\mathbf{t}_{init}\}\)</span>，并进一步用于变换下一阶段的输入。</p><p><strong>Fine Pose Estimation.</strong>：在使用初始位姿预测<spanclass="math inline">\(\Delta \mathbf{T}_{init}\)</span>将<spanclass="math inline">\(\mathbf{Q}_{cam}\)</span>变换为<spanclass="math inline">\(\tilde{\mathbf{Q}}_{cam}\)</span>之后，我们在两个密集点集，即<spanclass="math inline">\(\tilde{\mathbf{Q}}^f \in \mathbb{R}^{N^f \times3}\)</span>和<span class="math inline">\(\mathbf{P}^f \in\mathbb{R}^{N^f \times3}\)</span>之间执行精细匹配过程，以获得更精确的位姿。该网络通过分层编码范式利用几何细节，包括位置编码层和局部参考框架编码层。对于每个点，我们首先使用小型PointNet，即“PointNet:Deep Learning on Point Sets for 3D Classification andSegmentation”对其全局位置进行编码，然后构建<spanclass="math inline">\(SE(3)\)</span>无关局部参考框架（LRF）以收集局部描述符。这两种编码相互补充，因为局部描述符捕获小邻域内的精细几何结构，而位置编码层提供全局几何上下文。</p><p>以<spanclass="math inline">\(\tilde{\mathbf{Q}}^f\)</span>为例，构建局部参考框架（LRF）编码的过程如下：对于<spanclass="math inline">\(\tilde{\mathbf{Q}}^f\)</span>中的每个点<spanclass="math inline">\(\tilde{\mathbf{q}}^m\)</span>，通过将<spanclass="math inline">\(\tilde{\mathbf{q}}^m\)</span>的<spanclass="math inline">\(N_D\)</span>个邻近点分组，构建局部区域集合<spanclass="math inline">\(\tilde{\mathcal{Q}}^m = \{\tilde{\mathbf{q}}_j,\text{where } \Vert\tilde{\mathbf{q}}_j - \tilde{\mathbf{q}}^m\Vert_2\leq r\}_{j = 1}^{N_D}\)</span>。LRF的变换位姿<spanclass="math inline">\(\{\mathbf{R}_L^m, \mathbf{t}_L^m,s_L^m\}\)</span>的计算方式与GRF类似（见公式<spanclass="math inline">\(\eqref{eq3}\)</span>、<spanclass="math inline">\(\eqref{eq4}\)</span>、<spanclass="math inline">\(\eqref{eq5}\)</span>），区别在于LRF基于局部点集构建，而GRF基于整个点云。通过计算变换位姿，我们将局部点描述符计算为：</p><p><span class="math display">\[\begin{equation}\label{eq8}    \begin{aligned}        \mathcal{Q}_L^m &amp;= \tilde{\mathcal{Q}}_L^m =\{(\mathbf{R}_L^m)^T(\frac{\tilde{\mathbf{q}}_j -\mathbf{t}_L^m}{s_L^m})\}_{j = 1}^{N_D}, \\        \mathbf{Q}_L^f &amp;= \{\mathcal{Q}_L^m\}_{m = 1}^{N^f}.    \end{aligned}\end{equation}\]</span></p><p>类似地，我们将<spanclass="math inline">\(\mathbf{P}^f\)</span>变换为<spanclass="math inline">\(\mathbf{P}_L^f\)</span>。注意，由于LRF是位姿无关的，因此<spanclass="math inline">\(\mathcal{Q}_L^m =\tilde{\mathcal{Q}}_L^m\)</span>。然后，通过三层MLP从<spanclass="math inline">\(\mathbf{Q}_L^f\)</span>和<spanclass="math inline">\(\mathbf{P}_L^f\)</span>中提取LRF编码。</p><p>位置编码和LRF编码与几何和颜色特征相结合，作为几何Transformer的输入。与初始位姿预测类似，通过解码这些特征，我们获得精细的逐点特征<spanclass="math inline">\(\hat{F}^f_Q\)</span>、<spanclass="math inline">\(\hat{F}^f_P\)</span>以及重叠置信度<spanclass="math inline">\(\hat{O}^f_Q\)</span>、<spanclass="math inline">\(\hat{O}^f_P\)</span>，然后利用这些结果得到考虑重叠区域的精细相关矩阵<spanclass="math inline">\(\mathbf{X}^f \in \mathbb{R}^{(N^f + 1) \times (N^f+ 1)}\)</span>。最终位姿<span class="math inline">\(\Delta\mathbf{T}\)</span>通过使用加权SVD算法对<spanclass="math inline">\(\mathbf{X}^f\)</span>求解公式<spanclass="math inline">\(\eqref{eq1}\)</span>来预测。</p><h2 id="experiments">4. Experiments</h2><h2 id="conclusion-limitation-and-future-work">5. Conclusion, Limitationand Future work</h2><div class="pdf-container" data-target="https://arxiv.org/pdf/2411.16106" data-height="500px"></div>]]></content>
    
    
    <summary type="html">Unseen object pose estimation methods often rely on CAD models or multiple reference views, making the onboarding stage costly. To simplify reference acquisition, we aim to estimate the unseen object’s pose through a single unposed RGB-D reference image. While previous works leverage reference images as pose anchors to limit the range of relative pose, our scenario presents significant challenges since the relative transformation could vary across the entire SE(3) space. Moreover, factors like occlusion, sensor noise, and extreme geometry could result in low viewpoint overlap. To address these challenges, we present a novel approach and benchmark, termed UNOPose, for &lt;u&gt;UN&lt;/u&gt;seen &lt;u&gt;O&lt;/u&gt;ne-referencebased object &lt;u&gt;Pose&lt;/u&gt; estimation. Building upon a coarse-to-fine paradigm, UNOPose constructs an SE(3)-invariant reference frame to standardize object representation despite pose and size variations. To alleviate small overlap across viewpoints, we recalibrate the weight of each correspondence based on its predicted likelihood of being within the overlapping region. Evaluated on our proposed benchmark based on the BOP Challenge, UNOPose demonstrates superior performance, significantly outperforming traditional and learningbased methods in the one-reference setting and remaining competitive with CAD-model-based methods. The code and dataset are available at https://github.com/shanice-l/UNOPose.</summary>
    
    
    
    <category term="读万卷书" scheme="https://blog.032802.xyz/categories/%E8%AF%BB%E4%B8%87%E5%8D%B7%E4%B9%A6/"/>
    
    
    <category term="Object Pose Estimation" scheme="https://blog.032802.xyz/tags/Object-Pose-Estimation/"/>
    
    <category term="2025CVPR" scheme="https://blog.032802.xyz/tags/2025CVPR/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】One2Any: One-Reference 6D Pose Estimation for Any Object</title>
    <link href="https://blog.032802.xyz/paper-reading/2025/one2any-one-reference-6d-pose-estimation-for-any-object_2025_Liu.html"/>
    <id>https://blog.032802.xyz/paper-reading/2025/one2any-one-reference-6d-pose-estimation-for-any-object_2025_Liu.html</id>
    <published>2025-05-09T02:10:27.000Z</published>
    <updated>2025-05-19T01:10:27.000Z</updated>
    
    <content type="html"><![CDATA[<h1id="one2any-one-reference-6d-pose-estimation-for-any-object">One2Any:One-Reference 6D Pose Estimation for Any Object</h1><table><colgroup><col style="width: 10%" /><col style="width: 9%" /><col style="width: 12%" /><col style="width: 12%" /><col style="width: 43%" /><col style="width: 12%" /></colgroup><thead><tr><th style="text-align: center;">方法</th><th style="text-align: center;">类型</th><th style="text-align: center;">训练输入</th><th style="text-align: center;">推理输入</th><th style="text-align: center;">输出</th><th style="text-align: center;">pipeline</th></tr></thead><tbody><tr><td style="text-align: center;">One2Any</td><td style="text-align: center;">任意级</td><td style="text-align: center;">RGBDs</td><td style="text-align: center;">RGBDs</td><td style="text-align: center;">相对<spanclass="math inline">\(\mathbf{R}, \mathbf{t}\)</span></td><td style="text-align: center;"></td></tr></tbody></table><ul><li>2025.05.19：One2Any估计两张RGBD间的相对位姿，从NOCS中获得启发，One2Any直接从Ref中得到规范空间，使用Umeyama算法，将后续的Query与这个规范空间对齐，得到相对位姿。</li></ul><h2 id="abstract">Abstract</h2><h2 id="introduction">1. Introduction</h2><figure><imgsrc="https://img.032802.xyz/paper-reading/2025/one2any-one-reference-6d-pose-estimation-for-any-object_2025_Liu/teaser-v4.webp"alt="Figure 1. Given a single RGB-D image as a reference for an unseen object, our method estimates the pose of the object in a given query image, relative to the reference. The method first predicts a Reference Object Pose Embedding (ROPE) that encodes the object&#39;s texture, shape, and pose priors. During inference, each query RGB image is processed through a decoder to predict the Reference Object Coordinate (ROC) map and estimate the relative pose to the reference image. This approach effectively handles large viewpoint changes." /><figcaption aria-hidden="true">Figure 1. Given a single RGB-D image as areference for an unseen object, our method estimates the pose of theobject in a given query image, relative to the reference. The methodfirst predicts a Reference Object Pose Embedding (ROPE) that encodes theobject's texture, shape, and pose priors. During inference, each queryRGB image is processed through a decoder to predict the Reference ObjectCoordinate (ROC) map and estimate the relative pose to the referenceimage. This approach effectively handles large viewpointchanges.</figcaption></figure><h2 id="related-works">2. Related Works</h2><h2 id="method">3. Method</h2><h3 id="overview">3.1. Overview</h3><figure><imgsrc="https://img.032802.xyz/paper-reading/2025/one2any-one-reference-6d-pose-estimation-for-any-object_2025_Liu/architecture.webp"alt="Figure 2. Network architecture. The network takes a reference RGB-D image as input and learns a Reference Object Pose Embedding (ROPE) through a Reference Object Encoder (ROE). This embedding is subsequently integrated with the query feature map, which is extracted using a pre-trained VQVAE model [48] with the query RGB image as input. We use the U-Net architecture for effective integrate the ROPE with the query feature with cross-attentions layers. The decoder is trained to predict the ROC map. The final pose estimation is computed using the Umeyama algorithm [53]." /><figcaption aria-hidden="true">Figure 2. Network architecture. Thenetwork takes a reference RGB-D image as input and learns a ReferenceObject Pose Embedding (ROPE) through a Reference Object Encoder (ROE).This embedding is subsequently integrated with the query feature map,which is extracted using a pre-trained VQVAE model [48] with the queryRGB image as input. We use the U-Net architecture for effectiveintegrate the ROPE with the query feature with cross-attentions layers.The decoder is trained to predict the ROC map. The final pose estimationis computed using the Umeyama algorithm [53].</figcaption></figure><p>我们将问题表述为：给定单张参考RGBD图像和查询RGBD图像的相对物体位姿估计，且不依赖CAD模型或多视角图像。给定一幅参考RGBD图像，其RGB图像为<span class="math inline">\(A_I \in\mathbb{R}^{W \times H \times 3}\)</span>，深度图像为<spanclass="math inline">\(A_D \in \mathbb{R}^{W \timesH}\)</span>，目标物体掩码为<span class="math inline">\(A_M \in \{0,1\}^{W \times H}\)</span>，我们的目标是预测查询图像输入<spanclass="math inline">\(Q_I \in \mathbb{R}^{W \times H \times3}\)</span>、<span class="math inline">\(Q_D \in \mathbb{R}^{W \timesH}\)</span>、<span class="math inline">\(Q_M \in \{0, 1\}^{W \timesH}\)</span>中目标物体相对于参考视角中目标物体的位姿<spanclass="math inline">\([\mathbf{R} |\mathbf{t}]\)</span>。我们的核心思想是学习一个具有参数<spanclass="math inline">\(\theta_A\)</span>的参考物体编码器（ROE）<spanclass="math inline">\(f_A\)</span>，将参考输入<spanclass="math inline">\(A = [A_I, A_D,A_M]\)</span>嵌入到参考物体位姿嵌入（ROPE）中。通过在大型数据集上训练<spanclass="math inline">\(f_A\)</span>，嵌入向量<spanclass="math inline">\(\mathbf{e}_A\)</span>为物体位姿解码（OPD）模块提供条件，以生成查询图像的参考物体坐标（ROC）图。我们在图2中展示了网络架构。OPD模块包含用于提取查询图像特征并预测ROC图的编码器-解码器架构，这通过使用U-Net架构与ROPE集成得到进一步增强。在此，我们将具有参数<spanclass="math inline">\(\theta_Q\)</span>的<spanclass="math inline">\(g_{\theta_Q}\)</span>记为OPD模块，其将查询图像<spanclass="math inline">\(Q\)</span>作为输入以预测输出ROC：<spanclass="math inline">\(Y_Q \in \mathbb{R}^{W \times H \times3}\)</span>。于是，我们可以将整体问题表述为：</p><p><span class="math display">\[\begin{equation}\label{eq1}    Y_Q = g(Q, f_A(A; \theta_A); \theta_Q)\end{equation}\]</span></p><h3 id="reference-object-coordinate-roc">3.2. Reference ObjectCoordinate (ROC)</h3><figure><imgsrc="https://img.032802.xyz/paper-reading/2025/one2any-one-reference-6d-pose-estimation-for-any-object_2025_Liu/ROC_representation.webp"alt="Figure 3. ROC representations given a reference RGB-D image and a query RGB-D image. The ROC space is initially defined by the reference frame, using the camera intrinsics \mathbf{K} and the scaling matrix \mathbf{S} to a normalized space. The query image is subsequently aligned to this space using the relative pose [\mathbf{R} | \mathbf{t}] and the scale matrix \mathbf{S}. The ROC map is generated by mapping points in the ROC space to their corresponding 2D pixel locations and encoding the point positions as RGB values." /><figcaption aria-hidden="true">Figure 3. ROC representations given areference RGB-D image and a query RGB-D image. The ROC space isinitially defined by the reference frame, using the camera intrinsics<span class="math inline">\(\mathbf{K}\)</span> and the scaling matrix<span class="math inline">\(\mathbf{S}\)</span> to a normalized space.The query image is subsequently aligned to this space using the relativepose <span class="math inline">\([\mathbf{R} | \mathbf{t}]\)</span> andthe scale matrix <span class="math inline">\(\mathbf{S}\)</span>. TheROC map is generated by mapping points in the ROC space to theircorresponding 2D pixel locations and encoding the point positions as RGBvalues.</figcaption></figure><p>如3.1节所述，我们的目标是估计单张参考图像<spanclass="math inline">\(A\)</span>与单张查询图像<spanclass="math inline">\(Q\)</span>之间的相对位姿<spanclass="math inline">\([\mathbf{R} |\mathbf{t}]\)</span>。为此，我们受NOCS启发，提出使用一种称为参考物体坐标（ROC）的2D-3D映射。与NOCS不同的是，我们的方法无需规范坐标系，而是直接在参考相机坐标系中表示物体坐标。因此，ROC仅由参考坐标系定义，查询图像中的物体需变换至与参考坐标系对齐，并归一化到ROC空间中。</p><p>尽管这一改动看似简单，但对训练和推理均有重要影响。根据定义，NOCS要求将同一类别的所有物体对齐到单一规范空间，而ROC图的生成要容易得多，仅需一对参考图像和查询图像，即可直接为相对位姿估计提供合适的表示。</p><p>为了从参考视图构建ROC，我们首先通过深度值反投影像素坐标来获取参考物体的部分点云<spanclass="math inline">\(P_A\)</span>：</p><p><span class="math display">\[\begin{equation}\label{eq2}    P_A = \mathbf{K}^{-1}A_D[A_M = 1]\end{equation}\]</span></p><p>然后，我们通过对<spanclass="math inline">\(P_A\)</span>的齐次坐标应用变换<spanclass="math inline">\(\mathbf{S} \in \mathbb{R}^{4 \times4}\)</span>来获得ROC。</p><p><span class="math display">\[\begin{equation}\label{eq3}    Y_A = \mathbf{S}P_A\end{equation}\]</span></p><p>在此对符号稍作扩展，我们将映射<span class="math inline">\(A_I \toY_A\)</span>的ROC记为<spanclass="math inline">\(Y_A\)</span>。类似地，为了获取查询图像的ROC真实值，我们首先使用真实相对位姿<spanclass="math inline">\([\mathbf{R} |\mathbf{t}]\)</span>变换将查询点云转换到参考视图下，然后应用相同的缩放和平移变换<spanclass="math inline">\(\mathbf{S}\)</span>。</p><p><span class="math display">\[\begin{equation}\label{eq4}    Y_Q = \mathbf{S}([\mathbf{R} | \mathbf{t}]P_Q)\end{equation}\]</span></p><p>图3展示了构建ROC图的过程。图的上半部分显示了从参考RGBD图像生成ROC图<spanclass="math inline">\(Y_A\)</span>的过程，下半部分则展示了真实查询ROC图<spanclass="math inline">\(Y_Q\)</span>的生成过程。该方法基于参考坐标系建立了一个物体空间，该空间会随着参考物体及其位姿的变化而动态调整。</p><h3 id="reference-object-pose-embedding">3.3. Reference Object PoseEmbedding</h3><p>在仅给定单张RGBD参考图像的情况下，对查询RGBD图像进行位姿预测面临着独特的挑战。以往的方法通常依赖关键点特征匹配，如“High-resolutionopen-vocabulary object 6D pose estimation”、“POPE: 6-DoF Promptable PoseEstimation of Any Object, in Any Scene, with One Reference”和“NOPE:Novel Object Pose Estimation from a SingleImage”，但当图像中的可见区域重叠度较低或被严重遮挡时，这些方法就会失效。我们提出了一种替代方法，将参考图像编码为参考物体位姿嵌入（ROPE），从而能够有效地预测物体位姿。我们的目标是训练一个参考物体编码器（ROE），使其能够从单张RGBD参考图像中在潜在空间生成全面的物体表示。然后，ROPE表示可以从同一物体的任何测试图像中有效地预测参考相机空间中的ROC图，从而实现准确的位姿估计。</p><p>ROE记为<span class="math inline">\(f(A;\theta_A)\)</span>，其设计目的是从由参考RGB图像、参考ROC图和物体掩码组成的通道级联输入<spanclass="math inline">\(A\)</span>中提取潜在空间编码。如图2所示，该编码捕获了纹理和几何信息。编码器通过三个带残差连接的卷积层“DeepResidual Learning for ImageRecognition”处理输入，生成特征图，并将其标记化为带位置嵌入的补丁“AnImage is Worth 16x16 Words: Transformers for Image Recognition atScale”。这种条件嵌入可有效引导ROC图的生成，即使在存在遮挡的情况下也能保持对参考数据的保真度，如4.8节所示。</p><h3 id="object-pose-decoding-with-rope">3.4. Object Pose Decoding withROPE</h3><p>OPD模块<span class="math inline">\(g(Q, f ;\theta_Q)\)</span>基于从参考图像中提取的ROPE，对查询图像中的物体位姿进行解码。在具有代表性的ROPE的强监督下，OPD模块通过生成查询图像的ROC图来预测物体的位姿。</p><p>OPD架构采用了受StableDiffusion启发的编码器-解码器结构。为了更好地聚合ROPE表示，我们通过交叉注意力层集成来自查询图像的信息。具体来说，我们使用“High-resolutionopen-vocabulary object 6D poseestimation”中预训练的VQVAE模型来充分提取查询RGB图像<spanclass="math inline">\(Q_I\)</span>的特征图，并进一步提升模型的泛化能力。我们随后通过交叉注意力层，对以代表性ROPE为条件的类UNet网络“High-resolutionopen-vocabulary object 6D poseestimation”进行微调。图2展示了详细架构。该U-Net通过交叉注意力机制整合查询特征<spanclass="math inline">\(\mathcal{F}^Q\)</span>和参考嵌入<spanclass="math inline">\(\mathcal{F}^A\)</span>（即ROPE）。具体而言，在每个交叉注意力层中，查询特征<spanclass="math inline">\(\mathcal{F}^Q\)</span>与来自<spanclass="math inline">\(\mathcal{F}^A\)</span>的键<spanclass="math inline">\(k \in \mathbb{R}^{m \timesd_k}\)</span>和值嵌入<span class="math inline">\(v \in \mathbb{R}^{m\times d_v}\)</span>进行交互。</p><p><span class="math display">\[\begin{equation}\label{eq5}    k = \mathcal{F}^A \times W_k, \quad v = \mathcal{F}^A \times W_v,\quad \mathcal{F}^Q = \mathcal{F}^Q \times W_q\end{equation}\]</span></p><p>其中<spanclass="math inline">\(W\)</span>表示每个向量的权重矩阵，且交叉注意力应用于查询向量<spanclass="math inline">\(q\)</span>、键向量<spanclass="math inline">\(k\)</span>和值向量<spanclass="math inline">\(v\)</span>之间，</p><p><span class="math display">\[\begin{equation}\label{eq6}    \text{Attention}(q, k, v) =\text{softmax}\left(\frac{qk^T}{\sqrt{d_k}}\right)v\end{equation}\]</span></p><p><span class="math inline">\(d_k\)</span>是键向量<spanclass="math inline">\(k\)</span>的维度，用于缩放操作，该操作在注意力机制中用于确保梯度稳定。最终的相对特征图<spanclass="math inline">\(\mathcal{F}^{Q2A}\)</span>通过U-Net架构中的多个交叉注意力层实现。该架构使网络能够充分提取嵌入在ROPE中的位姿-形状信息。</p><p>解码器随后对<spanclass="math inline">\(F^{Q2A}\)</span>进行逐步细化和上采样，将其重建为原始图像尺寸。该解码器由五个包含残差连接和双线性上采样的卷积层构成，生成ROC图<spanclass="math inline">\(\hat{Y}_Q\)</span>，其准确表示了查询视图在ROC空间中的坐标。</p><p><strong>ROC map loss.</strong>遵循NOCS的方法，我们通过使用平滑L1损失函数，来自“Faster R-CNN: TowardsReal-Time Object Detection with Region ProposalNetworks”，监督ROC图预测值<spanclass="math inline">\(\hat{Y}_Q\)</span>与真实ROC图<spanclass="math inline">\(Y_Q\)</span>的一致性来训练网络<spanclass="math inline">\(\{f, g\}\)</span>，损失函数定义为：</p><p><span class="math display">\[\begin{equation}\label{eq7}    \begin{aligned}        \mathcal{L} &amp;= \frac{1}{N} \sum_i \sum_j Q_M(i, j)E(i, j) \\        c &amp;= Y_Q(i, j) - \hat{Y}_Q(i, j) \\        E(i, j) &amp;=            \begin{cases}                \frac{0.5(c)^2}{\beta}, &amp; (|c| &lt; \beta) \\                |c| - 0.5\beta, &amp; \text{otherwise}            \end{cases}    \end{aligned}\end{equation}\]</span></p><p><spanclass="math inline">\(\beta\)</span>为平滑阈值，设置为0.1。对于物体掩码像素位置，<spanclass="math inline">\(Q_M(i, j)=1\)</span>，否则为0。</p><h3 id="pose-estimation-from-roc-map">3.5. Pose Estimation from ROCMap</h3><p>相对位姿<span class="math inline">\([\mathbf{R} |\mathbf{t}]\)</span>通过测量预测的ROC图<spanclass="math inline">\(\hat{Y}_Q\)</span>与查询视点云<spanclass="math inline">\(P_Q\)</span>之间的变换来计算。注意，参考相机的位姿为<spanclass="math inline">\([\mathbf{I}_3 |\mathbf{0}]\)</span>。首先，我们使用参考ROC的缩放矩阵的逆<spanclass="math inline">\(\mathbf{S}^{-1}\)</span>对预测的ROC图<spanclass="math inline">\(\hat{Y}_Q\)</span>进行平移和缩放，使其与参考相机坐标系对齐。由此得到的点云<spanclass="math inline">\(\hat{P}_Q^A\)</span>表示参考相机坐标系下的查询物体。结合查询点云<spanclass="math inline">\(P_Q\)</span>，我们使用Umeyama方法来获取查询坐标系与参考坐标系之间的位姿预测<spanclass="math inline">\([\hat{\mathbf{R}} |\hat{\mathbf{t}}]\)</span>。数学上，我们可以将其表示为：</p><p><span class="math display">\[\begin{equation}\label{eq8}    \begin{aligned}        \hat{P}_Q^A &amp;= \mathbf{S}^{-1}\hat{Y}_Q[Q_M = 1] \\        [\hat{\mathbf{R}} | \hat{\mathbf{t}}] &amp;= \text{Umeyama}(P_Q,\hat{P}_Q^A)    \end{aligned}\end{equation}\]</span></p><h2 id="experimental-results">4. Experimental Results</h2><figure><imgsrc="https://img.032802.xyz/paper-reading/2025/one2any-one-reference-6d-pose-estimation-for-any-object_2025_Liu/linemod_ycbv_results.webp"alt="Figure 4. Qualitative results on YCB-Video [60] and LINEMOD [20] datasets. Predicted poses are displayed in green and groundtruth poses are in pink. We present FoundationPose [57] with the generated CAD models from the reference image (the top is the view close to the reference image, and the bottom is the view close to the query image), and we display Oryon [9] with the predicted correspondences. For our method, we also show the generated ROC map (bottom) compared with the GT ROC map (top)." /><figcaption aria-hidden="true">Figure 4. Qualitative results onYCB-Video [60] and LINEMOD [20] datasets. Predicted poses are displayedin green and groundtruth poses are in pink. We present FoundationPose[57] with the generated CAD models from the reference image (the top isthe view close to the reference image, and the bottom is the view closeto the query image), and we display Oryon [9] with the predictedcorrespondences. For our method, we also show the generated ROC map(bottom) compared with the GT ROC map (top).</figcaption></figure><h2 id="conclusions">5. Conclusions</h2><div class="pdf-container" data-target="https://arxiv.org/pdf/2505.04109" data-height="500px"></div>]]></content>
    
    
    <summary type="html">6D object pose estimation remains challenging for many applications due to dependencies on complete 3D models, multi-view images, or training limited to specific object categories. These requirements make generalization to novel objects difficult for which neither 3D models nor multi-view images may be available. To address this, we propose a novel method One2Any that estimates the relative 6-degrees of freedom (DOF) object pose using only a single reference-single query RGB-D image, without prior knowledge of its 3D model, multi-view data, or category constraints. We treat object pose estimation as an encoding-decoding process: first, we obtain a comprehensive Reference Object Pose Embedding (ROPE) that encodes an object&#39;s shape, orientation, and texture from a single reference view. Using this embedding, a U-Net-based pose decoding module produces Reference Object Coordinate (ROC) for new views, enabling fast and accurate pose estimation. This simple encodingdecoding framework allows our model to be trained on any pair-wise pose data, enabling large-scale training and demonstrating great scalability. Experiments on multiple benchmark datasets demonstrate that our model generalizes well to novel objects, achieving state-of-the-art accuracy and robustness even rivaling methods that require multi-view or CAD inputs, at a fraction of compute. Code is available at https://github.com/lmy1001/One2Any.</summary>
    
    
    
    <category term="读万卷书" scheme="https://blog.032802.xyz/categories/%E8%AF%BB%E4%B8%87%E5%8D%B7%E4%B9%A6/"/>
    
    
    <category term="Object Pose Estimation" scheme="https://blog.032802.xyz/tags/Object-Pose-Estimation/"/>
    
    <category term="2025CVPR" scheme="https://blog.032802.xyz/tags/2025CVPR/"/>
    
  </entry>
  
  <entry>
    <title>【代码复现】GIVEPose: Gradual Intra-class Variation Elimination for RGB-based  Category-Level Object Pose Estimation</title>
    <link href="https://blog.032802.xyz/code-running/ziqin-h_GIVEPose.html"/>
    <id>https://blog.032802.xyz/code-running/ziqin-h_GIVEPose.html</id>
    <published>2025-04-24T06:14:18.000Z</published>
    <updated>2025-05-13T07:14:18.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>到目前为止跑不起来，很多报错。</p></blockquote><h2 id="安装givepose环境">安装GIVEPose环境</h2><ol type="1"><li><p>修改<code>GIVEPose_env.yml</code>文件：</p><ol type="1"><li>第1行：<code>name: dinov2</code> -&gt;<code>name: givepose</code>；</li><li>第192行：注释<code>- xformers=0.0.21=py39_cu11.8.0_pyt2.0.1</code>；</li><li>第217行：注释<code>- dcnv3==1.1</code>；</li><li>第268行：注释<code>- pydensecrf==1.0rc2</code>。</li></ol></li><li><p>创建环境：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda <span class="built_in">env</span> create -f GIVEPose_env.yml</span><br></pre></td></tr></table></figure></p></li><li><p>激活环境：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate givepose</span><br></pre></td></tr></table></figure></p></li><li><p>安装<code>dcnv3</code>、<code>pydensecrf</code>和<code>detectron2</code>：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> network/ops_dcnv3/</span><br><span class="line">sh make.sh</span><br><span class="line"><span class="built_in">cd</span> ../../</span><br><span class="line"></span><br><span class="line">pip3 install git+https://github.com/lucasb-eyer/pydensecrf.git</span><br><span class="line"></span><br><span class="line">python -m pip install <span class="string">&#x27;git+https://github.com/facebookresearch/detectron2.git&#x27;</span></span><br></pre></td></tr></table></figure></p></li><li><p>Clone <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21lbnRpYW4vb2JqZWN0LWRlZm9ybW5ldA==">mentian/object-deformnet<i class="fa fa-external-link-alt"></i></span>并安装<code>nn_distance</code>，需要使用该仓库中的代码进行数据处理：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/mentian/object-deformnet.git</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> lib/nn_distance</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure></p></li><li><p>Clone <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xvbHJ1ZHkvTGFQb3Nl">lolrudy/LaPose<i class="fa fa-external-link-alt"></i></span>，需要使用该仓库中的代码进行数据处理：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/lolrudy/LaPose.git</span><br></pre></td></tr></table></figure></p></li></ol><h2 id="下载数据集">下载数据集</h2><ol type="1"><li><p>下载NOCS数据集：</p><ul><li><span class="exturl" data-url="aHR0cDovL2Rvd25sb2FkLmNzLnN0YW5mb3JkLmVkdS9vcmlvbi9ub2NzL2NhbWVyYV9jb21wb3NlZF9kZXB0aC56aXA=">http://download.cs.stanford.edu/orion/nocs/camera_composed_depth.zip<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2Rvd25sb2FkLmNzLnN0YW5mb3JkLmVkdS9vcmlvbi9ub2NzL2NhbWVyYV90cmFpbi56aXA=">http://download.cs.stanford.edu/orion/nocs/camera_train.zip<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2Rvd25sb2FkLmNzLnN0YW5mb3JkLmVkdS9vcmlvbi9ub2NzL2NhbWVyYV92YWwyNUsuemlw">http://download.cs.stanford.edu/orion/nocs/camera_val25K.zip<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2Rvd25sb2FkLmNzLnN0YW5mb3JkLmVkdS9vcmlvbi9ub2NzL2d0cy56aXA=">http://download.cs.stanford.edu/orion/nocs/gts.zip<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2Rvd25sb2FkLmNzLnN0YW5mb3JkLmVkdS9vcmlvbi9ub2NzL29ial9tb2RlbHMuemlw">http://download.cs.stanford.edu/orion/nocs/obj_models.zip<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2Rvd25sb2FkLmNzLnN0YW5mb3JkLmVkdS9vcmlvbi9ub2NzL3JlYWxfdGVzdC56aXA=">http://download.cs.stanford.edu/orion/nocs/real_test.zip<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2Rvd25sb2FkLmNzLnN0YW5mb3JkLmVkdS9vcmlvbi9ub2NzL3JlYWxfdHJhaW4uemlw">http://download.cs.stanford.edu/orion/nocs/real_train.zip<i class="fa fa-external-link-alt"></i></span></li></ul></li><li><p>下载NOCS数据集中缺失的物体模型（From: <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21lbnRpYW4vb2JqZWN0LWRlZm9ybW5ldC9pc3N1ZXMvMyNpc3N1ZWNvbW1lbnQtNjk4ODU4MzMy">https://github.com/mentian/object-deformnet/issues/3#issuecomment-698858332<i class="fa fa-external-link-alt"></i></span>）：</p><ul><li><span class="exturl" data-url="aHR0cHM6Ly9kcml2ZS5nb29nbGUuY29tL2ZpbGUvZC8xcldreEVWSkpoX2tXSXF4dWRuX2k2c0otaGhkMEU3VFYv">https://drive.google.com/file/d/1rWkxEVJJh_kWIqxudn_i6sJ-hhd0E7TV/<i class="fa fa-external-link-alt"></i></span></li></ul></li><li><p>下载<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0dvcmlsbGEtTGFiLVNDVVQvRHVhbFBvc2VOZXQ=">Gorilla-Lab-SCUT/DualPoseNet<i class="fa fa-external-link-alt"></i></span>的分割结果（LaPose使用该分割结果对数据进行预处理）：</p><ul><li><span class="exturl" data-url="aHR0cHM6Ly9kcml2ZS5nb29nbGUuY29tL2ZpbGUvZC8xUndBYkZXdzJJVFg5bVh6TFVFQmpQeV9nLU1OZHlIRVQv">https://drive.google.com/file/d/1RwAbFWw2ITX9mXzLUEBjPy_g-MNdyHET/<i class="fa fa-external-link-alt"></i></span></li></ul></li><li><p>下载使用<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21lbnRpYW4vb2JqZWN0LWRlZm9ybW5ldA==">mentian/object-deformnet<i class="fa fa-external-link-alt"></i></span>进行数据预处理时用到的数据到仓库文件夹中：</p><ul><li><span class="exturl" data-url="aHR0cHM6Ly9kcml2ZS5nb29nbGUuY29tL2ZpbGUvZC8xcDcyTmRZNEJpZV9zcmE5VTh6b1VOSTRmVHJRWmRibmMv">https://drive.google.com/file/d/1p72NdY4Bie_sra9U8zoUNI4fTrQZdbnc/<i class="fa fa-external-link-alt"></i></span></li></ul></li><li><p>下载GIVEPose训练时使用的IVFC map：</p><ul><li><span class="exturl" data-url="aHR0cHM6Ly9kcml2ZS5nb29nbGUuY29tL2ZpbGUvZC8xa0Y4Y2s3RU05bW56Nmp5eVNuY1ZVeHpYZ1J4SEg5ZS0v">https://drive.google.com/file/d/1kF8ck7EM9mnz6jyySncVUxzXgRxHH9e-/<i class="fa fa-external-link-alt"></i></span></li></ul></li></ol><h2 id="处理数据集">处理数据集</h2><h3 id="mentianobject-deformnet">mentian/object-deformnet</h3><p>请参考<ahref="https://blog.032802.xyz/code-running/Leeiieeo_AG-Pose.html#mentianobject-deformnet">https://blog.032802.xyz/code-running/Leeiieeo_AG-Pose.html#mentianobject-deformnet</a>的处理过程，这里不再重复</p><h3 id="lolrudylapose">lolrudy/LaPose</h3><p>遵循<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xvbHJ1ZHkvTGFQb3Nl">lolrudy/LaPose<i class="fa fa-external-link-alt"></i></span>的处理方式，首先解压上述文件，并组织如下，注意，data文件夹位于LaPose文件夹下：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">├── camera -&gt; /data1/dataset/nocs/tanmx/lapose/camera</span><br><span class="line">│   ├── train</span><br><span class="line">│   └── val</span><br><span class="line">├── camera_full_depths -&gt; /data1/dataset/nocs/tanmx/lapose/camera_full_depths/</span><br><span class="line">│   ├── train</span><br><span class="line">│   └── val</span><br><span class="line">├── deformnet_eval -&gt; /data1/dataset/nocs/tanmx/lapose/deformnet_eval</span><br><span class="line">│   ├── camera</span><br><span class="line">│   ├── mrcnn_results</span><br><span class="line">│   ├── nocs_results</span><br><span class="line">│   └── real</span><br><span class="line">├── gts -&gt; /data1/dataset/nocs/tanmx/lapose/gts/</span><br><span class="line">│   ├── real_test</span><br><span class="line">│   └── val</span><br><span class="line">├── obj_models -&gt; /data1/dataset/nocs/tanmx/lapose/obj_models/</span><br><span class="line">│   ├── camera_train.pkl</span><br><span class="line">│   ├── camera_val.pkl</span><br><span class="line">│   ├── mug_meta.pkl</span><br><span class="line">│   ├── real_test</span><br><span class="line">│   ├── real_test.pkl</span><br><span class="line">│   ├── real_train</span><br><span class="line">│   ├── real_train.pkl</span><br><span class="line">│   ├── train</span><br><span class="line">│   └── val</span><br><span class="line">├── real -&gt; /data1/dataset/nocs/tanmx/lapose/real/</span><br><span class="line">│   ├── test</span><br><span class="line">│   └── train</span><br><span class="line">└── segmentation_results -&gt; /data1/dataset/nocs/tanmx/lapose/data/segmentation_results/</span><br><span class="line">    ├── CAMERA25</span><br><span class="line">    └── REAL275</span><br><span class="line"></span><br><span class="line">25 directories, 5 files</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">ls</span> -l data/obj_models/</span><br><span class="line">total 31520</span><br><span class="line">drwxr-xr-x  6 tanmx tanmx     4096 May  3 12:11 ./</span><br><span class="line">drwxrwxr-x  9 tanmx tanmx     4096 May  3 12:08 ../</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx 26695895 May  3 12:11 camera_train.pkl</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx  4634293 May  3 12:11 camera_val.pkl</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx    23197 May  3 12:11 mug_meta.pkl</span><br><span class="line">drwxrwxr-x  2 tanmx tanmx     4096 Sep 27  2019 real_test/</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx   443600 May  3 12:11 real_test.pkl</span><br><span class="line">drwxrwxr-x  2 tanmx tanmx     4096 Sep 27  2019 real_train/</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx   443584 May  3 12:11 real_train.pkl</span><br><span class="line">drwxr-xr-x 12 tanmx tanmx     4096 Sep 27  2019 train/</span><br><span class="line">drwxr-xr-x 12 tanmx tanmx     4096 Sep 27  2019 val/</span><br></pre></td></tr></table></figure><p>按顺序执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python prepare_data/pose_data.py</span><br><span class="line">python prepare_data/shape_data.py</span><br></pre></td></tr></table></figure><p>执行<code>python prepare_data/pose_data.py</code>后目录变为：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">├── camera -&gt; /data1/dataset/nocs/tanmx/lapose/camera</span><br><span class="line">│   ├── train</span><br><span class="line">│   ├── train_list_all.txt</span><br><span class="line">│   ├── train_list.txt</span><br><span class="line">│   ├── val</span><br><span class="line">│   ├── val_list_all.txt</span><br><span class="line">│   └── val_list.txt</span><br><span class="line">├── camera_full_depths -&gt; /data1/dataset/nocs/tanmx/lapose/camera_full_depths/</span><br><span class="line">│   ├── train</span><br><span class="line">│   └── val</span><br><span class="line">├── deformnet_eval -&gt; /data1/dataset/nocs/tanmx/lapose/deformnet_eval</span><br><span class="line">│   ├── camera</span><br><span class="line">│   ├── mrcnn_results</span><br><span class="line">│   ├── nocs_results</span><br><span class="line">│   └── real</span><br><span class="line">├── gts -&gt; /data1/dataset/nocs/tanmx/lapose/gts/</span><br><span class="line">│   ├── real_test</span><br><span class="line">│   └── val</span><br><span class="line">├── obj_models -&gt; /data1/dataset/nocs/tanmx/lapose/obj_models/</span><br><span class="line">│   ├── camera_train.pkl</span><br><span class="line">│   ├── camera_val.pkl</span><br><span class="line">│   ├── mug_meta.pkl</span><br><span class="line">│   ├── real_test</span><br><span class="line">│   ├── real_test.pkl</span><br><span class="line">│   ├── real_train</span><br><span class="line">│   ├── real_train.pkl</span><br><span class="line">│   ├── train</span><br><span class="line">│   └── val</span><br><span class="line">├── real -&gt; /data1/dataset/nocs/tanmx/lapose/real/</span><br><span class="line">│   ├── test</span><br><span class="line">│   ├── test_list_all.txt</span><br><span class="line">│   ├── test_list.txt</span><br><span class="line">│   ├── train</span><br><span class="line">│   ├── train_list_all.txt</span><br><span class="line">│   └── train_list.txt</span><br><span class="line">└── segmentation_results -&gt; /data1/dataset/nocs/tanmx/lapose/data/segmentation_results/</span><br><span class="line">    ├── CAMERA25</span><br><span class="line">    └── REAL275</span><br><span class="line"></span><br><span class="line">25 directories, 13 files</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">ls</span> -l data/camera/</span><br><span class="line">total 9192</span><br><span class="line">drwxrwxr-x     4 tanmx tanmx    4096 May  4 05:44 ./</span><br><span class="line">drwxrwxr-x     9 tanmx tanmx    4096 May  3 12:08 ../</span><br><span class="line">drwxr-xr-x 27502 tanmx tanmx  577536 Jun 14  2019 train/</span><br><span class="line">-rw-rw-r--     1 tanmx tanmx 4675000 May  3 12:13 train_list_all.txt</span><br><span class="line">-rw-rw-r--     1 tanmx tanmx 3403978 May  4 03:26 train_list.txt</span><br><span class="line">drwxr-xr-x  2502 tanmx tanmx   69632 Jun 14  2019 val/</span><br><span class="line">-rw-rw-r--     1 tanmx tanmx  375000 May  3 12:13 val_list_all.txt</span><br><span class="line">-rw-rw-r--     1 tanmx tanmx  288300 May  4 05:44 val_list.txt</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">ls</span> -l data/real/</span><br><span class="line">total 288</span><br><span class="line">drwxrwxr-x 4 tanmx tanmx  4096 May  4 06:02 ./</span><br><span class="line">drwxrwxr-x 9 tanmx tanmx  4096 May  3 12:08 ../</span><br><span class="line">drwxrwxr-x 8 tanmx tanmx  4096 Nov 13  2018 <span class="built_in">test</span>/</span><br><span class="line">-rw-rw-r-- 1 tanmx tanmx 49572 May  3 12:13 test_list_all.txt</span><br><span class="line">-rw-rw-r-- 1 tanmx tanmx 49572 May  4 06:02 test_list.txt</span><br><span class="line">drwxrwxr-x 9 tanmx tanmx  4096 Jun 14  2019 train/</span><br><span class="line">-rw-rw-r-- 1 tanmx tanmx 82042 May  3 12:13 train_list_all.txt</span><br><span class="line">-rw-rw-r-- 1 tanmx tanmx 82042 May  4 04:51 train_list.txt</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">ls</span> -l data/obj_models/</span><br><span class="line">total 31520</span><br><span class="line">drwxr-xr-x  6 tanmx tanmx     4096 May  3 12:11 ./</span><br><span class="line">drwxrwxr-x  9 tanmx tanmx     4096 May  3 12:08 ../</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx 26695895 May  3 12:11 camera_train.pkl</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx  4634293 May  3 12:11 camera_val.pkl</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx    23197 May  3 12:11 mug_meta.pkl</span><br><span class="line">drwxrwxr-x  2 tanmx tanmx     4096 Sep 27  2019 real_test/</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx   443600 May  3 12:11 real_test.pkl</span><br><span class="line">drwxrwxr-x  2 tanmx tanmx     4096 Sep 27  2019 real_train/</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx   443584 May  3 12:11 real_train.pkl</span><br><span class="line">drwxr-xr-x 12 tanmx tanmx     4096 Sep 27  2019 train/</span><br><span class="line">drwxr-xr-x 12 tanmx tanmx     4096 Sep 27  2019 val/</span><br></pre></td></tr></table></figure><p>执行<code>python prepare_data/shape_data.py</code>后目录变为：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">├── camera -&gt; /data1/dataset/nocs/tanmx/lapose/camera</span><br><span class="line">│   ├── train</span><br><span class="line">│   ├── train_list_all.txt</span><br><span class="line">│   ├── train_list.txt</span><br><span class="line">│   ├── val</span><br><span class="line">│   ├── val_list_all.txt</span><br><span class="line">│   └── val_list.txt</span><br><span class="line">├── camera_full_depths -&gt; /data1/dataset/nocs/tanmx/lapose/camera_full_depths/</span><br><span class="line">│   ├── train</span><br><span class="line">│   └── val</span><br><span class="line">├── deformnet_eval -&gt; /data1/dataset/nocs/tanmx/lapose/deformnet_eval</span><br><span class="line">│   ├── camera</span><br><span class="line">│   ├── mrcnn_results</span><br><span class="line">│   ├── nocs_results</span><br><span class="line">│   └── real</span><br><span class="line">├── gts -&gt; /data1/dataset/nocs/tanmx/lapose/gts/</span><br><span class="line">│   ├── real_test</span><br><span class="line">│   └── val</span><br><span class="line">├── obj_models -&gt; /data1/dataset/nocs/tanmx/lapose/obj_models/</span><br><span class="line">│   ├── camera_train.pkl</span><br><span class="line">│   ├── camera_val.pkl</span><br><span class="line">│   ├── mug_meta.pkl</span><br><span class="line">│   ├── real_test</span><br><span class="line">│   ├── real_test.pkl</span><br><span class="line">│   ├── real_train</span><br><span class="line">│   ├── real_train.pkl</span><br><span class="line">│   ├── ShapeNetCore_2048.h5</span><br><span class="line">│   ├── ShapeNetCore_4096.h5</span><br><span class="line">│   ├── train</span><br><span class="line">│   └── val</span><br><span class="line">├── real -&gt; /data1/dataset/nocs/tanmx/lapose/real/</span><br><span class="line">│   ├── test</span><br><span class="line">│   ├── test_list_all.txt</span><br><span class="line">│   ├── test_list.txt</span><br><span class="line">│   ├── train</span><br><span class="line">│   ├── train_list_all.txt</span><br><span class="line">│   └── train_list.txt</span><br><span class="line">└── segmentation_results -&gt; /data1/dataset/nocs/tanmx/lapose/data/segmentation_results/</span><br><span class="line">    ├── CAMERA25</span><br><span class="line">    └── REAL275</span><br><span class="line"></span><br><span class="line">25 directories, 15 files</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">ls</span> -l data/camera/</span><br><span class="line">total 9192</span><br><span class="line">drwxrwxr-x     4 tanmx tanmx    4096 May  4 05:44 ./</span><br><span class="line">drwxrwxr-x     9 tanmx tanmx    4096 May  3 12:08 ../</span><br><span class="line">drwxr-xr-x 27502 tanmx tanmx  577536 Jun 14  2019 train/</span><br><span class="line">-rw-rw-r--     1 tanmx tanmx 4675000 May  3 12:13 train_list_all.txt</span><br><span class="line">-rw-rw-r--     1 tanmx tanmx 3403978 May  4 03:26 train_list.txt</span><br><span class="line">drwxr-xr-x  2502 tanmx tanmx   69632 Jun 14  2019 val/</span><br><span class="line">-rw-rw-r--     1 tanmx tanmx  375000 May  3 12:13 val_list_all.txt</span><br><span class="line">-rw-rw-r--     1 tanmx tanmx  288300 May  4 05:44 val_list.txt</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">ls</span> -l data/real/</span><br><span class="line">total 288</span><br><span class="line">drwxrwxr-x 4 tanmx tanmx  4096 May  4 06:02 ./</span><br><span class="line">drwxrwxr-x 9 tanmx tanmx  4096 May  3 12:08 ../</span><br><span class="line">drwxrwxr-x 8 tanmx tanmx  4096 Nov 13  2018 <span class="built_in">test</span>/</span><br><span class="line">-rw-rw-r-- 1 tanmx tanmx 49572 May  3 12:13 test_list_all.txt</span><br><span class="line">-rw-rw-r-- 1 tanmx tanmx 49572 May  4 06:02 test_list.txt</span><br><span class="line">drwxrwxr-x 9 tanmx tanmx  4096 Jun 14  2019 train/</span><br><span class="line">-rw-rw-r-- 1 tanmx tanmx 82042 May  3 12:13 train_list_all.txt</span><br><span class="line">-rw-rw-r-- 1 tanmx tanmx 82042 May  4 04:51 train_list.txt</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">ls</span> -l data/obj_models/</span><br><span class="line">total 116064</span><br><span class="line">drwxr-xr-x  6 tanmx tanmx     4096 May  4 11:15 ./</span><br><span class="line">drwxrwxr-x  9 tanmx tanmx     4096 May  3 12:08 ../</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx 26695895 May  4 10:28 camera_train.pkl</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx  4634293 May  4 10:32 camera_val.pkl</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx    23197 May  4 10:32 mug_meta.pkl</span><br><span class="line">drwxrwxr-x  2 tanmx tanmx     4096 Sep 27  2019 real_test/</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx   443600 May  4 10:32 real_test.pkl</span><br><span class="line">drwxrwxr-x  2 tanmx tanmx     4096 Sep 27  2019 real_train/</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx   443584 May  4 10:32 real_train.pkl</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx 28951728 May  4 11:15 ShapeNetCore_2048.h5</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx 57616135 May  4 10:35 ShapeNetCore_4096.h5</span><br><span class="line">drwxr-xr-x 12 tanmx tanmx     4096 Sep 27  2019 train/</span><br><span class="line">drwxr-xr-x 12 tanmx tanmx     4096 Sep 27  2019 val/</span><br></pre></td></tr></table></figure><h2 id="运行代码">运行代码</h2><p>将数据集组织如下：</p>]]></content>
    
    
    <summary type="html">复现GIVEPose的代码，并进行测试。</summary>
    
    
    
    <category term="代码复现" scheme="https://blog.032802.xyz/categories/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/"/>
    
    
    <category term="Object Pose Estimation" scheme="https://blog.032802.xyz/tags/Object-Pose-Estimation/"/>
    
    <category term="代码复现" scheme="https://blog.032802.xyz/tags/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/"/>
    
    <category term="2025CVPR" scheme="https://blog.032802.xyz/tags/2025CVPR/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】ES6D: A Computation Efficient and Symmetry-Aware 6D Pose Regression Framework</title>
    <link href="https://blog.032802.xyz/paper-reading/2022/es6d-a-computation-efficient-and-symmetry-aware-6d-pose-regression-framework_2022_Mo.html"/>
    <id>https://blog.032802.xyz/paper-reading/2022/es6d-a-computation-efficient-and-symmetry-aware-6d-pose-regression-framework_2022_Mo.html</id>
    <published>2025-04-23T02:11:31.000Z</published>
    <updated>2025-05-08T14:11:31.000Z</updated>
    
    <content type="html"><![CDATA[<h1id="es6d-a-computation-efficient-and-symmetry-aware-6d-pose-regression-framework">ES6D:A Computation Efficient and Symmetry-Aware 6D Pose RegressionFramework</h1><table><colgroup><col style="width: 4%" /><col style="width: 34%" /><col style="width: 10%" /><col style="width: 10%" /><col style="width: 30%" /><col style="width: 8%" /></colgroup><thead><tr><th style="text-align: center;">方法</th><th style="text-align: center;">类型</th><th style="text-align: center;">训练输入</th><th style="text-align: center;">推理输入</th><th style="text-align: center;">输出</th><th style="text-align: center;">pipeline</th></tr></thead><tbody><tr><td style="text-align: center;">ES6D</td><td style="text-align: center;">实例级（该方法针对对称物体设计）</td><td style="text-align: center;">RGBD + CAD</td><td style="text-align: center;">RGBD + CAD</td><td style="text-align: center;">绝对<spanclass="math inline">\(\mathbf{R}, \mathbf{t}\)</span></td><td style="text-align: center;"></td></tr></tbody></table><ul><li>2025.05.08：这篇文章主要解决的是对称物体的6D位姿估计问题，使用GroupedPrimitives(GP)来对物体分类，GP可以将同一类别的物体（类别是GP的类别，而不是物体的类别）抽象为几个点，以避免由形状引起的不确定性；还设计了位姿距离lossA(M)GPD，该loss曲线上的每一个极小值点都可以映射到一个正确的姿态。</li></ul><h2 id="abstract">Abstract</h2><h2 id="introduction">1. Introduction</h2><figure><imgsrc="https://img.032802.xyz/paper-reading/2022/es6d-a-computation-efficient-and-symmetry-aware-6d-pose-regression-framework_2022_Mo/Figure1.webp"alt="Figure 1. Comparison of A(M)GPD and ADD-S. Axis X shows the rotation angle of the object (from 0° to 360°). Axis Y shows the calculated distance. We set the initial pose as the ground truth. As we can see, all minima are mapped to correct poses in the A(M)GPD curve and several minima point to incorrect poses in the ADD-S curve." /><figcaption aria-hidden="true">Figure 1. Comparison of A(M)GPD andADD-S. Axis X shows the rotation angle of the object (from 0° to 360°).Axis Y shows the calculated distance. We set the initial pose as theground truth. As we can see, all minima are mapped to correct poses inthe A(M)GPD curve and several minima point to incorrect poses in theADD-S curve.</figcaption></figure><p><span class="math display">\[\begin{equation}\label{eq1}    l = \text{loss}(p, \hat{p}) = \text{loss}(N(I, w), \hat{p}),\end{equation}\]</span></p><p>In summary, the main contributions of this work are as follows.</p><ul><li>We propose a novel feature extraction network XYZNet for the RGB-Ddata, which is suitable for pose estimation with low computational costand superior performance.</li><li>The compact shape representation GP and the distance metric A(M)GPDare introduced to handle symmetries. The loss based on A(M)GPD canconstrain the regression network to converge to the correct state.</li><li>A numerical simulation and visualization method is carried out toanalyze the validity of the A(M)GPD loss. This analytical method isapplicable to other frameworks in 6D pose estimation.</li><li>The framework ES6D is proposed by using XYZNet and the A(M)GPD lossand achieves competitive performance on the YCB-Video and T-LESSdatasets.</li></ul><h2 id="related-work">2. Related Work</h2><h2 id="the-proposed-method">3. The Proposed Method</h2><h3 id="overview">3.1. Overview</h3><figure><imgsrc="https://img.032802.xyz/paper-reading/2022/es6d-a-computation-efficient-and-symmetry-aware-6d-pose-regression-framework_2022_Mo/Figure2.webp"alt="Figure 2. Network overview. First, the RGB-XYZ data is generated from the RGB-D image. The RGB-XYZ data is fed into a CNN module to extract local features, which encode color and geometry information. Second, the point cloud features are obtained by a PointNet-like CNN module and padded to the same size as the local features. Then, the local features and point cloud features are concatenated as the point-wise features for poses estimation. Finally, the pose with the maximum confidence is chosen as the final result." /><figcaption aria-hidden="true">Figure 2. Network overview. First, theRGB-XYZ data is generated from the RGB-D image. The RGB-XYZ data is fedinto a CNN module to extract local features, which encode color andgeometry information. Second, the point cloud features are obtained by aPointNet-like CNN module and padded to the same size as the localfeatures. Then, the local features and point cloud features areconcatenated as the point-wise features for poses estimation. Finally,the pose with the maximum confidence is chosen as the finalresult.</figcaption></figure><p>本文的目的是从一幅RGBD图像中检测出刚性物体，并在相机坐标系中估计出相应的旋转<spanclass="math inline">\(R \in SO(3)\)</span>和平移<spanclass="math inline">\(\boldsymbol{t} \in\mathbb{R}^3\)</span>。为此提出了如下的两阶段方案。</p><p>在第一阶段，利用“PoseCNN: A Convolutional Neural Network for 6DObject Pose Estimation in ClutteredScenes”的分割网络来获取目标物体的掩码和边界框。由边界框裁剪得到的每个掩码以及RGBD图像块都会被传送到第二阶段。</p><p>在第二阶段，提出了一个名为ES6D的实时框架来估计物体位姿。该框架的流程如图2所示。首先，经过归一化处理后，带掩码的深度像素会被转换为XYZmap。其次，XYZNet从RGB图像块和XYZmap的拼接结果中提取point-wise特征。然后，利用三个卷积头来预测point-wise平移偏移量、四元数以及置信度。最后，选择置信度最高的位姿作为最终结果。</p><h3 id="point-wise-feature-extraction">3.2. Point-wise featureextraction</h3><p>之前的工作“PVN3D: A Deep Point-wise 3D Keypoints Voting Network for6DoF Pose Estimation”和“DenseFusion: 6D Object Pose Estimation byIterative DenseFusion”已经证明，对于6D位姿估计而言，来自RGBD数据的point-wise特征比来自RGB图像的特征更加有效且稳健。最先进的方法PVN3D采用一种异构结构，该结构通过PointNet++获取点云特征，然后通过索引操作将点云特征与RGB特征链接起来。PointNet++通过一系列集合抽象层（SetAbstraction Layers,SAL）来提取局部特征，这些层会在预定义的搜索半径内对点云进行分组。然而，处理大量的点云非常耗时，并且如果我们减少集合抽象层的数量，其表示能力就会下降。2D卷积操作的一个特点是对相邻信息进行分组以提取局部特征。因此，所提出的XYZNet旨在通过对RGB-XYZ图像执行2D卷积操作来同时提取局部特征。</p><p>首先，经过掩码处理的深度像素被转换为点云<spanclass="math inline">\(\mathcal{P} = \{(x_i, y_i, z_i)\}_{i =1}^N\)</span>，然后使用点的中心<spanclass="math inline">\(\boldsymbol{p}_c =\text{mean}(\mathcal{P})\)</span>和一个比例因子<spanclass="math inline">\(\gamma\)</span>，将点<spanclass="math inline">\(P\)</span>进行平移和缩放到<spanclass="math inline">\([-1, 1]\)</span>区间。归一化后的点记为<spanclass="math inline">\(\dot{\mathcal{P}} = \{(\dot{x}_i, \dot{y}_i,\dot{z}_i)\}_{i = 1}^N\)</span>，并被格式化为一个XYZ map。通过将XYZmap与相应的RGB patch进行拼接，就可以得到严格对齐的RGB-XYZ数据。“AUnified Framework for Multi-View Multi-Class Object PoseEstimation”中的方法也采用了2D卷积网络从XYZmap中提取点云特征，但其性能远不如异构结构的方法（PVN3D和DenseFusion）。造成这种情况的主要原因是，当在XYZmap上使用2D卷积操作时，点云的空间信息会被丢弃。基于上述观察，我们设计了XYZNet，如图2所示。</p><p>XYZNet由三个部分组成：</p><ol type="1"><li>局部特征提取模块。使用2D卷积层来学习局部特征。设置不同的卷积核大小和下采样率，以扩大感受野。</li><li>空间信息编码模块。该模块的主要功能是提取点云特征。该模块将局部特征与XYZmap连接起来，以恢复空间结构，并利用<span class="math inline">\(1 \times1\)</span>卷积对每个点的局部特征和坐标进行编码。然后，通过最大池化得到全局特征，并将其与每个点的特征连接起来，以提供全局上下文信息。</li><li>特征聚合。将局部特征和点云特征连接起来作为point-wise特征。两种模态的融合使得位姿估计在纹理较少和严重遮挡的情况下也具有较强的鲁棒性。</li></ol><h3 id="d-pose-regression">3.3. 6D pose regression</h3><p>在完成XYZNet后，会得到point-wise特征集合<span class="math inline">\(F= \{\boldsymbol{f}_i\}_{i = 1}^{N}\)</span>，其中<spanclass="math inline">\(\boldsymbol{f}_i \in\mathbb{R}^d\)</span>。在本小节中，我们将阐述如何利用point-wise特征<spanclass="math inline">\(\boldsymbol{f}_i\)</span>以及对应的可见点<spanclass="math inline">\(\dot{\boldsymbol{p}}_i \in\dot{\mathcal{P}}\)</span>来估计旋转<span class="math inline">\(R_i \inSO(3)\)</span>和平移<span class="math inline">\(\boldsymbol{t}_i \in\mathbb{R}^3\)</span>。如图2所示，采用三个<span class="math inline">\(1\times 1\)</span>卷积头（<spanclass="math inline">\(\mathcal{B}_\mathcal{T}\)</span>、<spanclass="math inline">\(\mathcal{B}_\mathcal{Q}\)</span>、<spanclass="math inline">\(\mathcal{B}_\mathcal{C}\)</span>）来回归平移偏移量（<spanclass="math inline">\(\Delta\dot{\boldsymbol{t}}_i \in\mathbb{R}^3\)</span>）、四元数（<spanclass="math inline">\(\boldsymbol{q}_i \in \mathbb{R}^4\)</span>且<spanclass="math inline">\(\Vert\boldsymbol{q}_i\Vert =1\)</span>）和置信度（<span class="math inline">\(c_i \in [0,1]\)</span>）。</p><p><strong>3D translationregression</strong>：将归一化物体坐标系的原点视为一个虚拟关键点，通过计算可见点<spanclass="math inline">\(\dot{\boldsymbol{p}}_i\)</span>与原点之间的偏移量<spanclass="math inline">\(\Delta\dot{\boldsymbol{t}}_i\)</span>，就可以得到平移量<spanclass="math inline">\(\boldsymbol{t}_i\)</span>。其公式如下：</p><p><span class="math display">\[\begin{equation}\label{eq2}    \Delta\dot{\boldsymbol{t}}_i =\mathcal{B}_\mathcal{T}(\boldsymbol{f}_i),\end{equation}\]</span></p><p><span class="math display">\[\begin{equation}\label{eq3}    \boldsymbol{t}_i = \frac{(\dot{\boldsymbol{p}}_i +\Delta\dot{\boldsymbol{t}}_i)}{\gamma} + \boldsymbol{p}_c,\end{equation}\]</span></p><p>其中，可见点<spanclass="math inline">\(\dot{\boldsymbol{p}}_i\)</span>的偏移量分布在一个特定的球体中。与直接对物体平移进行回归相比，这个回归函数得到的输出空间更小。</p><p><strong>3D rotationregression</strong>：我们按照DenseFusion和PoseCNN的方法，采用四元数来表示旋转。我们得到旋转矩阵的方式如下：</p><p><span class="math display">\[\begin{equation}\label{eq4}    R_i = Quaternion\_matrix(Norm(\mathcal{B}_Q(\boldsymbol{f}_i))),\end{equation}\]</span></p><p><span class="math display">\[\begin{equation}\label{eq5}    Norm(\boldsymbol{q}) =\frac{\boldsymbol{q}_i}{\Vert\boldsymbol{q}_i\Vert},\end{equation}\]</span></p><p>其中，<spanclass="math inline">\(Quaternion\_matrix(\cdot)\)</span>表示将四元数转换为旋转矩阵的函数（ASurvey on the Computation of Quaternions from Rotation Matrices）。</p><p><strong>Confidenceregression</strong>：为了确定最佳的回归结果，我们设置了一个置信度估计头来评估每个特征的置信度<spanclass="math inline">\(c_i\)</span>。其公式如下：</p><p><span class="math display">\[\begin{equation}\label{eq6}    c_i = Sigmoid(\mathcal{B}_C(\boldsymbol{f}_i)),\end{equation}\]</span></p><p>我们采用DenseFusion中提到的自监督方法来训练置信度分支<spanclass="math inline">\(\mathcal{B}_C\)</span>。</p><h3 id="symmetry-aware-loss">3.4. Symmetry-aware loss</h3><p>现有的对称不变距离度量方法依赖于物体的三维形状，例如ADD-S、ACPD、MCPD、VSD（来自“OnEvaluation of 6D Object PoseEstimation”和“DenseFusion”）。然而，独特的形状和点对不匹配是导致错误最小值的原因。此外，在现实中，物体具有各种各样的形状，我们无法保证这些度量方法对每种形状都有效。因此，我们设计了分组基元（GP），将同一类别的物体抽象为几个点，以避免由形状引起的不确定性。此外，我们将这些点分成若干组，并根据公式<spanclass="math inline">\(\eqref{eq12}\)</span>和<spanclass="math inline">\(\eqref{eq13}\)</span>计算同一组中最近点之间的距离，这就避免了点对不匹配的问题。</p><figure><imgsrc="https://img.032802.xyz/paper-reading/2022/es6d-a-computation-efficient-and-symmetry-aware-6d-pose-regression-framework_2022_Mo/GP.webp"alt="Figure 3. The pipeline of the GP construction." /><figcaption aria-hidden="true">Figure 3. The pipeline of the GPconstruction.</figcaption></figure><p><strong>Groupedprimitives</strong>：我们在图3中展示了分组基元（GP）构建的流程。有了特定物体的三维模型后，我们可以根据公式<spanclass="math inline">\(\eqref{eq9}\)</span>和<spanclass="math inline">\(\eqref{eq10}\)</span>计算出所有的对称轴。用于分组的基元由对称轴的端点和物体质心组成。具体来说，需要以下三个步骤。</p><p><strong>Step1</strong>：这里定义并解释了对称轴-轴角的基本性质。物体<spanclass="math inline">\(O\)</span>绕轴<spanclass="math inline">\(\boldsymbol{e} = (e_x, e_y,e_z)\)</span>旋转角度<spanclass="math inline">\(\theta\)</span>后外观保持不变，那么轴<spanclass="math inline">\(\boldsymbol{e}\)</span>就是物体<spanclass="math inline">\(O\)</span>的一条对称轴。轴<spanclass="math inline">\(\boldsymbol{e}\)</span>和角度<spanclass="math inline">\(\theta\)</span>构成了一个对称轴-轴角<spanclass="math inline">\(\boldsymbol{a}\)</span>，其定义如下：</p><p><span class="math display">\[\begin{equation}\label{eq7}    \boldsymbol{a} = (\boldsymbol{e}, \theta), \quad\Vert\boldsymbol{e}\Vert = 1 \wedge \theta \in \{\frac{2\pi}{i}\}_{i =2}^M.\end{equation}\]</span></p><p>需要注意的是，<spanclass="math inline">\(2\pi\)</span>必须是对称角度<spanclass="math inline">\(\theta\)</span>的整数倍（Symmetry），并且对称轴-轴角<spanclass="math inline">\(\boldsymbol{a}\)</span>的阶数可以定义为：</p><p><span class="math display">\[\begin{equation}\label{eq8}    |\boldsymbol{a}| = \frac{2\pi}{\theta}(\boldsymbol{a}).\end{equation}\]</span></p><figure><imgsrc="https://img.032802.xyz/paper-reading/2022/es6d-a-computation-efficient-and-symmetry-aware-6d-pose-regression-framework_2022_Mo/Figure4.webp"alt="Figure 4. Grouped primitives and the visualization of A(M)GPD landscape. Based on the size of AO and ACO, symmetric objects can be classified into five categories. For each category, a typical toy model and its grouped primitives are presented in the first row plots. The second row shows the A(M)GPD landscape of each object in the rotation space, where the darker color represents the smaller value of A(M)GPD. The third row shows the minima in each landscape. Best viewed in color." /><figcaption aria-hidden="true">Figure 4. Grouped primitives and thevisualization of A(M)GPD landscape. Based on the size of AO and ACO,symmetric objects can be classified into five categories. For eachcategory, a typical toy model and its grouped primitives are presentedin the first row plots. The second row shows the A(M)GPD landscape ofeach object in the rotation space, where the darker color represents thesmaller value of A(M)GPD. The third row shows the minima in eachlandscape. Best viewed in color.</figcaption></figure><p>对称轴-轴角是一种冗余的表示形式。例如，对于图4中类别2的物体，如金字塔，它有四个对称轴-轴角：<spanclass="math inline">\((\boldsymbol{e}, \frac{\pi}{2})\)</span>、<spanclass="math inline">\((\boldsymbol{e}, \pi)\)</span>、<spanclass="math inline">\((-\boldsymbol{e}, \frac{\pi}{2})\)</span>和<spanclass="math inline">\((-\boldsymbol{e}, \pi)\)</span>，其中<spanclass="math inline">\(\boldsymbol{e}\)</span>与绿色线条平行。在这种情况下，由于对称轴<spanclass="math inline">\(\boldsymbol{e}\)</span>相同，这四个对称轴-轴角对于该物体而言具有相同的含义。由于旋转对称的周期性，这四个对称轴-轴角的角度必定存在最大公约数<spanclass="math inline">\(\frac{\pi}{2}\)</span>。需要注意的是，在本研究中仅使用角度为最大公约数的对称轴-轴角，例如<spanclass="math inline">\((\boldsymbol{e}, \frac{\pi}{2})\)</span>和<spanclass="math inline">\((-\boldsymbol{e}, \frac{\pi}{2})\)</span>。</p><p><strong>Step2</strong>：在以物体质心为原点的物体坐标系中，可以通过使用以下公式得到物体<spanclass="math inline">\(O\)</span>的一组粗略的对称轴-轴角：</p><p><span class="math display">\[\begin{equation}\label{eq9}    \hat{A}_O = \{\boldsymbol{a}|h(P_O, R(\boldsymbol{a})P_O) &lt;\epsilon\},\end{equation}\]</span></p><p>其中<span class="math inline">\(h\)</span>是Hausdorff距离，<spanclass="math inline">\(P_O\)</span>表示物体模型的顶点，<spanclass="math inline">\(R(\mathbf{a})\)</span>是对称轴-轴角<spanclass="math inline">\(\mathbf{a}\)</span>对应的旋转矩阵，并且允许的偏差由<spanclass="math inline">\(\varepsilon\)</span>界定。然后，基于这些对称轴，应用MeanShiftclustering algorithm（来自“Mean shift: a robust approach toward featurespace analysis”）来简化<spanclass="math inline">\(\hat{A}_O\)</span>：</p><p><span class="math display">\[\begin{equation}\label{eq10}    A_O = Mean\_Shift(\hat{A}_O),\end{equation}\]</span></p><p>此时，<span class="math inline">\(A_O\)</span>包含了物体<spanclass="math inline">\(O\)</span>的所有无冗余的对称轴-轴角，其中<spanclass="math inline">\(|A_O|\)</span>是<spanclass="math inline">\(A_O\)</span>的大小，且为<spanclass="math inline">\(2\)</span>的倍数，因为对称轴-轴角总是成对出现，例如<spanclass="math inline">\((\mathbf{e}, \frac{\pi}{2})\)</span>和<spanclass="math inline">\((-\mathbf{e},\frac{\pi}{2})\)</span>。此外，可以得到<spanclass="math inline">\(A_O\)</span>的一个子集<spanclass="math inline">\(AC_{O}\)</span>如下：</p><p><span class="math display">\[\begin{equation}\label{eq11}    AC_O = \{\boldsymbol{a} | \boldsymbol{a} \in A_O \wedge|\boldsymbol{a}| &gt; \rho\},\end{equation}\]</span></p><p>其中<span class="math inline">\(\rho\)</span>是松弛阈值。当<spanclass="math inline">\(|\boldsymbol{a}| &gt; \rho\)</span>时，我们将<spanclass="math inline">\(\boldsymbol{a}\)</span>视为连续的对称轴-轴角，并且当<spanclass="math inline">\(\rho\)</span>设置为<spanclass="math inline">\(6\)</span>时，涵盖了大多数应用情况，包括实验部分中要评估的所有物体。根据<spanclass="math inline">\(A_O\)</span>和<spanclass="math inline">\(AC_O\)</span>的大小，对称物体可以分为五类，如图4所示。</p><p><strong>Step 3</strong>：如图3所示，如果基元<spanclass="math inline">\(A\)</span>绕对称轴旋转特定角度后能够与基元<spanclass="math inline">\(B\)</span>重合，那么我们就认为基元<spanclass="math inline">\(A\)</span>和<spanclass="math inline">\(B\)</span>属于同一组。分组后的基元记为<spanclass="math inline">\(G = \{g_i\}_{i = 0}^K\)</span>，其中<spanclass="math inline">\(K\)</span>是集合<spanclass="math inline">\(G\)</span>的大小。分组原则的详细内容见补充材料。</p><p><strong>Pose distancemetric</strong>：基于分组基元（GP），设计了位姿距离度量A(M)GPD。A(M)GPD包含两个函数，第一个函数是平均分组基元距离（AGPD）：</p><p><span class="math display">\[\begin{equation}\label{eq12}    AGPD = \text{mean}_{g_i \in G}\ \text{mean}_{p_j \in g_i}\ \min_{p_k\in g_i, k \ne j} \Vert \hat{p}_j - \dot{p}_k \Vert,\end{equation}\]</span></p><p>其中<span class="math inline">\(\hat{p} = \hat{T}p\)</span>，<spanclass="math inline">\(\dot{p} = \dot{T}p\)</span>，<spanclass="math inline">\(p \in g(G)\)</span>，并且<spanclass="math inline">\(\hat{T}\)</span>，<spanclass="math inline">\(\dot{T} \in SE(3)\)</span>。当物体<spanclass="math inline">\(O\)</span>属于对称类别<spanclass="math inline">\(\{1, 3, 4,5\}\)</span>中的一个或者是非对称物体时，平均分组基元距离（AGPD）用于度量物体<spanclass="math inline">\(O\)</span>的两个位姿之间的距离。</p><p>第2类与其他类别不同。它只有一对对称轴，且这对对称轴具有有限的阶数。如果将平均分组基元距离（AGPD）用作损失函数，这种特性会在旋转空间中导致错误的最小值，如图1的第二行所示。为了解决这个问题，引入了第二个函数——最大分组基元距离（MGPD）：</p><p><span class="math display">\[\begin{equation}\label{eq13}    MGPD = \max_{g_i \in G} \max_{p_j \in g_i} \min_{p_k \in g_i, k \nej} \Vert \hat{p}_j - \dot{p}_k \Vert.\end{equation}\]</span></p><p><strong>Loss for regressiontraining</strong>：我们回归框架的总损失与DenseFusion中的损失类似，不同之处在于，我们使用A(M)GPD来计算预测值与真实值之间的误差，而不是使用ADD(S)。</p><h3 id="validation-of-amgpd">3.5. Validation of A(M)GPD</h3><p>在本小节中，提出了一种数值计算与可视化方法，用于检验A(M)GPD是否满足引言中所述的要求（1）allminima in the loss surface are mapped to the correctposes。为了更清晰地了解A(M)GPD在<span class="math inline">\(R \inSO(3)\)</span>上的情况，我们首先采用采样技术生成<spanclass="math inline">\(N\)</span>个旋转<span class="math inline">\(RC =\{R_i\}_{i = 1}^N\)</span>，这些旋转在<span class="math inline">\(R \inSO(3)\)</span>上密集分布。其次，将单位矩阵<spanclass="math inline">\(I_{3 \times 3}\)</span>视为真实值，而<spanclass="math inline">\(\dot{R} \in RC\)</span>作为预测值。<spanclass="math inline">\(I_{3 \times 3}\)</span>与<spanclass="math inline">\(\dot{R}\)</span>之间的A(M)GPD可表示为<spanclass="math inline">\(\dot{d}\)</span>。</p><p><span class="math display">\[\begin{equation}\label{eq14}    \dot{d} = \text{A(M)GPD}(I_{3 \times 3}, \dot{R}).\end{equation}\]</span></p><p>然后，我们借助旋转向量<span class="math inline">\(\boldsymbol{v} =(v_x, v_y, v_z)\)</span>来可视化<spanclass="math inline">\(\dot{d}\)</span>，其中向量的方向为旋转轴，长度为旋转角度<spanclass="math inline">\(\theta \in [0,\pi]\)</span>。如图4中第二行的图所示，<spanclass="math inline">\(\dot{R}\)</span>的坐标为<spanclass="math inline">\(\boldsymbol{v}(\dot{R})\)</span>，<spanclass="math inline">\(\dot{R}\)</span>的颜色值对应着相应的<spanclass="math inline">\(\dot{d}\)</span>（颜色越深表示<spanclass="math inline">\(\dot{d}\)</span>越小）。然而，在这些图中很难找到最小值，所以我们通过一个简单的算法进一步模拟梯度下降的过程。该算法的原理是<spanclass="math inline">\(\boldsymbol{v}(\dot{R})\)</span>不断地向<spanclass="math inline">\(\boldsymbol{v}(\hat{R})\)</span>移动，<spanclass="math inline">\(\boldsymbol{v}(\hat{R})\)</span>在<spanclass="math inline">\(\boldsymbol{v}(\dot{R})\)</span>的邻域内具有最小的<spanclass="math inline">\(\hat{d}\)</span>，并且这个点最终会停在一个局部最小值处。我们对每个<spanclass="math inline">\(\boldsymbol{v}(\dot{R})\)</span>都应用这个原理，并且在图4第三行的图中用红色星号标记出找到的最小值。如我们所见，所有的最小值都被映射到了正确的位姿上。其他物体的情况在补充材料中给出。</p><h2 id="experiments">4. Experiments</h2><figure><imgsrc="https://img.032802.xyz/paper-reading/2022/es6d-a-computation-efficient-and-symmetry-aware-6d-pose-regression-framework_2022_Mo/Figure5.webp"alt="Figure 5. Visualization on the T-LESS dataset with different training loss. The green, red, and blue lines represent the ground truth pose, the result from A(M)GPD loss, and the result from ADD(S) loss, respectively." /><figcaption aria-hidden="true">Figure 5. Visualization on the T-LESSdataset with different training loss. The green, red, and blue linesrepresent the ground truth pose, the result from A(M)GPD loss, and theresult from ADD(S) loss, respectively.</figcaption></figure><h2 id="limitations">5. Limitations</h2><h2 id="conclusion">6. Conclusion</h2><h2 id="supplementary-material">Supplementary Material</h2><figure><imgsrc="https://img.032802.xyz/paper-reading/2022/es6d-a-computation-efficient-and-symmetry-aware-6d-pose-regression-framework_2022_Mo/supplementary_fig1_XYZNet_new.webp"alt="Figure 1. The details of XYZNet." /><figcaption aria-hidden="true">Figure 1. The details ofXYZNet.</figcaption></figure><figure><imgsrc="https://img.032802.xyz/paper-reading/2022/es6d-a-computation-efficient-and-symmetry-aware-6d-pose-regression-framework_2022_Mo/supplementary_fig2_ycb.webp"alt="Figure 2. The details of grouped primitives in YCB-Video dataset. The first plot is the raw GP of objects, and the second plot is the processed GP of objects." /><figcaption aria-hidden="true">Figure 2. The details of groupedprimitives in YCB-Video dataset. The first plot is the raw GP ofobjects, and the second plot is the processed GP ofobjects.</figcaption></figure><figure><imgsrc="https://img.032802.xyz/paper-reading/2022/es6d-a-computation-efficient-and-symmetry-aware-6d-pose-regression-framework_2022_Mo/supplementary_fig3_ycb2.webp"alt="Figure 3. The validation of processed grouped primitives in YCB-Video dataset. For each object, the first column presents the grouped primitives. The second shows the A(M)GPD landscape in the rotation space, where the darker color represents the smaller value of A(M)GPD. The third column reveals the minima in each landscape. Best viewed in color." /><figcaption aria-hidden="true">Figure 3. The validation of processedgrouped primitives in YCB-Video dataset. For each object, the firstcolumn presents the grouped primitives. The second shows the A(M)GPDlandscape in the rotation space, where the darker color represents thesmaller value of A(M)GPD. The third column reveals the minima in eachlandscape. Best viewed in color.</figcaption></figure><figure><imgsrc="https://img.032802.xyz/paper-reading/2022/es6d-a-computation-efficient-and-symmetry-aware-6d-pose-regression-framework_2022_Mo/supplementary_fig4_C2.webp"alt="Figure 4. More instances of category 2. For each example, the first column presents the grouped primitives. The second shows the A(M)GPD landscape in the rotation space, where the darker color represents the smaller value of A(M)GPD. The third column reveals the minima in each landscape. Best viewed in color." /><figcaption aria-hidden="true">Figure 4. More instances of category 2.For each example, the first column presents the grouped primitives. Thesecond shows the A(M)GPD landscape in the rotation space, where thedarker color represents the smaller value of A(M)GPD. The third columnreveals the minima in each landscape. Best viewed in color.</figcaption></figure><figure><imgsrc="https://img.032802.xyz/paper-reading/2022/es6d-a-computation-efficient-and-symmetry-aware-6d-pose-regression-framework_2022_Mo/supplementary_fig5_C5.webp"alt="Figure 5. More instances of category 5. For each example, the first column presents the grouped primitives. The second shows the A(M)GPD landscape in the rotation space, where the darker color represents the smaller value of A(M)GPD. The third column reveals the minima in each landscape. Best viewed in color." /><figcaption aria-hidden="true">Figure 5. More instances of category 5.For each example, the first column presents the grouped primitives. Thesecond shows the A(M)GPD landscape in the rotation space, where thedarker color represents the smaller value of A(M)GPD. The third columnreveals the minima in each landscape. Best viewed in color.</figcaption></figure><figure><imgsrc="https://img.032802.xyz/paper-reading/2022/es6d-a-computation-efficient-and-symmetry-aware-6d-pose-regression-framework_2022_Mo/supplementary_fig6_3333.webp"alt="Figure 6. Visualization for 051_large_clamp and 052_extra_large_clamp on the YCB-Video testing dataset. The 051_large_clamp and 052_extra_large_clamp are marked with the red rectangle. The mask result comes from [5]." /><figcaption aria-hidden="true">Figure 6. Visualization for051_large_clamp and 052_extra_large_clamp on the YCB-Video testingdataset. The 051_large_clamp and 052_extra_large_clamp are marked withthe red rectangle. The mask result comes from [5].</figcaption></figure><figure><imgsrc="https://img.032802.xyz/paper-reading/2022/es6d-a-computation-efficient-and-symmetry-aware-6d-pose-regression-framework_2022_Mo/supplementary_fig7_1111.webp"alt="Figure 7. Visualization on the T-LESS dataset with different training loss. The green, red, and blue lines represent the ground truth pose, the result from A(M)GPD loss, and the result from ADD(S) loss, respectively." /><figcaption aria-hidden="true">Figure 7. Visualization on the T-LESSdataset with different training loss. The green, red, and blue linesrepresent the ground truth pose, the result from A(M)GPD loss, and theresult from ADD(S) loss, respectively.</figcaption></figure><figure><imgsrc="https://img.032802.xyz/paper-reading/2022/es6d-a-computation-efficient-and-symmetry-aware-6d-pose-regression-framework_2022_Mo/supplementary_fig8_2222.webp"alt="Figure 8. Visualization on the T-LESS dataset with different training loss. The green, red, and blue lines represent the ground truth pose, the result from A(M)GPD loss, and the result from ADD(S) loss, respectively." /><figcaption aria-hidden="true">Figure 8. Visualization on the T-LESSdataset with different training loss. The green, red, and blue linesrepresent the ground truth pose, the result from A(M)GPD loss, and theresult from ADD(S) loss, respectively.</figcaption></figure><figure><imgsrc="https://img.032802.xyz/paper-reading/2022/es6d-a-computation-efficient-and-symmetry-aware-6d-pose-regression-framework_2022_Mo/supplementary_fig9_Figure4.webp"alt="Figure 9. Demonstration of grouping. The first row shows the grouping operation for category 2, and the second row is for category 5." /><figcaption aria-hidden="true">Figure 9. Demonstration of grouping. Thefirst row shows the grouping operation for category 2, and the secondrow is for category 5.</figcaption></figure><div class="pdf-container" data-target="https://arxiv.org/pdf/2204.01080" data-height="500px"></div>]]></content>
    
    
    <summary type="html">In this paper, a computation efficient regression framework is presented for estimating the 6D pose of rigid objects from a single RGB-D image, which is applicable to handling symmetric objects. This framework is designed in a simple architecture that efficiently extracts point-wise features from RGB-D data using a fully convolutional network, called XYZNet, and directly regresses the 6D pose without any post refinement. In the case of symmetric object, one object has multiple ground-truth poses, and this one-to-many relationship may lead to estimation ambiguity. In order to solve this ambiguity problem, we design a symmetry-invariant pose distance metric, called average (maximum) grouped primitives distance or A(M)GPD. The proposed A(M)GPD loss can make the regression network converge to the correct state, i.e., all minima in the A(M)GPD loss surface are mapped to the correct poses. Extensive experiments on YCB-Video and TLESS datasets demonstrate the proposed framework&#39;s substantially superior performance in top accuracy and low computational cost. The relevant code is available in https://github.com/GANWANSHUI/ES6D.git.</summary>
    
    
    
    <category term="读万卷书" scheme="https://blog.032802.xyz/categories/%E8%AF%BB%E4%B8%87%E5%8D%B7%E4%B9%A6/"/>
    
    
    <category term="Object Pose Estimation" scheme="https://blog.032802.xyz/tags/Object-Pose-Estimation/"/>
    
    <category term="2022CVPR" scheme="https://blog.032802.xyz/tags/2022CVPR/"/>
    
  </entry>
  
  <entry>
    <title>【代码复现】Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation</title>
    <link href="https://blog.032802.xyz/code-running/Leeiieeo_AG-Pose.html"/>
    <id>https://blog.032802.xyz/code-running/Leeiieeo_AG-Pose.html</id>
    <published>2025-04-22T12:50:45.000Z</published>
    <updated>2025-05-08T06:10:45.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装ag-pose环境">安装AG-Pose环境</h2><ol type="1"><li><p>创建环境：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create --name agpose python=3.9</span><br></pre></td></tr></table></figure></p></li><li><p>激活环境：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate agpose</span><br></pre></td></tr></table></figure></p></li><li><p>安装PyTorch：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install torch==1.12 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu113</span><br></pre></td></tr></table></figure></p></li><li><p>安装其他依赖：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pip3 install gorilla-core==0.2.5.3</span><br><span class="line">pip3 install opencv-python</span><br><span class="line"></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/Leeiieeo/AG-Pose.git</span><br><span class="line"><span class="built_in">cd</span> AG-Pose/model/pointnet2</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure></p></li><li><p>Clone <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21lbnRpYW4vb2JqZWN0LWRlZm9ybW5ldA==">mentian/object-deformnet<i class="fa fa-external-link-alt"></i></span>并安装<code>nn_distance</code>，需要使用该仓库中的代码进行数据处理：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/mentian/object-deformnet.git</span><br><span class="line"><span class="built_in">cd</span> object-deformnet/lib/nn_distance</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure></p></li></ol><h2 id="下载数据集">下载数据集</h2><ol type="1"><li><p>下载NOCS数据集到数据集文件夹中：</p><ul><li><span class="exturl" data-url="aHR0cDovL2Rvd25sb2FkLmNzLnN0YW5mb3JkLmVkdS9vcmlvbi9ub2NzL2NhbWVyYV9jb21wb3NlZF9kZXB0aC56aXA=">http://download.cs.stanford.edu/orion/nocs/camera_composed_depth.zip<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2Rvd25sb2FkLmNzLnN0YW5mb3JkLmVkdS9vcmlvbi9ub2NzL2NhbWVyYV90cmFpbi56aXA=">http://download.cs.stanford.edu/orion/nocs/camera_train.zip<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2Rvd25sb2FkLmNzLnN0YW5mb3JkLmVkdS9vcmlvbi9ub2NzL2NhbWVyYV92YWwyNUsuemlw">http://download.cs.stanford.edu/orion/nocs/camera_val25K.zip<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2Rvd25sb2FkLmNzLnN0YW5mb3JkLmVkdS9vcmlvbi9ub2NzL2d0cy56aXA=">http://download.cs.stanford.edu/orion/nocs/gts.zip<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2Rvd25sb2FkLmNzLnN0YW5mb3JkLmVkdS9vcmlvbi9ub2NzL29ial9tb2RlbHMuemlw">http://download.cs.stanford.edu/orion/nocs/obj_models.zip<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2Rvd25sb2FkLmNzLnN0YW5mb3JkLmVkdS9vcmlvbi9ub2NzL3JlYWxfdGVzdC56aXA=">http://download.cs.stanford.edu/orion/nocs/real_test.zip<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cDovL2Rvd25sb2FkLmNzLnN0YW5mb3JkLmVkdS9vcmlvbi9ub2NzL3JlYWxfdHJhaW4uemlw">http://download.cs.stanford.edu/orion/nocs/real_train.zip<i class="fa fa-external-link-alt"></i></span></li></ul></li><li><p>下载<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0ppZWhvbmdMaW4vU2VsZi1EUERO">JiehongLin/Self-DPDN<i class="fa fa-external-link-alt"></i></span>的分割结果到仓库文件夹中：</p><ul><li><span class="exturl" data-url="aHR0cHM6Ly9kcml2ZS5nb29nbGUuY29tL2ZpbGUvZC8xaE5tTlJyN1lSQ2dnLWNfcWR2YUl6S0VkMmc0S2FjM3cv">https://drive.google.com/file/d/1hNmNRr7YRCgg-c_qdvaIzKEd2g4Kac3w/<i class="fa fa-external-link-alt"></i></span></li></ul></li><li><p>下载使用<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21lbnRpYW4vb2JqZWN0LWRlZm9ybW5ldA==">mentian/object-deformnet<i class="fa fa-external-link-alt"></i></span>进行数据预处理时用到的数据到仓库文件夹中：</p><ul><li><span class="exturl" data-url="aHR0cHM6Ly9kcml2ZS5nb29nbGUuY29tL2ZpbGUvZC8xcDcyTmRZNEJpZV9zcmE5VTh6b1VOSTRmVHJRWmRibmMv">https://drive.google.com/file/d/1p72NdY4Bie_sra9U8zoUNI4fTrQZdbnc/<i class="fa fa-external-link-alt"></i></span></li></ul></li><li><p>下载运行AG-Pose时使用到的分割结果：</p><ul><li><span class="exturl" data-url="aHR0cDovL2hvbWUudXN0Yy5lZHUuY24vfmxsaW54aWFvL3NlZ21lbnRhdGlvbl9yZXN1bHRzLnppcA==">http://home.ustc.edu.cn/~llinxiao/segmentation_results.zip<i class="fa fa-external-link-alt"></i></span></li></ul></li><li><p>下载NOCS数据集中缺失的物体模型（From: <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21lbnRpYW4vb2JqZWN0LWRlZm9ybW5ldC9pc3N1ZXMvMyNpc3N1ZWNvbW1lbnQtNjk4ODU4MzMy">https://github.com/mentian/object-deformnet/issues/3#issuecomment-698858332<i class="fa fa-external-link-alt"></i></span>）到数据集文件夹中：</p><ul><li><span class="exturl" data-url="aHR0cHM6Ly9kcml2ZS5nb29nbGUuY29tL2ZpbGUvZC8xcldreEVWSkpoX2tXSXF4dWRuX2k2c0otaGhkMEU3VFYv">https://drive.google.com/file/d/1rWkxEVJJh_kWIqxudn_i6sJ-hhd0E7TV/<i class="fa fa-external-link-alt"></i></span></li></ul></li></ol><h2 id="处理数据集">处理数据集</h2><h3 id="jiehonglinself-dpdn">JiehongLin/Self-DPDN</h3><p>遵循<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0ppZWhvbmdMaW4vU2VsZi1EUERO">JiehongLin/Self-DPDN<i class="fa fa-external-link-alt"></i></span>的处理方式，首先解压上述文件，并组织如下，注意，data文件夹位于Self-DPDN文件夹下：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">├── camera -&gt; /data1/dataset/nocs/tanmx/dpdn/camera</span><br><span class="line">│   ├── train</span><br><span class="line">│   └── val</span><br><span class="line">├── camera_full_depths -&gt; /data1/dataset/nocs/tanmx/dpdn/camera_full_depths/</span><br><span class="line">│   ├── train</span><br><span class="line">│   └── val</span><br><span class="line">├── gts -&gt; /data1/dataset/nocs/tanmx/dpdn/gts/</span><br><span class="line">│   ├── real_test</span><br><span class="line">│   └── val</span><br><span class="line">├── mean_shapes.npy</span><br><span class="line">├── obj_models -&gt; /data1/dataset/nocs/tanmx/dpdn/obj_models/</span><br><span class="line">│   ├── real_test</span><br><span class="line">│   ├── real_train</span><br><span class="line">│   ├── train</span><br><span class="line">│   └── val</span><br><span class="line">├── real -&gt; /data1/dataset/nocs/tanmx/dpdn/real/</span><br><span class="line">│   ├── test</span><br><span class="line">│   └── train</span><br><span class="line">└── segmentation_results -&gt; /data1/dataset/nocs/tanmx/dpdn/segmentation_results/</span><br><span class="line">    ├── test_trainedwithMask</span><br><span class="line">    ├── test_trainedwoMask</span><br><span class="line">    └── train_trainedwoMask</span><br><span class="line"></span><br><span class="line">21 directories, 1 file</span><br></pre></td></tr></table></figure><p>进入<code>obj_models/val/02876657/d3b53f56b4a7b3b3c9f016d57db96408</code>文件夹，其中内容为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">ls</span></span><br><span class="line">bbox.txt  model.mtl  model.obj</span><br></pre></td></tr></table></figure><p>查看<code>model.obj</code>文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cat</span> model.obj</span><br><span class="line">newmtl Material.57774a50243e5d429f3a4ea105ec3930</span><br><span class="line">d 0.5</span><br><span class="line">Tr 0.5</span><br><span class="line">Kd 0.392157 0.584314 0.929412</span><br><span class="line">newmtl Material.90b00bf69165eda831fe702671104217</span><br><span class="line">Tr 0.0</span><br><span class="line">Kd 0.6 0.117647 0.117647</span><br><span class="line">newmtl Material.dc7a3ca9ca091ddbf48733c3a604f557</span><br><span class="line">Tr 0.0</span><br><span class="line">Kd 1.0 1.0 1.0</span><br><span class="line">newmtl Material.b2cdaf4394d1a4f850458878b81c7fa1</span><br><span class="line">Tr 0.0</span><br><span class="line">Kd 0.8 0.6 0.0</span><br></pre></td></tr></table></figure><p>解压<span class="exturl" data-url="aHR0cHM6Ly9kcml2ZS5nb29nbGUuY29tL2ZpbGUvZC8xcldreEVWSkpoX2tXSXF4dWRuX2k2c0otaGhkMEU3VFYv">https://drive.google.com/file/d/1rWkxEVJJh_kWIqxudn_i6sJ-hhd0E7TV/<i class="fa fa-external-link-alt"></i></span>中的文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ unzip obj_models.zip</span><br><span class="line">$ <span class="built_in">ls</span></span><br><span class="line">02876657  obj_models.zip</span><br><span class="line">$ <span class="built_in">ls</span> 02876657/d3b53f56b4a7b3b3c9f016d57db96408/</span><br><span class="line">bbox.txt  info.txt  model_bad.mtl  model_bad.obj  model.mtl  model.obj</span><br></pre></td></tr></table></figure><p>删除原文件夹<code>obj_models/val/02876657/d3b53f56b4a7b3b3c9f016d57db96408</code>中的内容，将<code>02876657/d3b53f56b4a7b3b3c9f016d57db96408</code>文件夹中的内容复制到该文件夹中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">rm</span> obj_models/val/02876657/d3b53f56b4a7b3b3c9f016d57db96408/*</span><br><span class="line">$ <span class="built_in">cp</span> 02876657/d3b53f56b4a7b3b3c9f016d57db96408/* obj_models/val/02876657/d3b53f56b4a7b3b3c9f016d57db96408/</span><br><span class="line">$ <span class="built_in">ls</span> obj_models/val/02876657/d3b53f56b4a7b3b3c9f016d57db96408</span><br><span class="line">bbox.txt  info.txt  model_bad.mtl  model_bad.obj  model.mtl  model.obj</span><br></pre></td></tr></table></figure><p>执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python data_processing.py</span><br></pre></td></tr></table></figure><p>执行后目录变为：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">├── camera -&gt; /data1/dataset/nocs/tanmx/dpdn/camera</span><br><span class="line">│   ├── train</span><br><span class="line">│   ├── train_list_all.txt</span><br><span class="line">│   ├── train_list.txt</span><br><span class="line">│   ├── val</span><br><span class="line">│   └── val_list_all.txt</span><br><span class="line">├── camera_full_depths -&gt; /data1/dataset/nocs/tanmx/dpdn/camera_full_depths/</span><br><span class="line">│   ├── train</span><br><span class="line">│   └── val</span><br><span class="line">├── gts -&gt; /data1/dataset/nocs/tanmx/dpdn/gts/</span><br><span class="line">│   ├── real_test</span><br><span class="line">│   └── val</span><br><span class="line">├── mean_shapes.npy</span><br><span class="line">├── obj_models -&gt; /data1/dataset/nocs/tanmx/dpdn/obj_models/</span><br><span class="line">│   ├── real_test</span><br><span class="line">│   ├── real_train</span><br><span class="line">│   ├── train</span><br><span class="line">│   └── val</span><br><span class="line">├── real -&gt; /data1/dataset/nocs/tanmx/dpdn/real/</span><br><span class="line">│   ├── test</span><br><span class="line">│   ├── test_list_all.txt</span><br><span class="line">│   ├── train</span><br><span class="line">│   ├── train_list_all.txt</span><br><span class="line">│   └── train_list.txt</span><br><span class="line">└── segmentation_results -&gt; /data1/dataset/nocs/tanmx/dpdn/segmentation_results</span><br><span class="line">    ├── test_trainedwithMask</span><br><span class="line">    ├── test_trainedwoMask</span><br><span class="line">    └── train_trainedwoMask</span><br><span class="line"></span><br><span class="line">21 directories, 7 files</span><br></pre></td></tr></table></figure><p>执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">ls</span> -l data/camera/</span><br><span class="line">total 9712</span><br><span class="line">drwxr-xr-x 27502 tanmx tanmx  577536 Jun 14  2019 train</span><br><span class="line">-rw-rw-r--     1 tanmx tanmx 4675000 Apr 29 11:00 train_list_all.txt</span><br><span class="line">-rw-rw-r--     1 tanmx tanmx 4235159 Apr 29 15:57 train_list.txt</span><br><span class="line">drwxr-xr-x  2502 tanmx tanmx   69632 Jun 14  2019 val</span><br><span class="line">-rw-rw-r--     1 tanmx tanmx  375000 Apr 29 11:00 val_list_all.txt</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">ls</span> -l data/real/</span><br><span class="line">total 228</span><br><span class="line">drwxrwxr-x 8 tanmx tanmx  4096 Nov 13  2018 <span class="built_in">test</span></span><br><span class="line">-rw-rw-r-- 1 tanmx tanmx 49572 Apr 29 11:00 test_list_all.txt</span><br><span class="line">drwxrwxr-x 9 tanmx tanmx  4096 Jun 14  2019 train</span><br><span class="line">-rw-rw-r-- 1 tanmx tanmx 82042 Apr 29 11:00 train_list_all.txt</span><br><span class="line">-rw-rw-r-- 1 tanmx tanmx 82042 Apr 29 16:09 train_list.txt</span><br></pre></td></tr></table></figure><h3 id="mentianobject-deformnet">mentian/object-deformnet</h3><p>遵循<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21lbnRpYW4vb2JqZWN0LWRlZm9ybW5ldA==">mentian/object-deformnet<i class="fa fa-external-link-alt"></i></span>的处理方式，首先解压上述文件，并组织如下，注意，data文件夹位于object-deformnet文件夹下：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">├── camera -&gt; /data1/dataset/nocs/tanmx/spd/camera/</span><br><span class="line">│   ├── train</span><br><span class="line">│   └── val</span><br><span class="line">├── deformnet_eval -&gt; /data1/dataset/nocs/tanmx/spd/deformnet_eval/</span><br><span class="line">│   ├── camera</span><br><span class="line">│   ├── mrcnn_results</span><br><span class="line">│   ├── nocs_results</span><br><span class="line">│   └── real</span><br><span class="line">├── gts -&gt; /data1/dataset/nocs/tanmx/spd/gts/</span><br><span class="line">│   ├── real_test</span><br><span class="line">│   └── val</span><br><span class="line">├── obj_models -&gt; /data1/dataset/nocs/tanmx/spd/obj_models/</span><br><span class="line">│   ├── real_test</span><br><span class="line">│   ├── real_train</span><br><span class="line">│   ├── train</span><br><span class="line">│   └── val</span><br><span class="line">├── pose_dataset.py</span><br><span class="line">├── real -&gt; /data1/dataset/nocs/tanmx/spd/real/</span><br><span class="line">│   ├── test</span><br><span class="line">│   └── train</span><br><span class="line">└── shape_dataset.py</span><br><span class="line"></span><br><span class="line">19 directories, 2 files</span><br></pre></td></tr></table></figure><p>注意，这里也要和替换掉缺失的物体模型。</p><p><strong>按顺序</strong>执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> preprocess</span><br><span class="line">python shape_data.py</span><br><span class="line">python pose_data.py</span><br></pre></td></tr></table></figure><p>执行<code>python shape_data.py</code>后目录变为：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">├── camera -&gt; /data1/dataset/nocs/tanmx/spd/camera/</span><br><span class="line">│   ├── train</span><br><span class="line">│   └── val</span><br><span class="line">├── deformnet_eval -&gt; /data1/dataset/nocs/tanmx/spd/deformnet_eval/</span><br><span class="line">│   ├── camera</span><br><span class="line">│   ├── mrcnn_results</span><br><span class="line">│   ├── nocs_results</span><br><span class="line">│   └── real</span><br><span class="line">├── gts -&gt; /data1/dataset/nocs/tanmx/spd/gts/</span><br><span class="line">│   ├── real_test</span><br><span class="line">│   └── val</span><br><span class="line">├── obj_models -&gt; /data1/dataset/nocs/tanmx/spd/obj_models/</span><br><span class="line">│   ├── camera_train.pkl</span><br><span class="line">│   ├── camera_val.pkl</span><br><span class="line">│   ├── mug_meta.pkl</span><br><span class="line">│   ├── real_test</span><br><span class="line">│   ├── real_test.pkl</span><br><span class="line">│   ├── real_train</span><br><span class="line">│   ├── real_train.pkl</span><br><span class="line">│   ├── ShapeNetCore_2048.h5</span><br><span class="line">│   ├── ShapeNetCore_4096.h5</span><br><span class="line">│   ├── train</span><br><span class="line">│   └── val</span><br><span class="line">├── pose_dataset.py</span><br><span class="line">├── real -&gt; /data1/dataset/nocs/tanmx/spd/real/</span><br><span class="line">│   ├── test</span><br><span class="line">│   └── train</span><br><span class="line">└── shape_dataset.py</span><br><span class="line"></span><br><span class="line">19 directories, 9 files</span><br></pre></td></tr></table></figure><p>执行<code>python pose_data.py</code>后目录变为：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">├── camera -&gt; /data1/dataset/nocs/tanmx/spd/camera</span><br><span class="line">│   ├── train</span><br><span class="line">│   ├── train_list_all.txt</span><br><span class="line">│   ├── train_list.txt</span><br><span class="line">│   ├── val</span><br><span class="line">│   ├── val_list_all.txt</span><br><span class="line">│   └── val_list.txt</span><br><span class="line">├── deformnet_eval -&gt; /data1/dataset/nocs/tanmx/spd/deformnet_eval</span><br><span class="line">│   ├── camera</span><br><span class="line">│   ├── mrcnn_results</span><br><span class="line">│   ├── nocs_results</span><br><span class="line">│   └── real</span><br><span class="line">├── gts -&gt; /data1/dataset/nocs/tanmx/spd/gts/</span><br><span class="line">│   ├── real_test</span><br><span class="line">│   └── val</span><br><span class="line">├── obj_models -&gt; /data1/dataset/nocs/tanmx/spd/obj_models/</span><br><span class="line">│   ├── camera_train.pkl</span><br><span class="line">│   ├── camera_val.pkl</span><br><span class="line">│   ├── mug_meta.pkl</span><br><span class="line">│   ├── real_test</span><br><span class="line">│   ├── real_test.pkl</span><br><span class="line">│   ├── real_train</span><br><span class="line">│   ├── real_train.pkl</span><br><span class="line">│   ├── ShapeNetCore_2048.h5</span><br><span class="line">│   ├── ShapeNetCore_4096.h5</span><br><span class="line">│   ├── train</span><br><span class="line">│   └── val</span><br><span class="line">├── pose_dataset.py</span><br><span class="line">├── real -&gt; /data1/dataset/nocs/tanmx/spd/real/</span><br><span class="line">│   ├── test</span><br><span class="line">│   ├── test_list_all.txt</span><br><span class="line">│   ├── test_list.txt</span><br><span class="line">│   ├── train</span><br><span class="line">│   ├── train_list_all.txt</span><br><span class="line">│   └── train_list.txt</span><br><span class="line">└── shape_dataset.py</span><br><span class="line"></span><br><span class="line">19 directories, 17 files</span><br></pre></td></tr></table></figure><p>执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">ls</span> -l data/camera/</span><br><span class="line">total 9996</span><br><span class="line">drwxr-xr-x 27502 tanmx tanmx  577536 Jun 14  2019 train</span><br><span class="line">-rw-rw-r--     1 tanmx tanmx 4675000 Apr 29 14:00 train_list_all.txt</span><br><span class="line">-rw-rw-r--     1 tanmx tanmx 4235159 Apr 29 18:42 train_list.txt</span><br><span class="line">drwxr-xr-x  2502 tanmx tanmx   69632 Jun 14  2019 val</span><br><span class="line">-rw-rw-r--     1 tanmx tanmx  375000 Apr 29 14:00 val_list_all.txt</span><br><span class="line">-rw-rw-r--     1 tanmx tanmx  288300 Apr 29 19:15 val_list.txt</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">ls</span> -l data/real/</span><br><span class="line">total 280</span><br><span class="line">drwxrwxr-x 8 tanmx tanmx  4096 Nov 13  2018 <span class="built_in">test</span></span><br><span class="line">-rw-rw-r-- 1 tanmx tanmx 49572 Apr 29 14:00 test_list_all.txt</span><br><span class="line">-rw-rw-r-- 1 tanmx tanmx 49572 Apr 29 19:21 test_list.txt</span><br><span class="line">drwxrwxr-x 9 tanmx tanmx  4096 Jun 14  2019 train</span><br><span class="line">-rw-rw-r-- 1 tanmx tanmx 82042 Apr 29 14:00 train_list_all.txt</span><br><span class="line">-rw-rw-r-- 1 tanmx tanmx 82042 Apr 29 18:53 train_list.txt</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">ls</span> -l data/obj_models/</span><br><span class="line">total 116060</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx 26695895 Apr 29 11:32 camera_train.pkl</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx  4634293 Apr 29 11:36 camera_val.pkl</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx    23197 Apr 29 11:37 mug_meta.pkl</span><br><span class="line">drwxrwxr-x  2 tanmx tanmx     4096 Sep 27  2019 real_test</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx   443600 Apr 29 11:37 real_test.pkl</span><br><span class="line">drwxrwxr-x  2 tanmx tanmx     4096 Sep 27  2019 real_train</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx   443584 Apr 29 11:37 real_train.pkl</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx 28950893 Apr 29 12:21 ShapeNetCore_2048.h5</span><br><span class="line">-rw-rw-r--  1 tanmx tanmx 57618690 Apr 29 11:39 ShapeNetCore_4096.h5</span><br><span class="line">drwxr-xr-x 12 tanmx tanmx     4096 Sep 27  2019 train</span><br><span class="line">drwxr-xr-x 12 tanmx tanmx     4096 Sep 27  2019 val</span><br></pre></td></tr></table></figure><h2 id="运行代码">运行代码</h2><p>需要<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0ppZWhvbmdMaW4vU2VsZi1EUERO">JiehongLin/Self-DPDN<i class="fa fa-external-link-alt"></i></span>处理后的数据集及其list和<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21lbnRpYW4vb2JqZWN0LWRlZm9ybW5ldA==">mentian/object-deformnet<i class="fa fa-external-link-alt"></i></span>处理后的物体模型pkl，组织如下：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">├── camera -&gt; /data1/dataset/nocs/tanmx/agpose/camera</span><br><span class="line">│   ├── train</span><br><span class="line">│   ├── train_list_all.txt</span><br><span class="line">│   ├── train_list.txt</span><br><span class="line">│   ├── val</span><br><span class="line">│   └── val_list_all.txt</span><br><span class="line">├── camera_full_depths -&gt; /data1/dataset/nocs/tanmx/agpose/camera_full_depths/</span><br><span class="line">│   ├── train</span><br><span class="line">│   └── val</span><br><span class="line">├── gts -&gt; /data1/dataset/nocs/tanmx/agpose/gts/</span><br><span class="line">│   ├── real_test</span><br><span class="line">│   └── val</span><br><span class="line">├── obj_models -&gt; /data1/dataset/nocs/tanmx/agpose/obj_models/</span><br><span class="line">│   ├── camera_train.pkl</span><br><span class="line">│   ├── real_test</span><br><span class="line">│   ├── real_train</span><br><span class="line">│   ├── real_train.pkl</span><br><span class="line">│   ├── train</span><br><span class="line">│   └── val</span><br><span class="line">├── real -&gt; /data1/dataset/nocs/tanmx/agpose/real/</span><br><span class="line">│   ├── test</span><br><span class="line">│   ├── test_list_all.txt</span><br><span class="line">│   ├── train</span><br><span class="line">│   ├── train_list_all.txt</span><br><span class="line">│   └── train_list.txt</span><br><span class="line">└── segmentation_results -&gt; /data1/dataset/nocs/tanmx/agpose/segmentation_results</span><br><span class="line">    ├── CAMERA25</span><br><span class="line">    └── REAL275</span><br><span class="line"></span><br><span class="line">20 directories, 8 files</span><br></pre></td></tr></table></figure><p>之后按照仓库README中的命令运行代码即可。</p>]]></content>
    
    
    <summary type="html">复现AG-Pose的代码，并进行测试。</summary>
    
    
    
    <category term="代码复现" scheme="https://blog.032802.xyz/categories/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/"/>
    
    
    <category term="2024CVPR" scheme="https://blog.032802.xyz/tags/2024CVPR/"/>
    
    <category term="Object Pose Estimation" scheme="https://blog.032802.xyz/tags/Object-Pose-Estimation/"/>
    
    <category term="代码复现" scheme="https://blog.032802.xyz/tags/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>【VPS测评】Suda速达云 - [家宽]香港HKT区NAT 一号机 (04:00定时刷新IP) - 香港-KVM-小杯</title>
    <link href="https://blog.032802.xyz/vps-review/suda-hkt-ty-line-1.html"/>
    <id>https://blog.032802.xyz/vps-review/suda-hkt-ty-line-1.html</id>
    <published>2025-04-21T13:34:27.000Z</published>
    <updated>2025-04-21T13:34:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>配置信息：</p><ul><li>CPU：1 核心处理器</li><li>内存：512 MB RAM</li><li>存储：5 GB</li><li>端口映射：计算型端口映射（映射规则详见群组和帮助中心）</li><li>带宽：500M/500Mbps ，1T（双向计费，合理使用）</li></ul><p>购买链接：<span class="exturl" data-url="aHR0cHM6Ly9jbG91ZC5zdWRhdGVjaC5zdG9yZS9jYXJ0LnBocD9hPWNvbmZwcm9kdWN0Jmk9MA==">Suda速达云- 香港HKT区NAT 一号机 (04:00定时刷新IP)<i class="fa fa-external-link-alt"></i></span></p><h2 id="测试">2025-04-21测试</h2><h3 id="itdog">ITDOG</h3><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuaXRkb2cuY24vcGluZy8=">https://www.itdog.cn/ping/<i class="fa fa-external-link-alt"></i></span>：</p><p>结果图：<span class="exturl" data-url="aHR0cHM6Ly9pbWcuMDMyODAyLnh5ei92cHMtcmV2aWV3L3N1ZGEtaGt0LXR5LWxpbmUtMS8yMDI1LjA0LjIxLTEuMzYud2VicA==">ITDOG：1.36.*.*<i class="fa fa-external-link-alt"></i></span></p><h3 id="ipquality">IPQuality</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3h5a3QvSVBRdWFsaXR5">xykt/IPQuality<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash &lt;(curl -Ls IP.Check.Place)</span><br></pre></td></tr></table></figure><figure><img src="https://report.check.place/ip/38RPP2AH2.svg"alt="IP质量体检报告：1.36.*.*" /><figcaption aria-hidden="true">IP质量体检报告：1.36.*.*</figcaption></figure><h3 id="融合怪">融合怪</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NwaXJpdExITFMvZWNz">spiritLHLS/ecs<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -L https://gitlab.com/spiritysdx/za/-/raw/main/ecs.sh -o ecs.sh &amp;&amp; <span class="built_in">chmod</span> +x ecs.sh &amp;&amp; bash ecs.sh -m 1</span><br></pre></td></tr></table></figure><p><span class="exturl" data-url="aHR0cHM6Ly9wYXN0ZS5zcGlyaXRsaGwubmV0LyMvc2hvdy9uYWJoRC50eHQ=">https://paste.spiritlhl.net/#/show/nabhD.txt<i class="fa fa-external-link-alt"></i></span></p>]]></content>
    
    
    <summary type="html">本次测评的服务器是Suda速达云的香港HKT区NAT 一号机 (04:00定时刷新IP)，月付¥11.00CNY。</summary>
    
    
    
    <category term="VPS测评" scheme="https://blog.032802.xyz/categories/VPS%E6%B5%8B%E8%AF%84/"/>
    
    
    <category term="VPS" scheme="https://blog.032802.xyz/tags/VPS/"/>
    
    <category term="VPS测评" scheme="https://blog.032802.xyz/tags/VPS%E6%B5%8B%E8%AF%84/"/>
    
    <category term="香港VPS" scheme="https://blog.032802.xyz/tags/%E9%A6%99%E6%B8%AFVPS/"/>
    
    <category term="Suda速达云" scheme="https://blog.032802.xyz/tags/Suda%E9%80%9F%E8%BE%BE%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】Co-op: Correspondence-based Novel Object Pose Estimation</title>
    <link href="https://blog.032802.xyz/paper-reading/2025/co-op-correspondence-based-novel-object-pose-estimation_2025_Moon.html"/>
    <id>https://blog.032802.xyz/paper-reading/2025/co-op-correspondence-based-novel-object-pose-estimation_2025_Moon.html</id>
    <published>2025-04-05T09:50:51.000Z</published>
    <updated>2025-04-05T09:50:51.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="co-op-correspondence-based-novel-object-pose-estimation">Co-op:Correspondence-based Novel Object Pose Estimation</h1><table><colgroup><col style="width: 6%" /><col style="width: 7%" /><col style="width: 11%" /><col style="width: 11%" /><col style="width: 51%" /><col style="width: 10%" /></colgroup><thead><tr><th style="text-align: center;">方法</th><th style="text-align: center;">类型</th><th style="text-align: center;">训练输入</th><th style="text-align: center;">推理输入</th><th style="text-align: center;">输出</th><th style="text-align: center;">pipeline</th></tr></thead><tbody><tr><td style="text-align: center;">Co-op</td><td style="text-align: center;">任意级</td><td style="text-align: center;">RGB + CAD</td><td style="text-align: center;">RGB + CAD</td><td style="text-align: center;">绝对<spanclass="math inline">\(\mathbf{R}, \mathbf{t}\)</span></td><td style="text-align: center;"></td></tr></tbody></table><ul><li>2025.04.06：使用patch-patch的匹配来确定与查询最接近的模板并得到2D-3D的匹配，然后使用EPnP计算粗略位姿。在优化时，将查询和模板之间的flow定义为一个单变量拉普拉斯分布，预测这个分布以优化位姿。</li></ul><h2 id="abstract">Abstract</h2><h2 id="introduction">1. Introduction</h2><figure><imgsrc="https://img.032802.xyz/paper-reading/2025/co-op-correspondence-based-novel-object-pose-estimation_2025_Moon/main_figure.webp"alt="Figure 1. Examples of 6D pose estimation of novel objects. Our method estimates semi-dense or dense correspondences between the input image and rendered images and uses them to estimate the pose." /><figcaption aria-hidden="true">Figure 1. Examples of 6D pose estimationof novel objects. Our method estimates semi-dense or densecorrespondences between the input image and rendered images and usesthem to estimate the pose.</figcaption></figure><p>Our contributions can be summarized as follows:</p><ul><li>We present Co-op, a novel framework for unseen object poseestimation in RGB-only cases. Co-op does not require additional trainingor fine-tuning for new objects and outperforms existing methods by alarge margin on the seven core datasets of the BOP Challenge.</li><li>We propose a method for fast and accurate coarse pose estimationusing a hybrid representation that combines patch-level classificationand offset regression.</li><li>Additionally, we propose a precise object pose refinement method. Itestimates dense correspondences defined by probabilistic flow and learnsconfidence end-to-end through a differentiable PnP layer.</li></ul><h2 id="related-work">2. Related Work</h2><h2 id="method">3. Method</h2><figure><imgsrc="https://img.032802.xyz/paper-reading/2025/co-op-correspondence-based-novel-object-pose-estimation_2025_Moon/overview_figure.webp"alt="Figure 2. Overview. We estimate object pose through two main stages. In the Coarse Pose Estimation stage (Sec 3.1), we estimate semidense correspondences between the query image and templates and compute the initial pose using PnP. In the Pose Refinement stage (Sec 3.2), we refine the initial pose by estimating dense flow between the query and rendered images. Both stages utilize transformer encoders and decoders with identical structures, with the Pose Refinement stage additionally incorporating a DPT module after the decoder for dense prediction." /><figcaption aria-hidden="true">Figure 2. Overview. We estimate objectpose through two main stages. In the Coarse Pose Estimation stage (Sec3.1), we estimate semidense correspondences between the query image andtemplates and compute the initial pose using PnP. In the Pose Refinementstage (Sec 3.2), we refine the initial pose by estimating dense flowbetween the query and rendered images. Both stages utilize transformerencoders and decoders with identical structures, with the PoseRefinement stage additionally incorporating a DPT module after thedecoder for dense prediction.</figcaption></figure><h3 id="coarse-pose-estimation">3.1. Coarse Pose Estimation</h3><p><strong>TemplateGeneration</strong>：粗略位姿估计阶段使用和GigaPose类似的方法，生成仅包含面外旋转的模板来减少位姿估计需要的模板数。</p><p>使用“Templates for 3D Object Pose Estimation Revisited:Generalization to New Objects and Robustness to Occlusions”和“CNOS: AStrong Baseline for CAD-based Novel ObjectSegmentation”中的方法生成模板。</p><p><strong>HybridRepresentation</strong>：Query图像和Template图像分别使用<spanclass="math inline">\(\mathcal{I}_Q, \mathcal{I}_T \in \mathbb{R}^{H\times W \times3}\)</span>表示。为了提高泛化性，使用了一种结合patch分类和偏移回归的混合表示。</p><figure><imgsrc="https://img.032802.xyz/paper-reading/2025/co-op-correspondence-based-novel-object-pose-estimation_2025_Moon/hybrid_figure.webp"alt="Figure 3. Visualization of Our Hybrid Representation. Left: Patch-level classification results; matching patches are highlighted with the same color. Right: Offset regression within template patches to refine correspondences; red arrows represent the estimated offsets." /><figcaption aria-hidden="true">Figure 3. Visualization of Our HybridRepresentation. Left: Patch-level classification results; matchingpatches are highlighted with the same color. Right: Offset regressionwithin template patches to refine correspondences; red arrows representthe estimated offsets.</figcaption></figure><p>图3左侧是patch分类结果，右侧是偏移回归结果。</p><p>如图2所示，使用TransformerEncoder提取Query图像和Template图像的特征，然后使用TransformerDecoder生成patch分类结果和偏移回归结果。</p><p>Encoder将<span class="math inline">\(\mathcal{I}_Q\)</span>和<spanclass="math inline">\(\mathcal{I}_T\)</span>作为输入，提取特征图<spanclass="math inline">\(\mathcal{F}_Q, \mathcal{F}_T \in\mathbb{R}^{\frac{H}{16} \times \frac{W}{16} \times1024}\)</span>。然后Decoder和后续头处理<spanclass="math inline">\(\mathcal{F}_Q\)</span>和<spanclass="math inline">\(\mathcal{F}_T\)</span>，计算patch级分类<spanclass="math inline">\(\mathcal{C} \in \mathbb{R}^{\frac{H}{16} \times\frac{W}{16} \times K}\)</span>和xy-offsets <spanclass="math inline">\(\mathcal{U} \in \mathbb{R}^{\frac{H}{16} \times\frac{W}{16} \times 2}\)</span>。</p><p>其中，<span class="math inline">\(K = \frac{H}{16} \times\frac{W}{16} + 1\)</span>表示patch级分类的类数，<spanclass="math inline">\(\frac{H}{16} \times\frac{W}{16}\)</span>为patch的数量，最后的<spanclass="math inline">\(+1\)</span>表示没有匹配的情况，例如遮挡。对于特征图中的每个位置<spanclass="math inline">\((i, j)\)</span>，<spanclass="math inline">\(\mathcal{C}_{i, j} \in\mathbb{R}^K\)</span>包含分类分数，指示Query <spanclass="math inline">\((i, j)\)</span>位置上的patch和Template中<spanclass="math inline">\(\frac{H}{16} \times\frac{W}{16}\)</span>个patch的匹配情况。</p><p>在此之后，我们将索引定义为<span class="math inline">\(c_{i, j} = \arg\max_{k \in 1, 2, \cdots, K} \mathcal{C}_{i,j}^k\)</span>，将偏移的范围定义在<spanclass="math inline">\(\mathcal{U}_{i, j} \in [-0.5,0.5]\)</span>。在<span class="math inline">\(c_{i, j} \neK\)</span>的位置<span class="math inline">\((i,j)\)</span>上，可以使用下面的公式来计算Query中下标为<spanclass="math inline">\((i, j)\)</span>的patch的中心点在Templatepatch中的对应点<span class="math inline">\(\mathcal{M}_{i,j}^T\)</span>：</p><p><span class="math display">\[\begin{equation}\label{eq1}    \mathcal{M}_{i, j}^T =    \left(\begin{bmatrix}        c_{i, j} \mod 16 + 0.5 \\        \lfloor \frac{c_{i, j}}{16} \rfloor + 0.5    \end{bmatrix} + \mathcal{U}_{i, j}\right) \times 16.\end{equation}\]</span></p><p>添加<span class="math inline">\(\mathcal{U}_{i,j}\)</span>可优化特征图网格中的位置。乘16会将该位置映射回原始图像<spanclass="math inline">\(\mathcal{I}_Q\)</span>和<spanclass="math inline">\(\mathcal{I}_T\)</span>的坐标系，因为特征图中的每个位置都对应原始图像中一个大小为<spanclass="math inline">\(16 \times16\)</span>的patch。对应的Query图像位置<spanclass="math inline">\(\mathcal{M}_{i, j}^Q\)</span>为<spanclass="math inline">\(((i + 0.5) \times 16, (j + 0.5) \times16)\)</span>，即每个Query patch的中心位置。<spanclass="math inline">\(\mathcal{I}_Q\)</span>和<spanclass="math inline">\(\mathcal{I}_T\)</span>的对应关系定义为<spanclass="math inline">\(\mathcal{M}_{i, j} = (\mathcal{M}_{i, j}^Q,\mathcal{M}_{i, j}^T)\)</span>。</p><p><strong>Pose Fitting</strong>：我们计算<spanclass="math inline">\(\mathcal{I}_Q\)</span>和所有<spanclass="math inline">\(\mathcal{I}_T\)</span>的对应关系<spanclass="math inline">\(\mathcal{M}\)</span>，并选择与Query最相似的模板<spanclass="math inline">\(k\)</span>。每个模板的相似性得分定义如下：</p><p><span class="math display">\[\begin{equation}\label{eq2}    S_t = \sum_{i, j}    \begin{cases}        \max(\mathcal{C}_{i, j}), &amp; \text{if } c_{i, j} \ne K \\        0, &amp; \text{otherwise}    \end{cases}.\end{equation}\]</span></p><p>如果<span class="math inline">\(c_{i, j} =K\)</span>（表示遮挡或不匹配的patch），我们从总和中排除<spanclass="math inline">\(\max(\mathcal{C}_{i,j})\)</span>。通过计算每个模板的<spanclass="math inline">\(S_t\)</span>，我们选择相似度得分最高的模板作为<spanclass="math inline">\(\mathcal{I}_Q\)</span>的最佳匹配，然后基于模板中的深度信息构建2D-3D的对应关系，然后使用RANSAC和EPnP来计算初始位姿。</p><h3 id="pose-refinement">3.2. Pose Refinement</h3><p>如图2所示，细化模型就是在粗略位姿估计模型的基础上增加一个密集预测Transformer（DPT）。DPT支持像素级预测，从而实现精确的位姿优化。此外，使用渲染-比较方法迭代优化位姿，该阶段克服了在粗略位姿估计阶段使用预渲染模板所带来的限制，比如自遮挡。</p><p><strong>Probabilistic Flow Regression</strong>：与“Perspective FlowAggregation for Data-Limited 6D Object Pose Estimation”和“GenFlow:Generalizable Recurrent Flow for 6D Pose Refinement of NovelObjects”类似，细化模型估计Query <spanclass="math inline">\(\mathcal{I}_T\)</span>和Render <spanclass="math inline">\(\mathcal{I}_R\)</span>之间的流动以优化位姿。要从流中准确恢复位姿，必须防止不准确的流显著影响位姿计算。所以，“PerspectiveFlow Aggregation for Data-Limited 6D Object PoseEstimation”使用RANSAC从位姿估计中概率的排除不准确的流。但是，由于RANSAC对异常值分布敏感，因此当异常值普遍存在或分布不均匀时，RANSAC的性能会下降。</p><p>与之前基于流的细化方法不同，我们的目标是学习流的条件概率。根据“ASpanFormer:Detector-Free Image Matching with Adaptive Span Transformer”和“PDC-Net+:Enhanced Probabilistic Dense CorrespondenceNetwork”，我们将这个条件概率定义为<span class="math inline">\(p(Y |\mathcal{I}_Q, \mathcal{I}_R; \theta)\)</span>，其中<spanclass="math inline">\(Y\)</span>是<spanclass="math inline">\(\mathcal{I}_Q\)</span>和<spanclass="math inline">\(\mathcal{I}_R\)</span>之间的流，<spanclass="math inline">\(\theta\)</span>是模型参数。很多方法通过学习预测<spanclass="math inline">\(Y\)</span>的方差来实现这一点，并且使用高斯分布或拉普拉斯分布对预测密度进行建模。我们将其建模为单变量拉普拉斯分布以简化问题。具体来说，<spanclass="math inline">\(p(Y | \mathcal{I}_Q, \mathcal{I}_R;\theta)\)</span>建模为均值为<span class="math inline">\(\mu \in\mathbb{R}^{H \times W \times 2}\)</span>，尺度为<spanclass="math inline">\(b \in \mathbb{R}^{H \times W \times1}\)</span>的拉普拉斯分布，两者均由网络预测。将流估计表示为概率回归，使我们的模型通过调整尺度参数<spanclass="math inline">\(b\)</span>专注于高度可靠的对应关系。</p><p><strong>FlowConfidence</strong>：要使用模型估计的流来计算位姿，我们需要置信度<spanclass="math inline">\(\mathcal{W} \in \mathbb{R}^{H \times W \times1}\)</span>。<spanclass="math inline">\(\mathcal{W}\)</span>决定了在计算可微分PnP层中的位姿时，每个流误差的权重。<spanclass="math inline">\(\mathcal{W}\)</span>是根据确定性、敏感性和流概率计算得出的，这些概率是通过不同的损失函数学习的。确定性估计从<spanclass="math inline">\(\mathcal{I}_R\)</span>到<spanclass="math inline">\(\mathcal{I}_Q\)</span>的流是否被遮挡。灵敏度是从位姿损失中学习的，并高亮具有丰富纹理或物体边缘的区域。与“PDC-Net+:Enhanced Probabilistic Dense CorrespondenceNetwork”类似，我们定义了流概率<spanclass="math inline">\(P_R\)</span>，它表示真实流动<spanclass="math inline">\(y\)</span>位于平均光流<spanclass="math inline">\(\mu\)</span>、半径为<spanclass="math inline">\(R\)</span>范围内的概率。其计算方式如下：</p><p><span class="math display">\[\begin{equation}\label{eq3}    P_R = P(\Vert y - \mu\Vert_1 &lt; R) = 1 -\exp\left(-\frac{R}{b}\right).\end{equation}\]</span></p><p><spanclass="math inline">\(P_R\)</span>是可靠性的可解释度量，表示阈值为<spanclass="math inline">\(R\)</span>的流的准确性。流置信度<spanclass="math inline">\(\mathcal{W}\)</span>计算为确定性、敏感性和<spanclass="math inline">\(P_R\)</span>的元素乘积。这意味着在没有遮挡（高质量）、有判别信息可用于解决位姿（高灵敏度）以及准确性（高<spanclass="math inline">\(P_R\)</span>）时，<spanclass="math inline">\(\mathcal{W}\)</span>将具有更高的值。</p><p><strong>Pose Update</strong>：为了使用流<spanclass="math inline">\(Y\)</span>和流置信度<spanclass="math inline">\(\mathcal{W}\)</span>计算细化后的6D位姿<spanclass="math inline">\(\mathbf{P}_\text{refined}\)</span>，我们使用基于Levenberg-Marquardt(LM)的PnP求解器。给定输入位姿<spanclass="math inline">\(\mathbf{P}_\text{input} = [\mathbf{R}_\text{input}| \mathbf{t}_\text{input}]\)</span>，相机内参矩阵<spanclass="math inline">\(\mathbf{K}\)</span>和对应于<spanclass="math inline">\(\mathcal{I}_R\)</span>的深度图<spanclass="math inline">\(\mathcal{D}_R\)</span>，我们计算<spanclass="math inline">\(\mathcal{I}_R\)</span>对应的3D空间坐标<spanclass="math inline">\(\mathbf{x}_{u,v}^\text{3D}\)</span>，如下所示：</p><p><span class="math display">\[\begin{equation}\label{eq4}    \mathbf{x}_{u, v}^\text{3D} =\mathbf{R}_\text{input}^{-1}(\mathbf{K}^{-1}\mathcal{D}_R(u,v)\mathbf{x}_{u, v}^\text{2D} - \mathbf{t}_\text{input}),\end{equation}\]</span></p><p>其中<span class="math inline">\((u, v)\)</span>是<spanclass="math inline">\(\mathcal{I}_R\)</span>中的像素坐标，<spanclass="math inline">\(\mathcal{D}_R(u,v)\)</span>是每个像素的深度值，且<spanclass="math inline">\(\mathbf{x}_{u, v}^\text{2D} = (u, v,1)^T\)</span>。我们通过最小化加权重投影误差的平方和来优化6D位姿，如下所示：</p><p><span class="math display">\[\begin{equation}\label{eq5}    \underset{\mathbf{R},\mathbf{t}}{\arg\min}\frac{1}{2}\sum_u\sum_v\Vert\mathcal{W}(u, v)\times (\pi(\mathbf{R}\mathbf{x}_{u, v}^\text{3D} + \mathbf{t}) - ((u,v)^T + Y(u, v)))\Vert^2.\end{equation}\]</span></p><p>其中，<spanclass="math inline">\(\pi\)</span>是重投影函数，它使用相机内参<spanclass="math inline">\(\mathbf{K}\)</span>将相机坐标中的3D点映射到2D图像点。</p><p>根据之前的工作“GenFlow: Generalizable Recurrent Flow for 6D PoseRefinement of NovelObjects”，我们使用LM算法分三次迭代更新位姿，并使用Gauss-Newton算法进一步将其细化为最终位姿。</p><h3 id="pose-selection">3.3. Pose Selection</h3><p>在粗略位姿估计阶段，最佳评分模板可能无法提供用于细化的最佳初始位姿。例如，选择相对于GT旋转180度的模板（见图4）会使优化变得具有挑战性。</p><figure><imgsrc="https://img.032802.xyz/paper-reading/2025/co-op-correspondence-based-novel-object-pose-estimation_2025_Moon/selection_figure.webp"alt="Figure 4. Pose Selection. To achieve more precise pose estimation using a multiple hypothesis strategy, we introduce a pose selection stage (Sec 3.3)." /><figcaption aria-hidden="true">Figure 4. Pose Selection. To achieve moreprecise pose estimation using a multiple hypothesis strategy, weintroduce a pose selection stage (Sec 3.3).</figcaption></figure><p>为了解决这个问题，“MegaPose”、“GenFlow”、“GigaPose”、“FoundPose”、“FoundationPose”等方法采用多重假设策略：生成<spanclass="math inline">\(N\)</span>个粗略位姿估计，细化每个估计，并通过将渲染的结果和查询图像进行比较来选择最佳匹配。我们的位姿选择模型以查询图像和模板作为输入，在粗略位姿估计模型的基础上增加一个评分头来对每个位姿假设进行评分。即使这会增加推理时间，但是考虑多个优化位姿能够避免难以优化模板导致最终位姿较差的情况。</p><h3 id="training">3.4. Training</h3><p><strong>Datasets</strong>：为了训练我们的三个模型（粗略位姿估计器、位姿优化器和位姿选择器），我们需要具有GT6D位姿标注的RGBD图像。我们使用“MegaPose: 6D Pose Estimation of NovelObjects via Render &amp;Compare”提供的大规模数据集。该数据集由使用“BlenderProc2: A ProceduralPipeline for PhotorealisticRendering”生成的合成数据组成，其中包含来自“ShapeNet: An Information-Rich3D Model Repository”和“Google Scanned Objects: A High-Quality Dataset of3D Scanned Household Items”的各种对象，包括全面的GT6D位姿标注和对象掩码。</p><figure><imgsrc="https://img.032802.xyz/paper-reading/2025/co-op-correspondence-based-novel-object-pose-estimation_2025_Moon/coarse_figure_3rows.webp"alt="Figure 5. In-plane Rotation Invariant Matching Example. From left to right: Query image, semi-dense correspondences between the query image and the best scoring template, and the coarse pose recovered using the PnP algorithm." /><figcaption aria-hidden="true">Figure 5. In-plane Rotation InvariantMatching Example. From left to right: Query image, semi-densecorrespondences between the query image and the best scoring template,and the coarse pose recovered using the PnP algorithm.</figcaption></figure><p><strong>CoarseModel</strong>：我们的粗略位姿估计模型经过训练，用于估计查询图像<spanclass="math inline">\(\mathcal{I}_Q\)</span>和模板图像<spanclass="math inline">\(\mathcal{I}_T\)</span>之间的对应关系。和“GigaPose:Fast and Robust Novel Object Pose Estimation via OneCorrespondence”相似，我们选取的模板与围绕目标物体裁剪出的训练图像相比，具有相似的面外旋转角度，但面内旋转角度不同。如图5所示，我们的模型旨在估计查询图像和模板之间对于面内旋转具有不变性的对应关系。这种不变性意味着这些模板只需考虑面外旋转情况，从而大幅减少了所需模板的数量。由于我们拥有目标物体的3D模型，我们可以通过将3D模型投影到查询图像和模板图像的图像平面上，来生成这些对应关系。我们的模型经过训练就是为了估计这些生成的2D-2D的对应关系。</p><p><strong>RefinerModel</strong>：我们的优化模型的训练方式与之前采用“渲染-比较”方法的研究工作如“CosyPose”、“MegaPose”、“GenFlow”等类似。我们通过向真实位姿<spanclass="math inline">\(\mathbf{P}_\text{gt}\)</span>添加均值为0的高斯噪声来生成含噪声的输入位姿<spanclass="math inline">\(\mathbf{P}_\text{input}\)</span>，其中沿<spanclass="math inline">\(x\)</span>、<spanclass="math inline">\(y\)</span>、<spanclass="math inline">\(z\)</span>轴的平移噪声标准差分别为<spanclass="math inline">\((0.01, 0.01,0.05)\)</span>，并且在欧拉角中每个轴的旋转噪声标准差为15度。该模型经过训练，能够从<spanclass="math inline">\(\mathbf{P}_\text{input}\)</span>中预测出<spanclass="math inline">\(\mathbf{P}_\text{gt}\)</span>。</p><p><strong>SelectionModel</strong>：我们的选择模型经过训练，能够基于含噪输入姿态<spanclass="math inline">\(\mathbf{P}_\text{input}\)</span>来评估查询图像<spanclass="math inline">\(\mathcal{I}_Q\)</span>与参考图像<spanclass="math inline">\(\mathcal{I}_R\)</span>之间的相似度，以便从多个姿态假设中选出最准确的姿态。我们使用二元交叉熵损失函数来训练该模型，对于每个真实姿态<spanclass="math inline">\(\mathbf{P}_\text{gt}\)</span>，使用六个姿态：一个正样本，五个负样本。正样本与真实姿态<spanclass="math inline">\(\mathbf{P}_\text{gt}\)</span>之间的平移差异在<spanclass="math inline">\((0.01, 0.01,0.05)\)</span>范围内，旋转差异在5度以内。通过为正样本设置较小的旋转阈值，我们增强了模型区分与真实姿态相近的姿态的能力，从而提高了其判别能力。</p><h3 id="implementation-details">3.5. Implementation Details</h3><p>Co-op的编码器和解码器架构基于“CroCo v2: Improved Cross-viewCompletion Pre-training for Stereo Matching and OpticalFlow”，这是一个在大规模数据集上针对三维视觉任务训练的视觉基础模型。这使我们能够充分利用CroCov2预训练的优势。粗略位姿估计模型处理分辨率为<spanclass="math inline">\(224 \times224\)</span>的输入图像，而优化模型和选择模型则处理分辨率为<spanclass="math inline">\(256 \times256\)</span>的图像。有关模型配置、学习率和训练计划的详细信息，请参考补充材料。</p><p><strong>CoarseModel</strong>：在我们的粗略估计模型中，我们将半密集对应关系定义为一种混合表示，它结合了补丁级别的分类和偏移回归。因此，粗略估计模型使用两个损失函数进行训练：分类损失<spanclass="math inline">\(\mathcal{L}_{cls}\)</span>和回归损失<spanclass="math inline">\(\mathcal{L}_{reg}\)</span>。</p><p>当把从查询图像中patch中心<span class="math inline">\((i,j)\)</span>到模板的真实匹配定义为<spanclass="math inline">\(\mathcal{M}_{gt}^T=(\bar{u},\bar{v})\)</span>时，若存在匹配情况，用于<spanclass="math inline">\(\mathcal{L}_{cls}\)</span>的真实索引被定义为<spanclass="math inline">\(\frac{\bar{v}}{16} \times \frac{W}{16} +\frac{\bar{u}}{16}\)</span>。此处，<spanclass="math inline">\(\mathcal{W}\)</span>是模板图像的宽度，除以<spanclass="math inline">\(16\)</span>是考虑到因patch大小而导致的图像尺寸缩小。当不存在匹配时，真实索引被定义为<spanclass="math inline">\(\frac{H}{16} \times \frac{W}{16} +1\)</span>，它代表着一个额外的“无匹配”类别。因此，<spanclass="math inline">\(\mathcal{L}_{cls}\)</span>被定义为真实索引与模型输出的patch级别概率向量之间的交叉熵损失。</p><p>用于<spanclass="math inline">\(\mathcal{L}_{reg}\)</span>的偏移真实值<spanclass="math inline">\(\mathcal{U}_{gt}\)</span>定义如下：</p><p><span class="math display">\[\begin{equation}\label{eq6}    \mathcal{U}_{gt} = \frac{\mathcal{M}_{gt}^T}{16} -\left\lfloor\frac{\mathcal{M}_{gt}^T}{16}\right\rfloor - 0.5,\end{equation}\]</span></p><p>其中<spanclass="math inline">\(\mathcal{M}_{gt}^T\)</span>是模板中的真实匹配。减去<spanclass="math inline">\(\left\lfloor\frac{\mathcal{M}_{gt}^T}{16}\right\rfloor\)</span>再减去<spanclass="math inline">\(0.5\)</span>可将偏移量以patch为中心，其范围在<spanclass="math inline">\(-0.5\)</span>到<spanclass="math inline">\(0.5\)</span>之间。这里，<spanclass="math inline">\(\mathcal{U}\)</span>表示模型输出的预测偏移量。然后，回归损失<spanclass="math inline">\(\mathcal{L}_{reg}\)</span>被定义为<spanclass="math inline">\(\mathcal{U}\)</span>和<spanclass="math inline">\(\mathcal{U}_{gt}\)</span>之间的L1损失：</p><p><span class="math display">\[\begin{equation}\label{eq7}    \mathcal{L}_\text{reg} = \Vert \mathcal{U} - \mathcal{U}_{gt}\Vert_1.\end{equation}\]</span></p><p>因此，用于训练粗略模型的总损失<spanclass="math inline">\(\mathcal{L}_\text{coarse}\)</span>定义为：</p><p><span class="math display">\[\begin{equation}\label{eq8}    \mathcal{L}_\text{coarse} = \mathcal{L}_\text{cls} +\alpha\mathcal{L}_\text{reg}.\end{equation}\]</span></p><p>在上面的等式中，<spanclass="math inline">\(\alpha\)</span>设置为2。</p><p><strong>RefinerModel</strong>：我们的优化模型的训练使用了光流损失、确定性损失和姿态损失，这与“GenFlow”的方法类似。由于我们用拉普拉斯分布对网络的光流输出进行参数化，所以我们通过最小化负对数似然来训练光流。因此，光流损失定义如下：</p><p><span class="math display">\[\begin{equation}\label{eq9}    \mathcal{L}_{flow} = \sum_u\sum_v\left[\frac{|\mu_{u, v} -\bar{\mu}_{u, v}|}{b_{u, v}} + 2\log b_{u, v}\right].\end{equation}\]</span></p><p>在这个公式中，<spanclass="math inline">\(\mu_{u,v}\)</span>是渲染掩码内像素位置<spanclass="math inline">\((u, v)\)</span>处的光流，<spanclass="math inline">\(\bar{\mu}_{u,v}\)</span>是真实光流。<spanclass="math inline">\(b_{u,v}\)</span>是像素位置<spanclass="math inline">\((u,v)\)</span>处的尺度（不确定性）。此外，确定性损失<spanclass="math inline">\(\mathcal{L}_{cert}\)</span>被定义为二元交叉熵损失，用于判断从渲染图像<spanclass="math inline">\(\mathcal{I}_R\)</span>到查询图像<spanclass="math inline">\(\mathcal{I}_Q\)</span>的光流是否落在<spanclass="math inline">\(\mathcal{I}_Q\)</span>的真实掩码内。姿态损失<spanclass="math inline">\(\mathcal{L}_{pose}\)</span>用于量化优化后的姿态与真实姿态之间的差异，按照先前在“MegaPose”和“GenFlow”中使用的方法，它被定义为3D模型上对应3D点之间的距离。关于<spanclass="math inline">\(\mathcal{L}_{pose}\)</span>的详细信息在补充材料中给出。</p><p>优化模型的总体损失定义如下：</p><p><span class="math display">\[\begin{equation}\label{eq10}    \mathcal{L}_{refiner} = \mathcal{L}_{flow} + \beta\mathcal{L}_{cert}+ \gamma\mathcal{L}_{pose}.\end{equation}\]</span></p><p>每个损失的权重<span class="math inline">\(\beta\)</span>和<spanclass="math inline">\(\gamma\)</span>分别设置为5和20。</p><h2 id="experiments">4. Experiments</h2><h3 id="experimental-setup">4.1. Experimental Setup</h3><h3 id="bop-benchmark-results">4.2. BOP Benchmark Results</h3><h4 id="coarse-estimation">4.2.1 Coarse Estimation</h4><h4 id="pose-refinement-1">4.2.2 Pose Refinement</h4><h3 id="ablation-study">4.3. Ablation Study</h3><h2 id="conclusion">5. Conclusion</h2><h2 id="supplementary-material">Supplementary Material</h2><h3 id="training-details">6. Training Details</h3><h3 id="additional-experiments">7. Additional Experiments</h3><h3 id="qualitative-results">8. Qualitative Results</h3><figure><imgsrc="https://img.032802.xyz/paper-reading/2025/co-op-correspondence-based-novel-object-pose-estimation_2025_Moon/supp_coarse.webp"alt="Figure 6. Qualitative Results of Coarse Estimation. The first two columns on the left display the model&#39;s query image and the template with the highest similarity score to the query image. The third and fourth columns compare the CNOS [49] segmentation mask with patches that the model did not classify as &#39;no-match&#39;. From the fifth to the last columns, the correspondences between the query image and the template, as well as the resulting pose estimation results, are shown." /><figcaption aria-hidden="true">Figure 6. Qualitative Results of CoarseEstimation. The first two columns on the left display the model's queryimage and the template with the highest similarity score to the queryimage. The third and fourth columns compare the CNOS [49] segmentationmask with patches that the model did not classify as 'no-match'. Fromthe fifth to the last columns, the correspondences between the queryimage and the template, as well as the resulting pose estimationresults, are shown.</figcaption></figure><figure><imgsrc="https://img.032802.xyz/paper-reading/2025/co-op-correspondence-based-novel-object-pose-estimation_2025_Moon/refine_supp.webp"alt="Figure 7. Qualitative Results of Pose Refinement. From left to right: query image, initial pose rendering, flow, confidence, flow probability, certainty, sensitivity, and the refined pose (legend: 0.0 1.0). The flow probability and certainty reduce confidence in ambiguous or occluded areas, while sensitivity increases confidence in textured regions and object edges to improve pose refinement." /><figcaption aria-hidden="true">Figure 7. Qualitative Results of PoseRefinement. From left to right: query image, initial pose rendering,flow, confidence, flow probability, certainty, sensitivity, and therefined pose (legend: 0.0 1.0). The flow probability and certaintyreduce confidence in ambiguous or occluded areas, while sensitivityincreases confidence in textured regions and object edges to improvepose refinement.</figcaption></figure><figure><imgsrc="https://img.032802.xyz/paper-reading/2025/co-op-correspondence-based-novel-object-pose-estimation_2025_Moon/turbo.webp"alt="legend: 0.0 - 1.0" /><figcaption aria-hidden="true">legend: 0.0 - 1.0</figcaption></figure><div class="pdf-container" data-target="https://arxiv.org/pdf/2503.17731" data-height="500px"></div>]]></content>
    
    
    <summary type="html">We propose Co-op, a novel method for accurately and robustly estimating the 6DoF pose of objects unseen during training from a single RGB image. Our method requires only the CAD model of the target object and can precisely estimate its pose without any additional fine-tuning. While existing model-based methods suffer from inefficiency due to using a large number of templates, our method enables fast and accurate estimation with a small number of templates. This improvement is achieved by finding semidense correspondences between the input image and the pre-rendered templates. Our method achieves strong generalization performance by leveraging a hybrid representation that combines patch-level classification and offset regression. Additionally, our pose refinement model estimates probabilistic flow between the input image and the rendered image, refining the initial estimate to an accurate pose using a differentiable PnP layer. We demonstrate that our method not only estimates object poses rapidly but also outperforms existing methods by a large margin on the seven core datasets of the BOP Challenge, achieving state-of-theart accuracy.</summary>
    
    
    
    <category term="读万卷书" scheme="https://blog.032802.xyz/categories/%E8%AF%BB%E4%B8%87%E5%8D%B7%E4%B9%A6/"/>
    
    
    <category term="Object Pose Estimation" scheme="https://blog.032802.xyz/tags/Object-Pose-Estimation/"/>
    
    <category term="2025CVPR" scheme="https://blog.032802.xyz/tags/2025CVPR/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】Any6D: Model-free 6D Pose Estimation of Novel Objects</title>
    <link href="https://blog.032802.xyz/paper-reading/2025/any6d-model-free-6d-pose-estimation-of-novel-objects_2025_Lee.html"/>
    <id>https://blog.032802.xyz/paper-reading/2025/any6d-model-free-6d-pose-estimation-of-novel-objects_2025_Lee.html</id>
    <published>2025-04-03T02:44:15.000Z</published>
    <updated>2025-04-03T02:44:15.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="any6d-model-free-6d-pose-estimation-of-novel-objects">Any6D:Model-free 6D Pose Estimation of Novel Objects</h1><table><colgroup><col style="width: 7%" /><col style="width: 9%" /><col style="width: 12%" /><col style="width: 12%" /><col style="width: 44%" /><col style="width: 12%" /></colgroup><thead><tr><th style="text-align: center;">方法</th><th style="text-align: center;">类型</th><th style="text-align: center;">训练输入</th><th style="text-align: center;">推理输入</th><th style="text-align: center;">输出</th><th style="text-align: center;">pipeline</th></tr></thead><tbody><tr><td style="text-align: center;">Any6D</td><td style="text-align: center;">任意级</td><td style="text-align: center;">RGBDs</td><td style="text-align: center;">RGBDs</td><td style="text-align: center;">相对<spanclass="math inline">\(\mathbf{R}, \mathbf{t}\)</span></td><td style="text-align: center;"></td></tr></tbody></table><ul><li>2025.04.05：任意物体方法，输入一张锚点图<spanclass="math inline">\(I_A\)</span>和一张查询图<spanclass="math inline">\(I_Q\)</span>，估计锚点图到查询图的相对位姿。首先基于锚点图，用图生3D模型来生成3D模型<spanclass="math inline">\(O_N\)</span>，然后将<spanclass="math inline">\(O_N\)</span>与<spanclass="math inline">\(I_A\)</span>对齐，对齐的过程会得到变换<spanclass="math inline">\(\mathbf{T}_a \in\text{SE}(3)\)</span>和物体大小<span class="math inline">\(s \in\mathbb{R}^3\)</span>，使用这两个变换能将<spanclass="math inline">\(O_N\)</span>变换为<spanclass="math inline">\(O_{M^\prime}\)</span>。<spanclass="math inline">\(O_{M^\prime}\)</span>会再次进行优化得到<spanclass="math inline">\(O_M\)</span>和<spanclass="math inline">\(\mathbf{T}_{O_M \to A}\)</span>，然后基于<spanclass="math inline">\(O_M\)</span>和<spanclass="math inline">\(I_Q\)</span>，使用渲染-比较的方法，得到<spanclass="math inline">\(\mathbf{T}_{O_M \to Q}\)</span>。结合<spanclass="math inline">\(\mathbf{T}_{O_M \to A}\)</span>和<spanclass="math inline">\(\mathbf{T}_{O_M \to Q}\)</span>，得到<spanclass="math inline">\(\mathbf{T}_{A \to Q}\)</span>。</li></ul><h2 id="abstract">Abstract</h2><h2 id="introduction">1. Introduction</h2><figure><imgsrc="https://img.032802.xyz/paper-reading/2025/any6d-model-free-6d-pose-estimation-of-novel-objects_2025_Lee/teaser_png.webp"alt="Figure 1. Our method accurately estimates 6D object pose for novel objects on drastically different scenes and viewpoints using only a single RGB-D anchor image. We achieve robust pose estimation without requiring precise CAD models or posed multi-view reference images." /><figcaption aria-hidden="true">Figure 1. Our method accurately estimates6D object pose for novel objects on drastically different scenes andviewpoints using only a single RGB-D anchor image. We achieve robustpose estimation without requiring precise CAD models or posed multi-viewreference images.</figcaption></figure><p>The main contributions of our work are as follows:</p><ul><li>We introduce Any6DPose, a novel framework that enables 6D pose andsize estimation of novel objects in different scenes from only a singlereference image.</li><li>We propose a straightforward yet effective object alignmenttechnique that addresses the challenges of existing 3D generationmodels, specifically improving 2D-3D alignment and size estimation foraccurate pose estimation.</li><li>We validate our approach through extensive experiments,demonstrating superior performance compared to state-of-the-art methodsacross five benchmark datasets.</li></ul><h2 id="related-works">2. Related Works</h2><h2 id="method">3. Method</h2><figure><imgsrc="https://img.032802.xyz/paper-reading/2025/any6d-model-free-6d-pose-estimation-of-novel-objects_2025_Lee/method_png.webp"alt="Figure 2. Overview of the Any6D framework for model-free object pose estimation. First, we reconstruct normalized object shape O_N from the image-to3D model. Then, we estimate accurate object pose and size from anchor image I_A using the proposed object alignment (Sec. 3.1). Next, we use the query image I_Q to estimate the pose with the reconstructed metric-scale object shape O_M (Sec. 3.2)." /><figcaption aria-hidden="true">Figure 2. Overview of the Any6D frameworkfor model-free object pose estimation. First, we reconstruct normalizedobject shape <span class="math inline">\(O_N\)</span> from theimage-to3D model. Then, we estimate accurate object pose and size fromanchor image <span class="math inline">\(I_A\)</span> using the proposedobject alignment (Sec. 3.1). Next, we use the query image <spanclass="math inline">\(I_Q\)</span> to estimate the pose with thereconstructed metric-scale object shape <spanclass="math inline">\(O_M\)</span> (Sec. 3.2).</figcaption></figure><p>给定锚点RGBD图像<spanclass="math inline">\(I_A\)</span>和查询RGBD图像<spanclass="math inline">\(I_Q\)</span>，我们的任务是估计出这两张图像之间的相对位姿。查询图像可能在和锚点图像完全不同的视角和场景下捕获同一对象。我们的任务是估计出<spanclass="math inline">\(I_A\)</span>和<spanclass="math inline">\(I_Q\)</span>之间的相对位姿<spanclass="math inline">\(\mathbf{T}_{A \to Q} \in\text{SE}(3)\)</span>，其中<span class="math inline">\(\mathbf{T}_{A \toQ}\)</span>定义为一个刚体变换<span class="math inline">\([R |t]\)</span>，包含一个旋转<span class="math inline">\(R \in\text{SO}(3)\)</span>和平移<span class="math inline">\(t \in\mathbb{R}^3\)</span>。</p><p>在先前的方法中，一部分方法使用可见部分匹配来估计两个视角之间同一物体的相对位姿，这些方法在视角之间的重叠较大时有效，但在视角变化较大时性能较差；另一部分方法通过重建3D模型来实现完全到部分的匹配。</p><p>提出的Any6D可以估计锚点图像<spanclass="math inline">\(I_A\)</span>和查询图像<spanclass="math inline">\(I_Q\)</span>之间的相对位姿<spanclass="math inline">\(\mathbf{T}_{A \toQ}\)</span>，方法包括两个部分：</p><ol type="1"><li>首先使用image-to-3D模型，在不考虑真实世界的比例和位姿的前提下，从锚点图像重建归一化物体模型<spanclass="math inline">\(O_N\)</span>，然后通过确定实际对象大小<spanclass="math inline">\(s \in \mathbb{R}^3\)</span>和位姿<spanclass="math inline">\(\mathbf{T}_{O_M \toA}\)</span>来估计度量尺度物体模型<spanclass="math inline">\(O_M\)</span>，并将其在二维和三维空间中进行精确对齐；</li><li>接下来，我们利用重建的度量尺度物体模型<spanclass="math inline">\(O_M\)</span>和查询图像<spanclass="math inline">\(I_Q\)</span>进行姿态估计，通过融合<spanclass="math inline">\(\mathbf{T}_{O_M \to A}\)</span>和<spanclass="math inline">\(\mathbf{T}_{O_M \to Q}\)</span>推到出相对变换<spanclass="math inline">\(\mathbf{T}_{A \toQ}\)</span>。Any6D框架的完整工作流程如图2所示。</li></ol><h3 id="coarse-object-alignment">3.1. Coarse Object Alignment</h3><p>据我们所知，现阶段还没有可靠的RGBD单视角度量尺度重建方案可以有效的处理各种对象。所以这里使用“InstantMesh:Efficient 3D Mesh Generation from a Single Image with Sparse-view LargeReconstructionModels”。然而，一个关键的限制在于，3D物体重建只能生成具有归一化尺度的物体模型<spanclass="math inline">\(O_N\)</span>（XYZ轴的范围都是<spanclass="math inline">\([-1,1]\)</span>），这意味着重建的模型未针对实际场景进行正确的尺寸缩放或空间定位。所以我们进行了物体对齐：首先估计物体形状<spanclass="math inline">\(O_M\)</span>的粗略尺寸，然后通过联合求解<spanclass="math inline">\(\mathbf{T}_{O_M \toA}\)</span>来优化该尺寸。我们的方法涉及在锚点图像<spanclass="math inline">\(I_A\)</span>与归一化形状<spanclass="math inline">\(O_N\)</span>之间进行3D和2D的物体模型对齐，具体包括变换<spanclass="math inline">\(\mathbf{T}_a \in \text{SE}(3)\)</span>和尺寸<spanclass="math inline">\(s \in \mathbb{R}^3\)</span>。</p><p>具体来说，我们基于<spanclass="math inline">\(I_A\)</span>，使用由粗到细的方法来估计物体尺寸<spanclass="math inline">\(s\)</span>。首先通过比较<spanclass="math inline">\(I_A\)</span>和<spanclass="math inline">\(O_N\)</span>之间来自各自物体中心的点云来初始化粗略对象大小。虽然可以使用点的均值来直接估计中心，但是由于锚点图像中的部分视角问题和锚点图像中存在离群点，这种直接估计的方法是不可靠的。如图3(a)和图3(b)所示。</p><figure><imgsrc="https://img.032802.xyz/paper-reading/2025/any6d-model-free-6d-pose-estimation-of-novel-objects_2025_Lee/points_png.webp"alt="Figure 3. Visualization of each point clouds and center of mustard object." /><figcaption aria-hidden="true">Figure 3. Visualization of each pointclouds and center of mustard object.</figcaption></figure><p>因为锚点图像中物体的部分可见性，使用简单的boundingbox轴对齐方法同样也会导致不准确，如图3(c)所示。因此，我们提出使用有向边界框来确定物体中心，如图3(d)所示，这为<spanclass="math inline">\(I_A\)</span>提供了更可靠的粗略中心估计。对于轴对齐，我们将有向边界框与XYZ轴对齐，对不同的旋转角度进行采样，然后计算<spanclass="math inline">\(I_A\)</span>和<spanclass="math inline">\(O_N\)</span>在不同角度旋转后boundingbox之间的IoU。能够使IoU最大的旋转和缩放组合会把<spanclass="math inline">\(O_N\)</span>变换为粗略对齐的物体模型，并将其更新为初始物体模型<spanclass="math inline">\(O_{M^\prime}\)</span>，随后<spanclass="math inline">\(O_{M^\prime}\)</span>将被用于精确的位姿和尺寸估计。</p><h3 id="fine-object-alignment">3.2. Fine Object Alignment</h3><p>方法首先重建粗略物体模型<spanclass="math inline">\(O_{M^\prime}\)</span>，然后通过物体和大小联合优化来优化位姿和物体大小。</p><p>优化部分的灵感来自“FoundationPose: Unified 6D Pose Estimation andTracking of NovelObjects”，但是FoundationPose需要物体的CAD模型，在没有物体CAD模型的情况下，则需要多张有位姿标注的参考图像。因此，我们开发了一个联合模块，将大小估计任务融入到位姿优化过程中。这使我们能够可靠地同时估计大小和位姿。</p><p>优化流程主要包括三个主要模块：位姿估计、大小估计和轴对齐，这些模块在一个统一的过程中协同工作，通过在大小优化和位姿优化这两项任务之间交替进行来实现。我们首先使用<spanclass="math inline">\(O_{M^\prime}\)</span>来估计初始位姿，同时优化物体大小。在FoundationPose中，采样生成位姿假设的过程只在<spanclass="math inline">\(\text{SO}(3)\)</span>中进行，而没有考虑大小的变化。与FoundationPose不同的是，我们的方法除了在<spanclass="math inline">\(\text{SO}(3)\)</span>中采样，还额外采样了不同的大小。具体来说，大小采样在每个轴上的范围为<spanclass="math inline">\(\Delta s \in [s_0, s_1]\)</span>（其中<spanclass="math inline">\(s_0 = 0.6, s_1 =1.4\)</span>）。然后我们使用FoundationPose中的模块来优化位姿假设，并将优化后的位姿进行渲染，以与查询图像比较。最优位姿的选择会基于FoundationPose中位姿选择模块给出的比较分数进行选择。当最优大小确定时，我们会将物体大小进行缩放，并进入到位姿优化阶段，包括一个轴对齐步骤，以实现更高的精度。这种更新后的对齐方式利用了大小和位姿的联合估计，比传统的基于IoU的方法具有更高的精度。</p><p>在优化了物体参数之后，我们确定最终的物体位姿<spanclass="math inline">\(T_{O_M \toA}\)</span>，它能为重建的物体模型提供精确的对齐。利用锚点图像<spanclass="math inline">\(I_A\)</span>，查询图像<spanclass="math inline">\(I_Q\)</span>和重建的物体模型<spanclass="math inline">\(O_M\)</span>，我们通过组合两个变换来估计相对位姿<spanclass="math inline">\(\mathbf{T}_{A \toQ}\)</span>：从物体模型到锚点图像的变换<spanclass="math inline">\(\mathbf{T}_{O_M \toA}\)</span>和从物体模型到查询图像的变换<spanclass="math inline">\(\mathbf{T}_{O_M \toQ}\)</span>。相对位姿可以表示如下：</p><p><span class="math display">\[\begin{equation}\label{eq1}    \mathbf{T}_{A \to Q} = (\mathbf{T}_{O_M \to A})^{-1} \cdot\mathbf{T}_{O_M \to Q}\end{equation}\]</span></p><p>对于位姿选择，我们采用一个两阶段的渲染-比较策略。首先，一个位姿排序网络会通过比较渲染结果和裁剪后的观测图像来评估每个假设，得到一个嵌入向量以量化对齐质量。然后，我们对所有位姿假设的拼接嵌入向量应用自注意力机制，融入全局上下文信息，从而生成最终分数，以便选出最优位姿。</p><h2 id="experiments">4. Experiments</h2><h3 id="datasets">4.1. Datasets</h3><h3 id="metrics">4.2. Metrics</h3><h3 id="comparison-with-state-of-the-art">4.3. Comparison withState-of-the-art</h3><h3 id="qualitative-results">4.4. Qualitative Results</h3><figure><imgsrc="https://img.032802.xyz/paper-reading/2025/any6d-model-free-6d-pose-estimation-of-novel-objects_2025_Lee/ho3d_qual_png.webp"alt="Figure 4. Qualitative comparison of state-of-the-art methods on the HO3D Dataset. In this challenging scenario, the left anchor image shows only partially visible objects, while the query images are not visible due to occlusion or different viewing angles. This represents the most challenging case for matching. Gedi, being a depth-based method, shows ambiguity when dealing with RGB-based non-symmetric objects." /><figcaption aria-hidden="true">Figure 4. Qualitative comparison ofstate-of-the-art methods on the HO3D Dataset. In this challengingscenario, the left anchor image shows only partially visible objects,while the query images are not visible due to occlusion or differentviewing angles. This represents the most challenging case for matching.Gedi, being a depth-based method, shows ambiguity when dealing withRGB-based non-symmetric objects.</figcaption></figure><figure><imgsrc="https://img.032802.xyz/paper-reading/2025/any6d-model-free-6d-pose-estimation-of-novel-objects_2025_Lee/ycbineoat_qual_png.webp"alt="Figure 5. Qualitative comparison of state-of-the-art methods on the YCBInEOAT Dataset. In this challenging scenario, the left anchor image shows only partially visible objects, while the query images are not visible due to occlusion or different viewing angles. This represents the most challenging case for matching. Gedi, being a depth-based method, shows ambiguity when dealing with RGB-based non-symmetric objects." /><figcaption aria-hidden="true">Figure 5. Qualitative comparison ofstate-of-the-art methods on the YCBInEOAT Dataset. In this challengingscenario, the left anchor image shows only partially visible objects,while the query images are not visible due to occlusion or differentviewing angles. This represents the most challenging case for matching.Gedi, being a depth-based method, shows ambiguity when dealing withRGB-based non-symmetric objects.</figcaption></figure><h3 id="ablation-studies">4.5. Ablation Studies</h3><figure><imgsrc="https://img.032802.xyz/paper-reading/2025/any6d-model-free-6d-pose-estimation-of-novel-objects_2025_Lee/ablation_png.webp"alt="Figure 6. Comparison of shape quality between baseline method and ours." /><figcaption aria-hidden="true">Figure 6. Comparison of shape qualitybetween baseline method and ours.</figcaption></figure><h2 id="conclusion">5. Conclusion</h2><div class="pdf-container" data-target="https://arxiv.org/pdf/2503.18673" data-height="500px"></div>]]></content>
    
    
    <summary type="html">We introduce Any6D, a model-free framework for 6D object pose estimation that requires only a single RGB-D anchor image to estimate both the 6D pose and size of unknown objects in novel scenes. Unlike existing methods that rely on textured 3D models or multiple viewpoints, Any6D leverages a joint object alignment process to enhance 2D-3D alignment and metric scale estimation for improved pose accuracy. Our approach integrates a renderand-compare strategy to generate and refine pose hypotheses, enabling robust performance in scenarios with occlusions, non-overlapping views, diverse lighting conditions, and large cross-environment variations. We evaluate our method on five challenging datasets: REAL275, ToyotaLight, HO3D, YCBINEOAT, and LM-O, demonstrating its effectiveness in significantly outperforming state-of-the-art methods for novel object pose estimation. Project page: https://taeyeop.com/any6d.</summary>
    
    
    
    <category term="读万卷书" scheme="https://blog.032802.xyz/categories/%E8%AF%BB%E4%B8%87%E5%8D%B7%E4%B9%A6/"/>
    
    
    <category term="Object Pose Estimation" scheme="https://blog.032802.xyz/tags/Object-Pose-Estimation/"/>
    
    <category term="2025CVPR" scheme="https://blog.032802.xyz/tags/2025CVPR/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】VI-Net: Boosting Category-level 6D Object Pose Estimation via Learning Decoupled Rotations on the Spherical Representations</title>
    <link href="https://blog.032802.xyz/paper-reading/2023/vi-net-boosting-category-level-6d-object-pose-estimation-via-learning-decoupled-rotations-on-the-spherical-representations_2023_Lin.html"/>
    <id>https://blog.032802.xyz/paper-reading/2023/vi-net-boosting-category-level-6d-object-pose-estimation-via-learning-decoupled-rotations-on-the-spherical-representations_2023_Lin.html</id>
    <published>2025-04-01T03:04:51.000Z</published>
    <updated>2025-04-01T03:04:51.000Z</updated>
    
    <content type="html"><![CDATA[<h1id="vi-net-boosting-category-level-6d-object-pose-estimation-via-learning-decoupled-rotations-on-the-spherical-representations">VI-Net:Boosting Category-level 6D Object Pose Estimation via Learning DecoupledRotations on the Spherical Representations</h1><table><colgroup><col style="width: 6%" /><col style="width: 6%" /><col style="width: 16%" /><col style="width: 16%" /><col style="width: 44%" /><col style="width: 8%" /></colgroup><thead><tr><th style="text-align: center;">方法</th><th style="text-align: center;">类型</th><th style="text-align: center;">训练输入</th><th style="text-align: center;">推理输入</th><th style="text-align: center;">输出</th><th style="text-align: center;">pipeline</th></tr></thead><tbody><tr><td style="text-align: center;">VI-Net</td><td style="text-align: center;">类别级</td><td style="text-align: center;">RGBD + 物体类别</td><td style="text-align: center;">RGBD + 物体类别</td><td style="text-align: center;">绝对<spanclass="math inline">\(\mathbf{R}, \mathbf{t}, \mathbf{s}\)</span></td><td style="text-align: center;"></td></tr></tbody></table><ul><li>2025.04.01：类别级方法，使用一个网络估计旋转，另一个网络估计平移和缩放，24年CVPRSecondPose沿用了这个方法，对于旋转，该方法将旋转估计分为视角旋转和面内旋转，视作一个分类任务，分别估计，最后组合；对于平移和缩放，该方法使用PointNet++提取特征，直接估计平移和旋转</li></ul><h2 id="abstract">Abstract</h2><h2 id="introduction">1. Introduction</h2><figure><imgsrc="https://img.032802.xyz/paper-reading/2023/vi-net-boosting-category-level-6d-object-pose-estimation-via-learning-decoupled-rotations-on-the-spherical-representations_2023_Lin/intro.webp"alt="Figure 1. An illustration of the factorization of rotation \mathbf{R} into a viewpoint (out-of-plane) rotation \mathbf{R}_{vp} and an in-plane rotation \mathbf{R}_{ip} (around Z-axis). Notations are explained in Sec. 3." /><figcaption aria-hidden="true">Figure 1. An illustration of thefactorization of rotation <spanclass="math inline">\(\mathbf{R}\)</span> into a viewpoint(out-of-plane) rotation <spanclass="math inline">\(\mathbf{R}_{vp}\)</span> and an in-plane rotation<span class="math inline">\(\mathbf{R}_{ip}\)</span> (around Z-axis).Notations are explained in Sec. 3.</figcaption></figure><figure><imgsrc="https://img.032802.xyz/paper-reading/2023/vi-net-boosting-category-level-6d-object-pose-estimation-via-learning-decoupled-rotations-on-the-spherical-representations_2023_Lin/vi-net.webp"alt="Figure 2. An illustration of VI-Net for rotation estimation. We firstly construct a Spherical Feature Pyramid Network based on spatial spherical convolutions (SPA-SConv) to exact the high-level spherical feature map \mathcal{S}. On top of \mathcal{S}, a V-Branch is employed to search the canonical zenith direction on the sphere via binary classification for the generation of the viewpoint rotation \mathbf{R}_{vp}, while another I-Branch is used to estimate the in-plane rotation \mathbf{R}_{ip} by transforming \mathcal{S} to view the object from the canonical zenith direction. Finally we have \mathbf{R} = \mathbf{R}_{vp}\mathbf{R}_{ip}. Best view in the electronic version." /><figcaption aria-hidden="true">Figure 2. An illustration of VI-Net forrotation estimation. We firstly construct a Spherical Feature PyramidNetwork based on spatial spherical convolutions (SPA-SConv) to exact thehigh-level spherical feature map <spanclass="math inline">\(\mathcal{S}\)</span>. On top of <spanclass="math inline">\(\mathcal{S}\)</span>, a V-Branch is employed tosearch the canonical zenith direction on the sphere via binaryclassification for the generation of the viewpoint rotation <spanclass="math inline">\(\mathbf{R}_{vp}\)</span>, while another I-Branchis used to estimate the in-plane rotation <spanclass="math inline">\(\mathbf{R}_{ip}\)</span> by transforming <spanclass="math inline">\(\mathcal{S}\)</span> to view the object from thecanonical zenith direction. Finally we have <spanclass="math inline">\(\mathbf{R} =\mathbf{R}_{vp}\mathbf{R}_{ip}\)</span>. Best view in the electronicversion.</figcaption></figure><h2 id="related-work">2. Related Work</h2><h2 id="correlation-on-the-sphere-and-rotation-decomposition">3.Correlation on the Sphere and Rotation Decomposition</h2><p>如图1所示，使用<spanclass="math inline">\(\mathbf{R}^T\)</span>将物体坐标系XYZ与相机坐标系X'Y'Z'对齐（假设物体坐标系的XYZ的原点和相机坐标系的X'Y'Z'的原点重合），这个过程可以分为两步：</p><ol type="1"><li>第一步是将Z'轴与Z轴对齐，这一步由旋转矩阵<spanclass="math inline">\(\mathbf{R}_{vp}^T\)</span>实现，“vp”代表的是“viewpoint”，即视角；</li><li>第二步是将X'轴和Y'轴旋转到X轴和Y轴，这一步由旋转矩阵<spanclass="math inline">\(\mathbf{R}_{ip}^T\)</span>实现，“ip”代表的是“in-plane”，即平面内。</li></ol><p>那么将物体坐标系XYZ与相机坐标系X'Y'Z'对齐的过程可以表示为：<spanclass="math inline">\(\mathbf{R}^T =\mathbf{R}_{ip}^T\mathbf{R}_{vp}^T\)</span>。</p><p>由于在<spanclass="math inline">\(SO(3)\)</span>中，矩阵的转置就是矩阵的逆，那么将物体坐标系XYZ和相机坐标系X'Y'Z'恢复到原来的相对位置的过程可以表示为：</p><p><span class="math display">\[\begin{equation}\label{eq1}    \mathbf{R} = \mathbf{R}_{vp}\mathbf{R}_{ip},\end{equation}\]</span></p><p>现在从物体坐标系XYZ和相机坐标系X'Y'Z'对齐时的状态出发，假设在物体坐标系Z轴上有一点<spanclass="math inline">\((0, 0, 1)\)</span>，使用<spanclass="math inline">\(\mathbf{R} =\mathbf{R}_{vp}\mathbf{R}_{ip}\)</span>的过程恢复到原来的相对位置，同样也分为两步：</p><ol type="1"><li><p>首先使用<spanclass="math inline">\(\mathbf{R}_{ip}\)</span>让物体坐标系XYZ旋转绕Z'轴旋转<spanclass="math inline">\(\beta \in [0, 2\pi]\)</span>，其旋转矩阵为：</p><p><span class="math display">\[\begin{equation}\label{eq2}     \mathbf{R}_{ip} =     \begin{bmatrix}         \cos\beta &amp; -\sin\beta &amp; 0 \\         \sin\beta &amp; \cos\beta &amp; 0 \\         0 &amp; 0 &amp; 1     \end{bmatrix}.\end{equation}\]</span></p></li><li><p>此时，物体坐标系XYZ与相机坐标系X'Y'Z'不完全重合（Z轴与Z'轴是重合的，但X轴与X'轴、Y轴与Y'轴不重合），先将物体绕着Y'轴旋转<spanclass="math inline">\(\theta \in [0, \pi]\)</span>，其旋转矩阵为：</p><p><span class="math display">\[\mathbf{R}_Y(\theta) =\begin{bmatrix}     \cos\theta &amp; 0 &amp; \sin\theta \\     0 &amp; 1 &amp; 0 \\     -\sin\theta &amp; 0 &amp; \cos\theta\end{bmatrix};\]</span></p></li><li><p>再将物体绕着Z'轴旋转<span class="math inline">\(\varphi \in [0,2\pi]\)</span>，其旋转矩阵为：</p><p><span class="math display">\[\mathbf{R}_Z(\varphi) =\begin{bmatrix}     \cos\varphi &amp; -\sin\varphi &amp; 0 \\     \sin\varphi &amp; \cos\varphi &amp; 0 \\     0 &amp; 0 &amp; 1\end{bmatrix};\]</span></p></li></ol><p>那么结合上面的第2步和第3步，Z轴上的这一点的坐标变为<spanclass="math inline">\((v_x, v_y, v_z)\)</span>，那么可以计算<spanclass="math inline">\((r, \varphi,\theta)\)</span>为（这里我不理解，我觉得公式是错的）：</p><p><span class="math display">\[\begin{equation}\label{eq3}    \left\{\begin{matrix}        \begin{aligned}            r &amp;= 1 \\            \varphi &amp;= \arctan(\frac{v_x}{v_z}) \\            \theta &amp;= \arccos(\frac{v_y}{r})        \end{aligned}    \end{matrix}\right.,\end{equation}\]</span></p><p>结合<span class="math inline">\(\mathbf{R}_Y(\theta)\)</span>和<spanclass="math inline">\(\mathbf{R}_Z(\varphi)\)</span>，可以得到：</p><p><span class="math display">\[\begin{equation}\label{eq4}    \begin{aligned}        \mathbf{R}_{vp} &amp;= \mathbf{R}_Z(\varphi)\mathbf{R}_Y(\theta)\\        &amp;= \begin{bmatrix}            \cos\varphi &amp; -\sin\varphi &amp; 0 \\            \sin\varphi &amp; \cos\varphi &amp; 0 \\            0 &amp; 0 &amp; 1        \end{bmatrix}        \begin{bmatrix}            \cos\theta &amp; 0 &amp; \sin\theta \\            0 &amp; 1 &amp; 0 \\            -\sin\theta &amp; 0 &amp; \cos\theta        \end{bmatrix}    \end{aligned}.\end{equation}\]</span></p><p>结合<span class="math inline">\(\eqref{eq4}\)</span>和<spanclass="math inline">\(\eqref{eq2}\)</span>，可以发现<spanclass="math inline">\(\mathbf{R} =\mathbf{R}_{vp}\mathbf{R}_{ip}\)</span>与ZYZ欧拉角<spanclass="math inline">\(\varphi, \theta, \beta\)</span>的参数化一致。</p><p>根据以上推导，估计物体的旋转可以分解为两个步骤：</p><ol type="1"><li>在球体上搜索点<span class="math inline">\((v_x, v_y,v_z)\)</span>，以获得<span class="math inline">\(\varphi\)</span>和<spanclass="math inline">\(\theta\)</span>，共同得到视角旋转<spanclass="math inline">\(\mathbf{R}_{vp}\)</span>；</li><li>通过<spanclass="math inline">\(\mathbf{R}_{vp}\)</span>将Z'轴与Z轴对齐，然后回归面内旋转<spanclass="math inline">\(\mathbf{R}_{ip}\)</span>。</li></ol><h2 id="vi-net-for-rotation-estimation">4. VI-Net for RotationEstimation</h2><h3 id="conversion-as-spherical-representations">4.1. Conversion asSpherical Representations</h3><p>给定一个点集<span class="math inline">\(\mathcal{P} \in \mathbb{R}^{N\times 3}\)</span>，每一个点都对应一个特征<spanclass="math inline">\(\mathcal{F} \in \mathbb{R}^{N \timesC_0}\)</span>（比如径向距离、RGB值、表面法线等），首先使用“LearningSO(3) Equivariant Representations with Spherical CNNs”和“DualPoseNet:Category-level 6D Object Pose and Size Estimation Using Dual PoseNetwork with Refined Learning of PoseConsistency”中的方法，生成在球体上定义的特征图<spanclass="math inline">\(\mathcal{S}_0 \in \mathbb{R}^{C_0 \times H_0\times W_0}\)</span>，其中<spanclass="math inline">\(N\)</span>是点的数量，<spanclass="math inline">\(H_0 \times W_0\)</span>是球面采样分辨率，<spanclass="math inline">\(C_0\)</span>是特征维度（比如<spanclass="math inline">\(C_0 = 1\)</span>为径向距离，<spanclass="math inline">\(C_0 = 3\)</span>为RGB值或表面法线）。</p><figure><imgsrc="https://img.032802.xyz/paper-reading/2023/vi-net-boosting-category-level-6d-object-pose-estimation-via-learning-decoupled-rotations-on-the-spherical-representations_2023_Lin/spa-sconv.webp"alt="Figure 3. An illustration of the feature padding in SPA-SConv. Best view in the electronic version." /><figcaption aria-hidden="true">Figure 3. An illustration of the featurepadding in SPA-SConv. Best view in the electronic version.</figcaption></figure><p>具体来说，将球面坐标系沿方位轴（水平方向）和倾角轴（竖直方向）均匀的划分为<spanclass="math inline">\(H_0 \timesW_0\)</span>个网格（如图3所示），在每一个网格中，我们搜索径向距离最大的点，表示为<spanclass="math inline">\(\mathbf{p}_{h, w}^{max}\)</span>，并让<spanclass="math inline">\(\mathcal{S}_0(h, w) = \mathbf{f}_{h, w}^{max} \in\mathcal{F}\)</span>，对应于<span class="math inline">\(\mathbf{p}_{h,w}^{max}\)</span>；如果这个区域内没有点，则让<spanclass="math inline">\(\mathcal{S}_0(h, w) = \mathbf{0}\)</span>。</p><h3 id="spherical-feature-pyramid-network">4.2. Spherical FeaturePyramid Network</h3><p>使用ResNet18构建特征金字塔网络（FPN），用SPAtial SphericalConvolutions (SPA-SConvs)，代替传统的2D卷积，得到高级语义球形特征图<spanclass="math inline">\(\mathcal{S} \in \mathbb{R}^{C \times H \timesW}\)</span>，如图2所示。第5部分中会有详细介绍。</p><h3 id="v-branch">4.3. V-Branch</h3><p>V-Branch估计视角旋转<spanclass="math inline">\(\mathbf{R}_{vp}\)</span>，视角旋转中包含了方位角<spanclass="math inline">\(\varphi\)</span>和倾角<spanclass="math inline">\(\theta\)</span>，为了简化任务，可以分别估计<spanclass="math inline">\(\varphi\)</span>和<spanclass="math inline">\(\theta\)</span>，这也有效缓解了正负样本对不平衡的问题。图2给出了V-Branch的图示。</p><p>以两层MLP提升<spanclass="math inline">\(\mathcal{S}\)</span>的特征维度，得到<spanclass="math inline">\(\mathcal{S}_{vp} \in \mathbb{R}^{C_{vp} \times H\times W}\)</span>。为了学习水平方位角<spanclass="math inline">\(\varphi\)</span>，在竖直方向上的倾角维度对<spanclass="math inline">\(S_{vp}\)</span>进行最大池化，得到<spanclass="math inline">\(\mathcal{F}_\varphi \in \mathbb{R}^{C_{vp} \timesW}\)</span>，再将其送入另一个MLP中得到<spanclass="math inline">\(W\)</span>方位角区域的概率图<spanclass="math inline">\(\mathcal{Y}_\alpha \in\mathbb{R}^W\)</span>。<spanclass="math inline">\(\mathcal{Y}_\alpha\)</span>中的每一个元素都表示该区域成为目标的可能性。将具有最大概率的元素的索引表示为<spanclass="math inline">\(w_{max}\)</span>，则有：</p><p><span class="math display">\[\begin{equation}\label{eq5}    \varphi = \frac{w_{max} + 0.5}{W} \times 2\pi.\end{equation}\]</span></p><p>同样，对于倾角<spanclass="math inline">\(\theta\)</span>，沿水平方向上的方位角维度对<spanclass="math inline">\(S_{vp}\)</span>进行最大池化，得到<spanclass="math inline">\(\mathcal{F}_\theta \in \mathbb{R}^{C_{vp} \timesH}\)</span>，然后得到<span class="math inline">\(\mathcal{Y}_\beta \in\mathbb{R}^H\)</span>，那么<spanclass="math inline">\(\theta\)</span>可以计算如下：</p><p><span class="math display">\[\begin{equation}\label{eq6}    \theta = \frac{h_{max} + 0.5}{H} \times \pi.\end{equation}\]</span></p><p>其中<span class="math inline">\(h_{max}\)</span>是<spanclass="math inline">\(\mathcal{Y}_\beta\)</span>中最大概率的索引。最后，结合<spanclass="math inline">\(\eqref{eq5}\)</span>、<spanclass="math inline">\(\eqref{eq6}\)</span>和<spanclass="math inline">\(\eqref{eq4}\)</span>，可以得到视角旋转<spanclass="math inline">\(\mathbf{R}_{vp} =\mathbf{R}_Z(\varphi)\mathbf{R}_Y(\theta)\)</span>。</p><h3 id="i-branch">4.4. I-Branch</h3><p>在V-Branch之后，我们已经得到了视角旋转<spanclass="math inline">\(\mathbf{R}_{vp}\)</span>，那么就可以使用<spanclass="math inline">\(\mathbf{R}_{vp}\)</span>将Z'轴与Z轴对齐（如图1所示）。对齐后可以构建一个新的球面特征图<spanclass="math inline">\(S_{ip} \in \mathbb{R}^{C \times H \timesW}\)</span>。<spanclass="math inline">\(\mathcal{S}\)</span>的视点不变性使得在特征空间中实现变换以获得<spanclass="math inline">\(S_{ip}\)</span>成为可能。</p><p>对于分辨率为<span class="math inline">\(H \timesW\)</span>的规则球面映射，我们将所有<spanclass="math inline">\(HW\)</span>离散锚点的中心点表示为点集<spanclass="math inline">\(\mathcal{G} =\{\mathbf{g}\}\)</span>。当我们用<spanclass="math inline">\(\mathbf{R}_{vp}\)</span>将点集<spanclass="math inline">\(\mathcal{P} = \{\mathbf{p}\}\)</span>旋转到<spanclass="math inline">\(\mathcal{P}^\prime = \{\mathbf{p}^\prime\} =\{\mathbf{R}_{vp}^T\mathbf{p}\}\)</span>时，<spanclass="math inline">\(\mathcal{S}\)</span>的锚点也旋转为<spanclass="math inline">\(\mathcal{G}^\prime = \{\mathbf{g}^\prime\} =\{\mathbf{R}_{vp}^T\mathbf{g}\}\)</span>；我们将<spanclass="math inline">\(\mathbf{g}^\prime\)</span>的特征标记为<spanclass="math inline">\(\mathcal{S}^{\mathbf{g}^\prime}\)</span>。</p><p>为了从顶部观察物体，需要为变换后的<spanclass="math inline">\(\mathcal{P}^\prime\)</span>构建一个新的球形特征图<spanclass="math inline">\(\mathcal{S}_{ip}\)</span>。对于<spanclass="math inline">\(\mathcal{S}_{ip}\)</span>，我们使用“PointNet++:Deep Hierarchical Feature Learning on Point Sets in a MetricSpace”中的点特征加权插值方法，在每个锚点<spanclass="math inline">\(\mathbf{g}\)</span>上生成其特征，表示为<spanclass="math inline">\(\mathcal{S}_{ip}^\mathbf{g}\)</span>，如下所示：</p><p><span class="math display">\[\begin{equation}\label{eq7}    \mathcal{S}_{ip}^\mathbf{g} = \frac{\sum_{i = 1}^k a_i\mathcal{S}^{\mathbf{g}^\prime_i}}{\sum_{i = 1}^k a_i},\end{equation}\]</span></p><p>其中<span class="math inline">\(a_i = \frac{1}{\Vert\mathbf{g} -\mathbf{g}^\prime_i\Vert}\)</span>是按点距离衡量的插值权重，<spanclass="math inline">\(\{\mathbf{g}_i^\prime\}_{i = 1}^k \subset\mathcal{G}^\prime\)</span>是<spanclass="math inline">\(\mathbf{g}\)</span>的<spanclass="math inline">\(k\)</span>个最近邻。</p><p>通过<span class="math inline">\(\eqref{eq7}\)</span>实现从<spanclass="math inline">\(\mathcal{S}\)</span>到<spanclass="math inline">\(\mathcal{S}_{ip}\)</span>的转换后，使用卷积来降低<spanclass="math inline">\(\mathcal{S}_{ip}\)</span>的分辨率，并为<spanclass="math inline">\(\mathbf{R}_{ip}\)</span>的回归提取一个全局特征图，如图2所示。旋转的连续6D表示作为回归的输出，然后转换为旋转矩阵<spanclass="math inline">\(\mathbf{R}_{ip}\)</span>。这里预测的<spanclass="math inline">\(\mathbf{R}_{ip}\)</span>并不只包含面内旋转，而是残差视点旋转和精确面内旋转的组合。</p><h3 id="training-of-vi-net">4.5. Training of VI-Net</h3><p>对于V-Branch，使用来自“Focal Loss for Dense ObjectDetection”的焦点损失，给定GT <spanclass="math inline">\(\hat{\mathcal{Y}}_\varphi \in\mathbb{R}^W\)</span>和<spanclass="math inline">\(\hat{\mathcal{Y}}_\theta \in\mathbb{R}^H\)</span>，损失函数为：</p><p><span class="math display">\[\begin{equation}\label{eq8}    \mathcal{L}_{vp} = \mathcal{D}_{FL}(\mathcal{Y}_\varphi,\hat{\mathcal{Y}}_\varphi) + \mathcal{D}_{FL}(\mathcal{Y}_\theta,\hat{\mathcal{Y}}_\theta),\end{equation}\]</span></p><p>其中：</p><p><span class="math display">\[\begin{equation}\label{eq9}    \mathcal{D}_{FL}(\mathcal{Y}, \hat{\mathcal{Y}}) = \frac{1}{M}\sum_{i = 1}^M -\alpha(1 - t_{i, t})^\gamma \log(y_{i, t}),\end{equation}\]</span></p><p>且：</p><p><span class="math display">\[\begin{equation}\label{eq10}    y_{i, t} =    \begin{cases}        y_i, &amp; \text{if} &amp; \hat{y}_i = 1 \\        1 - y_i, &amp; \text{if} &amp; \hat{y}_i = 0    \end{cases},\end{equation}\]</span></p><p>其中<span class="math inline">\(\mathcal{Y} = \{y_i\}_{i =1}^M\)</span>，<span class="math inline">\(\hat{\mathcal{Y}} =\{\hat{y}_i \in \{0, 1\}\}_{i = 1}^M\)</span>。<spanclass="math inline">\(\alpha\)</span>是权重因子，<spanclass="math inline">\(\gamma\)</span>表示调节因子的指数。</p><p>给定GT <spanclass="math inline">\(\hat{\mathbf{R}}\)</span>，我们对I-Branch的输出<spanclass="math inline">\(\mathbf{R} =\mathbf{R}_{vp}\mathbf{R}_{ip}\)</span>监督如下：</p><p><span class="math display">\[\begin{equation}\label{eq11}    \mathcal{L}_{ip} = \Vert\mathbf{R} - \hat{\mathbf{R}}\Vert =\Vert\mathbf{R}_{vp}\mathbf{R}_{ip} - \hat{\mathbf{R}}\Vert.\end{equation}\]</span></p><p>结合<span class="math inline">\(\eqref{eq8}\)</span>和<spanclass="math inline">\(\eqref{eq11}\)</span>，VI-Net的训练目标为：</p><p><span class="math display">\[\begin{equation}\label{eq12}    \min \mathcal{L} = \mathcal{L}_{ip} + \lambda \mathcal{L}_{vp}.\end{equation}\]</span></p><p>其中<spanclass="math inline">\(\lambda\)</span>是平衡两个损失的权重因子。</p><h2 id="spatial-spherical-convolution">5. Spatial SphericalConvolution</h2><p>方法建立具有规则2D空间大小的球形特征图以实现旋转估计，特征图建立后，就可以使用传统的2D卷积来处理球形信号。但是，直接在球形特征图上使用2D卷积会有边界问题，比如特征图<spanclass="math inline">\(S_0\)</span>上的<span class="math inline">\(S_0(h,1)\)</span>和<span class="math inline">\(S_0(h,W)\)</span>在球面上是相连的，而在特征图中则有很大的距离。此外，为了支持I-Branch中的特征转换以便从天顶方向查看，backbone中的卷积也需要是视角一致的。这里提出了SPAtialSphericalConvolution，即SPA-SConv，在球面上连续提取视角一致的特征，可以灵活的适应现有的卷积架构，例如4.2节中的FPN。</p><p>给定输入球面特征图<span class="math inline">\(\mathcal{S}_l \in\mathbb{R}^{C_l \times H_l \timesW_l}\)</span>和卷积参数（如卷积核大小<spanclass="math inline">\(K\)</span>、步长<spanclass="math inline">\(s\)</span>和输出通道数<spanclass="math inline">\(C_{l +1}\)</span>等），我们分两步实现SPA-SConv：</p><ol type="1"><li>将<span class="math inline">\(\mathcal{S}_l\)</span>padding到<spanclass="math inline">\(S_l^{pad} \in \mathbb{R}^{C_l \times (H_l + 2P)\times (W_l + 2P)}\)</span>，其中<span class="math inline">\(P = \frac{K- 1}{2}\)</span>；</li><li>将对称卷积应用于<spanclass="math inline">\(S_l^{pad}\)</span>，基于没有padding的常规2D卷积，得到输出球面特征图<spanclass="math inline">\(\mathcal{S}_{l + 1} \in \mathbb{R}^{C_{l + 1}\times H_{l + 1} \times W_{l + 1}}\)</span>，其中<spanclass="math inline">\(H_{l + 1} =\lfloor\frac{H_l}{s}\rfloor\)</span>，<span class="math inline">\(W_{l +1} =\lfloor\frac{W_l}{s}\rfloor\)</span>（为简单起见，我们假设2D卷积核的长度和宽度都是<spanclass="math inline">\(K\)</span>，<spanclass="math inline">\(K\)</span>是一个奇数）。</li></ol><p>图3可视化了从<spanclass="math inline">\(\mathcal{S}_l\)</span>到<spanclass="math inline">\(S_l^{pad}\)</span>的padding过程。首先，将<spanclass="math inline">\(\mathcal{S}_l\)</span>的中心作为<spanclass="math inline">\(\mathcal{S}_l^{pad}\)</span>的中心：</p><p><span class="math display">\[\begin{equation}\label{eq13}    S_l^{pad}(h + P, w + P) = \mathcal{S}_l(h, w),\end{equation}\]</span></p><p>对于<span class="math inline">\(\forall h = 1, 2, \cdots,H_l\)</span>和<span class="math inline">\(\forall w = 1, 2, \cdots,W_l\)</span>。接下来，我们沿倾角方向填充<spanclass="math inline">\(\mathcal{S}_l\)</span>：</p><p><span class="math display">\[\begin{equation}\label{eq14}    \begin{aligned}        S_l^{pad}(p, w + P) &amp;= \mathcal{S}_l^{pad}(2P - p + 1,w^\prime), \\        \text{and } S_l^{pad}(H_l + P + p, w + P) &amp;=\mathcal{S}_l^{pad}(H_l + P - p + 1, w^\prime),    \end{aligned}\end{equation}\]</span></p><p>其中：</p><p><span class="math display">\[\begin{equation}\label{eq15}    w^\prime =    \begin{cases}        w + \frac{W_l}{2} + P &amp; \text{if} &amp; w \le \frac{W_l}{2}\\        w - \frac{W_l}{2} + P &amp; &amp; \text{otherwise}    \end{cases},\end{equation}\]</span></p><p>对于<span class="math inline">\(\forall p = 1, 2, \cdots,P\)</span>和<span class="math inline">\(\forall w = 1, 2, \cdots,W_l\)</span>。最后，我们沿方位角方向填充<spanclass="math inline">\(\mathcal{S}_l\)</span>：</p><p><span class="math display">\[\begin{equation}\label{eq16}    \begin{aligned}        S_l^{pad}(h, p) &amp;= \mathcal{S}_l^{pad}(h, W_l + p), \\        \text{and } S_l^{pad}(h, W_l + P + p) &amp;=\mathcal{S}_l^{pad}(h, P + p),    \end{aligned},\end{equation}\]</span></p><p>对于<span class="math inline">\(\forall p = 1, 2, \cdots,P\)</span>和<span class="math inline">\(\forall w = 1, 2, \cdots,W_l\)</span>。</p><p>得到<spanclass="math inline">\(S_l^{pad}\)</span>后，我们就可以使用2D卷积来实现视点一致的对称卷积运算：</p><p><span class="math display">\[\begin{equation}\label{eq17}    \mathcal{S}_{l + 1} = \text{Max}(\text{Conv}(\mathcal{S}_l^{pad};\kappa_l), \text{Conv}(\mathcal{S}_l^{pad}, \text{Flip}(\kappa_l))),\end{equation}\]</span></p><p>其中，<spanclass="math inline">\(\text{Conv}\)</span>表示2D卷积、<spanclass="math inline">\(\text{Flip}\)</span>表示卷积核的水平翻转、<spanclass="math inline">\(\text{Max}\)</span>表示元素级最大池化。根据“PointNet:Deep Learning on Point Sets for 3D Classification andSegmentation”中的方法，<spanclass="math inline">\(\text{Max}\)</span>作为对称函数来聚合特征，并保持视点一致的性质，补充材料中给出了此特性的证明。</p><h2 id="category-level-6d-object-pose-estimation">6. Category-level 6DObject Pose Estimation</h2><p>VI-Net估计<span class="math inline">\(\mathbf{R}, \mathbf{t},\mathbf{s}\)</span>，且在推理时CAD模型不可用。给定一张RGBD图，首先使用MaskRCNN分割物体，对于每个裁剪出的具有点集<spanclass="math inline">\(\mathcal{P} =\{\mathbf{p}\}\)</span>的物体，首先使用PointNet++估计<spanclass="math inline">\(\mathbf{t}\)</span>和<spanclass="math inline">\(\mathbf{s}\)</span>，然后在旋转估计时，将估计出的<spanclass="math inline">\(\mathbf{t}\)</span>和<spanclass="math inline">\(\mathbf{s}\)</span>作为初始化输入到VI-Net中进行归一化：<spanclass="math inline">\(\mathcal{P}^\prime = \{\frac{\mathbf{p} -\mathbf{t}}{\Vert\mathbf{s}\Vert}\}\)</span>，最后使用VI-Net估计<spanclass="math inline">\(\mathbf{R}\)</span>。</p><h3 id="experimental-setups">6.1. Experimental Setups</h3><h3 id="comparisons-with-existing-methods">6.2. Comparisons withExisting Methods</h3><table><caption style="text-align: left; caption-side: bottom;">Table 1. Quantitative comparisons of different methods forcategory-level 6D object pose estimation on REAL275 and CAMERA25datasets [31]. Overall best results are in bold and the second bestresults are underlined. '∗' denotes the IoU metrics used in [23] ratherthan those in [31].</caption><thead><tr><th rowspan="2" style="border-top: 2px solid black; border-bottom: 1px solid black; border-right: 1px solid black;">Method</th><th rowspan="2" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black; border-right: 1px solid black;">Use of<br>Shape Priors</th><th colspan="5" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black; border-right: 1px solid black;">REAL275</th><th colspan="5" style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">CAMERA25</th></tr><tr><th style="text-align: center; border-bottom: 1px solid black;">IoU<sub>75</sub>∗</th><th style="text-align: center; border-bottom: 1px solid black;">5°2cm</th><th style="text-align: center; border-bottom: 1px solid black;">5°5cm</th><th style="text-align: center; border-bottom: 1px solid black;">10°2cm</th><th style="text-align: center; border-bottom: 1px solid black; border-right: 1px solid black;">10°5cm</th><th style="text-align: center; border-bottom: 1px solid black;">IoU<sub>75</sub>∗</th><th style="text-align: center; border-bottom: 1px solid black;">5°2cm</th><th style="text-align: center; border-bottom: 1px solid black;">5°5cm</th><th style="text-align: center; border-bottom: 1px solid black;">10°2cm</th><th style="text-align: center; border-bottom: 1px solid black;">10°5cm</th></tr></thead><tbody><tr><td style="border-right: 1px solid black;">NOCS [31]</td><td style="text-align: center; border-right: 1px solid black;">✕</td><td style="text-align: center;">9.4</td><td style="text-align: center;">7.2</td><td style="text-align: center;">10.0</td><td style="text-align: center;">13.8</td><td style="text-align: center; border-right: 1px solid black;">25.2</td><td style="text-align: center;">37.0</td><td style="text-align: center;">32.3</td><td style="text-align: center;">40.9</td><td style="text-align: center;">48.2</td><td style="text-align: center;">64.6</td></tr><tr><td style="border-right: 1px solid black;">FS-Net [5]</td><td style="text-align: center; border-right: 1px solid black;">✕</td><td style="text-align: center;">-</td><td style="text-align: center;">-</td><td style="text-align: center;">28.2</td><td style="text-align: center;">-</td><td style="text-align: center; border-right: 1px solid black;">60.8</td><td style="text-align: center;">-</td><td style="text-align: center;">-</td><td style="text-align: center;">-</td><td style="text-align: center;">-</td><td style="text-align: center;">-</td></tr><tr><td style="border-right: 1px solid black;">DualPoseNet [20]</td><td style="text-align: center; border-right: 1px solid black;">✕</td><td style="text-align: center;">30.8</td><td style="text-align: center;">29.3</td><td style="text-align: center;">35.9</td><td style="text-align: center;">50.0</td><td style="text-align: center; border-right: 1px solid black;">66.8</td><td style="text-align: center;">71.7</td><td style="text-align: center;">64.7</td><td style="text-align: center;">70.7</td><td style="text-align: center;">77.2</td><td style="text-align: center;">84.7</td></tr><tr><td style="border-right: 1px solid black;">GPV-Pose [8]</td><td style="text-align: center; border-right: 1px solid black;">✕</td><td style="text-align: center;">-</td><td style="text-align: center;">32.0</td><td style="text-align: center;">42.9</td><td style="text-align: center;">-</td><td style="text-align: center; border-right: 1px solid black;">73.3</td><td style="text-align: center;">-</td><td style="text-align: center;">72.1</td><td style="text-align: center;">79.1</td><td style="text-align: center;">-</td><td style="text-align: center;">89.0</td></tr><tr><td style="border-bottom: 1px solid black; border-right: 1px solid black;">SS-ConvNet [18]</td><td style="text-align: center; border-bottom: 1px solid black; border-right: 1px solid black;">✕</td><td style="text-align: center; border-bottom: 1px solid black;">-</td><td style="text-align: center; border-bottom: 1px solid black;">36.6</td><td style="text-align: center; border-bottom: 1px solid black;">43.4</td><td style="text-align: center; border-bottom: 1px solid black;">52.6</td><td style="text-align: center; border-bottom: 1px solid black; border-right: 1px solid black;">63.5</td><td style="text-align: center; border-bottom: 1px solid black;">-</td><td style="text-align: center; border-bottom: 1px solid black;">-</td><td style="text-align: center; border-bottom: 1px solid black;">-</td><td style="text-align: center; border-bottom: 1px solid black;">-</td><td style="text-align: center; border-bottom: 1px solid black;">-</td></tr><tr><td style="border-bottom: 1px solid black; border-right: 1px solid black;">PN2 + VI-Net (Ours)</td><td style="text-align: center; border-bottom: 1px solid black; border-right: 1px solid black;">✕</td><td style="text-align: center; border-bottom: 1px solid black;"><strong>48.3</strong></td><td style="text-align: center; border-bottom: 1px solid black;"><strong>50.0</strong></td><td style="text-align: center; border-bottom: 1px solid black;"><strong>57.6</strong></td><td style="text-align: center; border-bottom: 1px solid black;"><strong>70.8</strong></td><td style="text-align: center; border-bottom: 1px solid black; border-right: 1px solid black;"><strong>82.1</strong></td><td style="text-align: center; border-bottom: 1px solid black;"><strong>79.1</strong></td><td style="text-align: center; border-bottom: 1px solid black;"><u>74.1</u></td><td style="text-align: center; border-bottom: 1px solid black;"><strong>81.4</strong></td><td style="text-align: center; border-bottom: 1px solid black;">79.3</td><td style="text-align: center; border-bottom: 1px solid black;">87.3</td></tr><tr><td style="border-right: 1px solid black;">SPD [28]</td><td style="text-align: center; border-right: 1px solid black;">✓</td><td style="text-align: center;">27.0</td><td style="text-align: center;">19.3</td><td style="text-align: center;">21.4</td><td style="text-align: center;">43.2</td><td style="text-align: center; border-right: 1px solid black;"><u>54.1</u></td><td style="text-align: center;">46.9</td><td style="text-align: center;">54.3</td><td style="text-align: center;">59.0</td><td style="text-align: center;">73.3</td><td style="text-align: center;">81.5</td></tr><tr><td style="border-right: 1px solid black;">CR-Net [32]</td><td style="text-align: center; border-right: 1px solid black;">✓</td><td style="text-align: center;">33.2</td><td style="text-align: center;">27.8</td><td style="text-align: center;">34.3</td><td style="text-align: center;">47.2</td><td style="text-align: center; border-right: 1px solid black;">60.8</td><td style="text-align: center;">75.0</td><td style="text-align: center;">72.0</td><td style="text-align: center;">76.4</td><td style="text-align: center;">81.0</td><td style="text-align: center;">87.7</td></tr><tr><td style="border-right: 1px solid black;">CenterSnap-R [14]</td><td style="text-align: center; border-right: 1px solid black;">✓</td><td style="text-align: center;">-</td><td style="text-align: center;">-</td><td style="text-align: center;">29.1</td><td style="text-align: center;">-</td><td style="text-align: center; border-right: 1px solid black;">64.3</td><td style="text-align: center;">-</td><td style="text-align: center;">-</td><td style="text-align: center;">66.2</td><td style="text-align: center;">-</td><td style="text-align: center;">81.3</td></tr><tr><td style="border-right: 1px solid black;">ACR-Pose [10]</td><td style="text-align: center; border-right: 1px solid black;">✓</td><td style="text-align: center;">-</td><td style="text-align: center;">31.6</td><td style="text-align: center;">36.9</td><td style="text-align: center;">54.8</td><td style="text-align: center; border-right: 1px solid black;">65.9</td><td style="text-align: center;">-</td><td style="text-align: center;">70.4</td><td style="text-align: center;">74.1</td><td style="text-align: center;">82.6</td><td style="text-align: center;">87.8</td></tr><tr><td style="border-right: 1px solid black;">SAR-Net [17]</td><td style="text-align: center; border-right: 1px solid black;">✓</td><td style="text-align: center;">-</td><td style="text-align: center;">31.6</td><td style="text-align: center;">42.3</td><td style="text-align: center;">50.3</td><td style="text-align: center; border-right: 1px solid black;">68.3</td><td style="text-align: center;">-</td><td style="text-align: center;">66.7</td><td style="text-align: center;">70.9</td><td style="text-align: center;">75.3</td><td style="text-align: center;">80.3</td></tr><tr><td style="border-right: 1px solid black;">SSP-Pose [37]</td><td style="text-align: center; border-right: 1px solid black;">✓</td><td style="text-align: center;">-</td><td style="text-align: center;">34.7</td><td style="text-align: center;">44.6</td><td style="text-align: center;">-</td><td style="text-align: center; border-right: 1px solid black;">77.8</td><td style="text-align: center;">-</td><td style="text-align: center;">64.7</td><td style="text-align: center;">75.5</td><td style="text-align: center;">-</td><td style="text-align: center;">87.4</td></tr><tr><td style="border-right: 1px solid black;">SGPA [4]</td><td style="text-align: center; border-right: 1px solid black;">✓</td><td style="text-align: center;">37.1</td><td style="text-align: center;">35.9</td><td style="text-align: center;">39.6</td><td style="text-align: center;">61.3</td><td style="text-align: center; border-right: 1px solid black;">70.7</td><td style="text-align: center;">69.1</td><td style="text-align: center;">70.7</td><td style="text-align: center;">74.5</td><td style="text-align: center;"><u>82.7</u></td><td style="text-align: center;">88.4</td></tr><tr><td style="border-right: 1px solid black;">RBP-Pose [36]</td><td style="text-align: center; border-right: 1px solid black;">✓</td><td style="text-align: center;">-</td><td style="text-align: center;">38.2</td><td style="text-align: center;">48.1</td><td style="text-align: center;">63.1</td><td style="text-align: center; border-right: 1px solid black;"><u>79.2</u></td><td style="text-align: center;">-</td><td style="text-align: center;">73.5</td><td style="text-align: center;">79.6</td><td style="text-align: center;">82.1</td><td style="text-align: center;"><strong>89.5</strong></td></tr><tr><td style="border-right: 1px solid black;">SPD + CATRE [23]</td><td style="text-align: center; border-right: 1px solid black;">✓</td><td style="text-align: center;"><u>43.6</u></td><td style="text-align: center;">45.8</td><td style="text-align: center;"><u>54.4</u></td><td style="text-align: center;">61.4</td><td style="text-align: center; border-right: 1px solid black;">73.1</td><td style="text-align: center;"><u>76.1</u></td><td style="text-align: center;"><strong>75.4</strong></td><td style="text-align: center;"><u>80.3</u></td><td style="text-align: center;"><strong>83.3</strong></td><td style="text-align: center;"><u>89.3</u></td></tr><tr><td style="border-bottom: 2px solid black; border-right: 1px solid black;">DPDN [19]</td><td style="text-align: center; border-bottom: 2px solid black; border-right: 1px solid black;">✓</td><td style="text-align: center; border-bottom: 2px solid black;">-</td><td style="text-align: center; border-bottom: 2px solid black;"><u>46.0</u></td><td style="text-align: center; border-bottom: 2px solid black;">50.7</td><td style="text-align: center; border-bottom: 2px solid black;"><u>70.4</u></td><td style="text-align: center; border-bottom: 2px solid black; border-right: 1px solid black;">78.4</td><td style="text-align: center; border-bottom: 2px solid black;">-</td><td style="text-align: center; border-bottom: 2px solid black;">-</td><td style="text-align: center; border-bottom: 2px solid black;">-</td><td style="text-align: center; border-bottom: 2px solid black;">-</td><td style="text-align: center; border-bottom: 2px solid black;">-</td></tr></tbody></table><figure><imgsrc="https://img.032802.xyz/paper-reading/2023/vi-net-boosting-category-level-6d-object-pose-estimation-via-learning-decoupled-rotations-on-the-spherical-representations_2023_Lin/vis_sota.webp"alt="Figure 4. Qualitative comparisons between the state-of-the-art method of DPDN [19] and our proposed one on REAL275 dataset [31]." /><figcaption aria-hidden="true">Figure 4. Qualitative comparisons betweenthe state-of-the-art method of DPDN [19] and our proposed one on REAL275dataset [31].</figcaption></figure><figure><imgsrc="https://img.032802.xyz/paper-reading/2023/vi-net-boosting-category-level-6d-object-pose-estimation-via-learning-decoupled-rotations-on-the-spherical-representations_2023_Lin/mAP.webp"alt="Figure 5. Plottings of per-category average precision versus different rotation/translation error thresholds for our proposed method on REAL275 dataset [31]." /><figcaption aria-hidden="true">Figure 5. Plottings of per-categoryaverage precision versus different rotation/translation error thresholdsfor our proposed method on REAL275 dataset [31].</figcaption></figure><h3 id="ablation-studies-and-analyses">6.3. Ablation Studies andAnalyses</h3><table><caption style="text-align: left; caption-side: bottom;">Table 2. Ablation studies of the variants of our VI-Net with or withoutrotation decomposition (R.D.) on REAL275 dataset [31].</caption><thead><tr><th style="border-top: 2px solid black; border-bottom: 1px solid black; border-right: 1px solid black;"></th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black; border-right: 1px solid black;">R.D.</th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">5°2cm</th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">5°5cm</th></tr></thead><tbody><tr><td style="border-right: 1px solid black;">Baseline1: Avg Pool + MLP</td><td style="text-align: center; border-right: 1px solid black;">✕</td><td style="text-align: center;">45.0</td><td style="text-align: center;">51.8</td></tr><tr><td style="border-right: 1px solid black;">Baseline2: Flattening + MLP</td><td style="text-align: center; border-right: 1px solid black;">✕</td><td style="text-align: center;">42.7</td><td style="text-align: center;">48.8</td></tr><tr><td style="border-bottom: 2px solid black; border-right: 1px solid black;">VI-Net</td><td style="text-align: center; border-bottom: 2px solid black; border-right: 1px solid black;">✓</td><td style="text-align: center; border-bottom: 2px solid black;"><strong>50.0</strong></td><td style="text-align: center; border-bottom: 2px solid black;"><strong>57.6</strong></td></tr></tbody></table><table><caption style="text-align: left; caption-side: bottom;">Table 3. Ablation studies of the variants of the V-Branch and I-Branchin our VI-Net on REAL275 dataset [31].</caption><thead><tr><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black; border-right: 1px solid black;">Variants of V-Branch</th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black; border-right: 1px solid black;">Feature Trans. in I-Branch</th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">5°2cm</th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">5°5cm</th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">10°2cm</th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">10°5cm</th></tr></thead><tbody><tr><td style="text-align: center; border-right: 1px solid black;">Direct Regression</td><td style="text-align: center; border-right: 1px solid black;">✕</td><td style="text-align: center;">26.3</td><td style="text-align: center;">28.9</td><td style="text-align: center;">54.1</td><td style="text-align: center;">61.1</td></tr><tr><td style="text-align: center; border-right: 1px solid black;">Binary Classification (1-branch)</td><td style="text-align: center; border-right: 1px solid black;">✕</td><td style="text-align: center;">30.8</td><td style="text-align: center;">34.4</td><td style="text-align: center;">65.5</td><td style="text-align: center;">75.3</td></tr><tr><td style="text-align: center; border-bottom: 1px solid black; border-right: 1px solid black;">Binary Classification (2-branch)</td><td style="text-align: center; border-bottom: 1px solid black; border-right: 1px solid black;">✕</td><td style="text-align: center; border-bottom: 1px solid black;">33.7</td><td style="text-align: center; border-bottom: 1px solid black;">37.9</td><td style="text-align: center; border-bottom: 1px solid black;">65.4</td><td style="text-align: center; border-bottom: 1px solid black;">75.4</td></tr><tr><td style="text-align: center; border-right: 1px solid black;">Direct Regression</td><td style="text-align: center; border-right: 1px solid black;">✓</td><td style="text-align: center;">43.1</td><td style="text-align: center;">48.3</td><td style="text-align: center;">68.0</td><td style="text-align: center;">78.1</td></tr><tr><td style="text-align: center; border-right: 1px solid black;">Binary Classification (1-branch)</td><td style="text-align: center; border-right: 1px solid black;">✓</td><td style="text-align: center;">49.3</td><td style="text-align: center;">56.9</td><td style="text-align: center;">69.4</td><td style="text-align: center;">79.7</td></tr><tr><td style="text-align: center; border-bottom: 2px solid black; border-right: 1px solid black;">Binary Classification (2-branch)</td><td style="text-align: center; border-bottom: 2px solid black; border-right: 1px solid black;">✓</td><td style="text-align: center; border-bottom: 2px solid black;"><strong>50.0</strong></td><td style="text-align: center; border-bottom: 2px solid black;"><strong>57.6</strong></td><td style="text-align: center; border-bottom: 2px solid black;"><strong>70.8</strong></td><td style="text-align: center; border-bottom: 2px solid black;"><strong>82.1</strong></td></tr></tbody></table><table><caption style="text-align: left; caption-side: bottom;">Table 4. Quantitative comparisons of SPE-SConv [9] and variants of ourproposed SPA-SConv on REAL275 dataset [31].</caption><thead><tr><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black; border-right: 1px solid black;">Type of<br>Convolution</th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black; border-right: 1px solid black;">Padding</th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black; border-right: 1px solid black;">Symmetric<br>Operation</th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">5°2cm</th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">5°5cm</th></tr></thead><tbody><tr><td style="text-align: center; border-bottom: 1px solid black; border-right: 1px solid black;">SPE-SConv [9]</td><td style="text-align: center; border-bottom: 1px solid black; border-right: 1px solid black;">-</td><td style="text-align: center; border-bottom: 1px solid black; border-right: 1px solid black;">-</td><td style="text-align: center; border-bottom: 1px solid black;">35.1</td><td style="text-align: center; border-bottom: 1px solid black;">40.8</td></tr><tr><td rowspan="3" style="text-align: center; border-bottom: 2px solid black; border-right: 1px solid black;">SPA-SConv</td><td style="text-align: center; border-right: 1px solid black;">✕</td><td style="text-align: center; border-right: 1px solid black;">✕</td><td style="text-align: center;">46.9</td><td style="text-align: center;">53.2</td></tr><tr><td style="text-align: center; border-right: 1px solid black;">✕</td><td style="text-align: center; border-right: 1px solid black;">✓</td><td style="text-align: center;">48.5</td><td style="text-align: center;">55.5</td></tr><tr><td style="text-align: center; border-bottom: 2px solid black; border-right: 1px solid black;">✓</td><td style="text-align: center; border-bottom: 2px solid black; border-right: 1px solid black;">✓</td><td style="text-align: center; border-bottom: 2px solid black;"><strong>50.0</strong></td><td style="text-align: center; border-bottom: 2px solid black;"><strong>57.6</strong></td></tr></tbody></table><table><caption style="text-align: left; caption-side: bottom;">Table 5. Quantitative comparisons between VI-Net+ts and PN2 fortranslation and size estimation on REAL275 dataset [31].</caption><thead><tr><th style="border-top: 2px solid black; border-bottom: 1px solid black; border-right: 1px solid black;"></th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">5°2cm</th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">5°5cm</th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">10°2cm</th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">10°5cm</th></tr></thead><tbody><tr><td style="border-right: 1px solid black;">VI-Net<sub>+ts</sub></td><td style="text-align: center;">45.0</td><td style="text-align: center;">56.0</td><td style="text-align: center;">65.9</td><td style="text-align: center;">80.5</td></tr><tr><td style="border-bottom: 2px solid black; border-right: 1px solid black;">PN2 + VI-Net</td><td style="text-align: center; border-bottom: 2px solid black;"><strong>50.0</strong></td><td style="text-align: center; border-bottom: 2px solid black;"><strong>57.6</strong></td><td style="text-align: center; border-bottom: 2px solid black;"><strong>70.8</strong></td><td style="text-align: center; border-bottom: 2px solid black;"><strong>82.1</strong></td></tr></tbody></table><table><caption style="text-align: left; caption-side: bottom;">Table 6. Quantitative comparisons of different input data types ofVI-Net on REAL275 dataset [31].</caption><thead><tr><th style="border-top: 2px solid black; border-bottom: 1px solid black; border-right: 1px solid black;">Data Type</th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">5°2cm</th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">5°5cm</th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">10°2cm</th><th style="text-align: center; border-top: 2px solid black; border-bottom: 1px solid black;">10°5cm</th></tr></thead><tbody><tr><td style="text-align: center; border-right: 1px solid black;">Depth</td><td style="text-align: center;">43.0</td><td style="text-align: center;">52.1</td><td style="text-align: center;">64.3</td><td style="text-align: center;">77.0</td></tr><tr><td style="text-align: center; border-bottom: 2px solid black; border-right: 1px solid black;">RGB + Depth</td><td style="text-align: center; border-bottom: 2px solid black;"><strong>50.0</strong></td><td style="text-align: center; border-bottom: 2px solid black;"><strong>57.6</strong></td><td style="text-align: center; border-bottom: 2px solid black;"><strong>70.8</strong></td><td style="text-align: center; border-bottom: 2px solid black;"><strong>82.1</strong></td></tr></tbody></table><figure><imgsrc="https://img.032802.xyz/paper-reading/2023/vi-net-boosting-category-level-6d-object-pose-estimation-via-learning-decoupled-rotations-on-the-spherical-representations_2023_Lin/vis_viewpoint.webp"alt="Figure 6. Qualitative results of the predicted viewpoint rotation \mathbf{R}_{vp} from V-Branch and the final output rotation \mathbf{R} from VI-Net." /><figcaption aria-hidden="true">Figure 6. Qualitative results of thepredicted viewpoint rotation <spanclass="math inline">\(\mathbf{R}_{vp}\)</span> from V-Branch and thefinal output rotation <span class="math inline">\(\mathbf{R}\)</span>from VI-Net.</figcaption></figure><h2 id="conclusion">7. Conclusion</h2><div class="pdf-container" data-target="https://arxiv.org/pdf/2308.09916" data-height="500px"></div>]]></content>
    
    
    <summary type="html">Rotation estimation of high precision from an RGB-D object observation is a huge challenge in 6D object pose estimation, due to the difficulty of learning in the non-linear space of SO(3). In this paper, we propose a novel rotation estimation network, termed as VI-Net, to make the task easier by decoupling the rotation as the combination of a viewpoint rotation and an in-plane rotation. More specifically, VI-Net bases the feature learning on the sphere with two individual branches for the estimates of two factorized rotations, where a V-Branch is employed to learn the viewpoint rotation via binary classification on the spherical signals, while another I-Branch is used to estimate the in-plane rotation by transforming the signals to view from the zenith direction. To process the spherical signals, a Spherical Feature Pyramid Network is constructed based on a novel design of SPAtial Spherical Convolution (SPA-SConv), which settles the boundary problem of spherical signals via feature padding and realizes viewpoint-equivariant feature extraction by symmetric convolutional operations. We apply the proposed VI-Net to the challenging task of categorylevel 6D object pose estimation for predicting the poses of unknown objects without available CAD models; experiments on the benchmarking datasets confirm the efficacy of our method, which outperforms the existing ones with a large margin in the regime of high precision.</summary>
    
    
    
    <category term="读万卷书" scheme="https://blog.032802.xyz/categories/%E8%AF%BB%E4%B8%87%E5%8D%B7%E4%B9%A6/"/>
    
    
    <category term="Object Pose Estimation" scheme="https://blog.032802.xyz/tags/Object-Pose-Estimation/"/>
    
    <category term="2023ICCV" scheme="https://blog.032802.xyz/tags/2023ICCV/"/>
    
  </entry>
  
  <entry>
    <title>2025 CVPR Object Pose Estimation论文列表</title>
    <link href="https://blog.032802.xyz/paper-list/2025cvpr-ope.html"/>
    <id>https://blog.032802.xyz/paper-list/2025cvpr-ope.html</id>
    <published>2025-03-30T13:58:37.000Z</published>
    <updated>2025-07-09T09:51:37.000Z</updated>
    
    <content type="html"><![CDATA[<table><colgroup><col style="width: 2%" /><col style="width: 32%" /><col style="width: 32%" /><col style="width: 32%" /></colgroup><thead><tr><th>序号</th><th>标题</th><th>论文链接</th><th>代码是否开源</th></tr></thead><tbody><tr><td>1</td><td>Any6D: Model-free 6D Pose Estimation of Novel Objects</td><td><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzI1MDMuMTg2NzM=">https://arxiv.org/abs/2503.18673<i class="fa fa-external-link-alt"></i></span></td><td><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RhZXllb3BsL0FueTZE">taeyeopl/Any6D<i class="fa fa-external-link-alt"></i></span></td></tr><tr><td>2</td><td>CAP-Net: A Unified Network for 6D Pose and Size Estimation ofCategorical Articulated Parts from a Single RGB-D Image</td><td><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzI1MDQuMTEyMzA=">https://arxiv.org/abs/2504.11230<i class="fa fa-external-link-alt"></i></span></td><td><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1NoYW5lSHVhbmdIWi9DQVBOZXQ=">ShaneHuangHZ/CAPNet<i class="fa fa-external-link-alt"></i></span></td></tr><tr><td>3</td><td>Co-op: Correspondence-based Novel Object Pose Estimation</td><td><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzI1MDMuMTc3MzE=">https://arxiv.org/abs/2503.17731<i class="fa fa-external-link-alt"></i></span></td><td></td></tr><tr><td>4</td><td>CRISP: Object Pose and Shape Estimation with Test-TimeAdaptation</td><td><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzI0MTIuMDEwNTI=">https://arxiv.org/abs/2412.01052<i class="fa fa-external-link-alt"></i></span></td><td><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL01JVC1TUEFSSy9DUklTUA==">MIT-SPARK/CRISP<i class="fa fa-external-link-alt"></i></span></td></tr><tr><td>5</td><td>GaPT-DAR: Category-level Garments Pose Tracking via Integrated 2DDeformation and 3D Reconstruction</td><td><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudC9DVlBSMjAyNS9wYXBlcnMvWmhhbmdfR2FQVC1EQVJfQ2F0ZWdvcnktbGV2ZWxfR2FybWVudHNfUG9zZV9UcmFja2luZ192aWFfSW50ZWdyYXRlZF8yRF9EZWZvcm1hdGlvbl9hbmRfQ1ZQUl8yMDI1X3BhcGVyLnBkZg==">https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_GaPT-DAR_Category-level_Garments_Pose_Tracking_via_Integrated_2D_Deformation_and_CVPR_2025_paper.pdf<i class="fa fa-external-link-alt"></i></span></td><td><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3phbmx5MjAvR2FQVC1EQVI=">zanly20/GaPT-DAR<i class="fa fa-external-link-alt"></i></span></td></tr><tr><td>6</td><td>GCE-Pose: Global Context Enhancement for Category-level Object PoseEstimation</td><td><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzI1MDIuMDQyOTM=">https://arxiv.org/abs/2502.04293<i class="fa fa-external-link-alt"></i></span></td><td></td></tr><tr><td>7</td><td>GIVEPose: Gradual Intra-class Variation Elimination for RGB-basedCategory-Level Object Pose Estimation</td><td><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzI1MDMuMTUxMTA=">https://arxiv.org/abs/2503.15110<i class="fa fa-external-link-alt"></i></span></td><td><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3ppcWluLWgvR0lWRVBvc2U=">ziqin-h/GIVEPose<i class="fa fa-external-link-alt"></i></span></td></tr><tr><td>8</td><td>iG-6DoF: Model-free 6DoF Pose Estimation for Unseen Object viaIterative 3D Gaussian Splatting</td><td><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudC9DVlBSMjAyNS9wYXBlcnMvQ2FvX2lHLTZEb0ZfTW9kZWwtZnJlZV82RG9GX1Bvc2VfRXN0aW1hdGlvbl9mb3JfVW5zZWVuX09iamVjdF92aWFfSXRlcmF0aXZlX0NWUFJfMjAyNV9wYXBlci5wZGY=">https://openaccess.thecvf.com/content/CVPR2025/papers/Cao_iG-6DoF_Model-free_6DoF_Pose_Estimation_for_Unseen_Object_via_Iterative_CVPR_2025_paper.pdf<i class="fa fa-external-link-alt"></i></span></td><td></td></tr><tr><td>9</td><td>Leveraging Global Stereo Consistency for Category-Level Shape and 6DPose Estimation from Stereo Images</td><td><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudC9DVlBSMjAyNS9wYXBlcnMvUWl1X0xldmVyYWdpbmdfR2xvYmFsX1N0ZXJlb19Db25zaXN0ZW5jeV9mb3JfQ2F0ZWdvcnktTGV2ZWxfU2hhcGVfYW5kXzZEX1Bvc2VfQ1ZQUl8yMDI1X3BhcGVyLnBkZg==">https://openaccess.thecvf.com/content/CVPR2025/papers/Qiu_Leveraging_Global_Stereo_Consistency_for_Category-Level_Shape_and_6D_Pose_CVPR_2025_paper.pdf<i class="fa fa-external-link-alt"></i></span></td><td></td></tr><tr><td>10</td><td>ONDA-Pose: Occlusion-Aware Neural Domain Adaptation forSelf-Supervised 6D Object Pose Estimation</td><td><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudC9DVlBSMjAyNS9wYXBlcnMvVGFuX09OREEtUG9zZV9PY2NsdXNpb24tQXdhcmVfTmV1cmFsX0RvbWFpbl9BZGFwdGF0aW9uX2Zvcl9TZWxmLVN1cGVydmlzZWRfNkRfT2JqZWN0X1Bvc2VfQ1ZQUl8yMDI1X3BhcGVyLnBkZg==">https://openaccess.thecvf.com/content/CVPR2025/papers/Tan_ONDA-Pose_Occlusion-Aware_Neural_Domain_Adaptation_for_Self-Supervised_6D_Object_Pose_CVPR_2025_paper.pdf<i class="fa fa-external-link-alt"></i></span></td><td><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3Rhbi10YW8xMS9PTkRBLVBvc2U=">tan-tao11/ONDA-Pose<i class="fa fa-external-link-alt"></i></span></td></tr><tr><td>11</td><td>One2Any: One-Reference 6D Pose Estimation for Any Object</td><td><span class="exturl" data-url="aHR0cHM6Ly93d3cuYXJ4aXYub3JnL2Ficy8yNTA1LjA0MTA5">https://www.arxiv.org/abs/2505.04109<i class="fa fa-external-link-alt"></i></span></td><td>2025.07.09空仓库：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xteTEwMDEvT25lMkFueQ==">lmy1001/One2Any<i class="fa fa-external-link-alt"></i></span></td></tr><tr><td>12</td><td>Pos3R: 6D Pose Estimation for Unseen Objects Made Easy</td><td><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudC9DVlBSMjAyNS9wYXBlcnMvRGVuZ19Qb3MzUl82RF9Qb3NlX0VzdGltYXRpb25fZm9yX1Vuc2Vlbl9PYmplY3RzX01hZGVfRWFzeV9DVlBSXzIwMjVfcGFwZXIucGRm">https://openaccess.thecvf.com/content/CVPR2025/papers/Deng_Pos3R_6D_Pose_Estimation_for_Unseen_Objects_Made_Easy_CVPR_2025_paper.pdf<i class="fa fa-external-link-alt"></i></span></td><td></td></tr><tr><td>13</td><td>RefPose: Leveraging Reference Geometric Correspondences for Accurate6D Pose Estimation of Unseen Objects</td><td><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzI1MDUuMTA4NDE=">https://arxiv.org/abs/2505.10841<i class="fa fa-external-link-alt"></i></span></td><td></td></tr><tr><td>14</td><td>Rethinking Correspondence-based Category-Level Object PoseEstimation</td><td><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudC9DVlBSMjAyNS9wYXBlcnMvUmVuX1JldGhpbmtpbmdfQ29ycmVzcG9uZGVuY2UtYmFzZWRfQ2F0ZWdvcnktTGV2ZWxfT2JqZWN0X1Bvc2VfRXN0aW1hdGlvbl9DVlBSXzIwMjVfcGFwZXIucGRm">https://openaccess.thecvf.com/content/CVPR2025/papers/Ren_Rethinking_Correspondence-based_Category-Level_Object_Pose_Estimation_CVPR_2025_paper.pdf<i class="fa fa-external-link-alt"></i></span></td><td>2025.07.09空仓库：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1Jlbkh1YW4xOTk5L1Nwb3RQb3Nl">RenHuan1999/SpotPose<i class="fa fa-external-link-alt"></i></span></td></tr><tr><td>15</td><td>SCFlow2: Plug-and-Play Object Pose Refiner with Shape-ConstraintScene Flow</td><td><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzI1MDQuMDkxNjA=">https://arxiv.org/abs/2504.09160<i class="fa fa-external-link-alt"></i></span></td><td>2025.07.09空仓库：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1ctUVkvU0NGbG93Mg==">W-QY/SCFlow2<i class="fa fa-external-link-alt"></i></span></td></tr><tr><td>16</td><td>Structure-Aware Correspondence Learning for Relative PoseEstimation</td><td><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzI1MDMuMTg2NzE=">https://arxiv.org/abs/2503.18671<i class="fa fa-external-link-alt"></i></span></td><td></td></tr><tr><td>17</td><td>UA-Pose: Uncertainty-Aware 6D Object Pose Estimation and OnlineObject Completion with Partial References</td><td><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzI1MDYuMDc5OTY=">https://arxiv.org/abs/2506.07996<i class="fa fa-external-link-alt"></i></span></td><td>2025.07.09空仓库：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21pbmZlbmxpL1VBLVBvc2U=">minfenli/UA-Pose<i class="fa fa-external-link-alt"></i></span></td></tr><tr><td>18</td><td>UNOPose: Unseen Object Pose Estimation with an Unposed RGB-DReference Image</td><td><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzI0MTEuMTYxMDY=">https://arxiv.org/abs/2411.16106<i class="fa fa-external-link-alt"></i></span></td><td><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NoYW5pY2UtbC9VTk9Qb3Nl">shanice-l/UNOPose<i class="fa fa-external-link-alt"></i></span></td></tr></tbody></table><p>以上信息截止至2025年7月9日17:49。</p>]]></content>
    
    
    <summary type="html">2025 CVPR已发表论文中，和Object Pose Estimation相关的论文列表</summary>
    
    
    
    <category term="论文列表" scheme="https://blog.032802.xyz/categories/%E8%AE%BA%E6%96%87%E5%88%97%E8%A1%A8/"/>
    
    
    <category term="Object Pose Estimation" scheme="https://blog.032802.xyz/tags/Object-Pose-Estimation/"/>
    
    <category term="CVPR" scheme="https://blog.032802.xyz/tags/CVPR/"/>
    
    <category term="论文列表" scheme="https://blog.032802.xyz/tags/%E8%AE%BA%E6%96%87%E5%88%97%E8%A1%A8/"/>
    
  </entry>
  
  <entry>
    <title>【VPS测评】ZgoCloud - Los Angeles Global VPS - Specials - Starter</title>
    <link href="https://blog.032802.xyz/vps-review/zgocloud-los-angeles-global-vps-specials-starter.html"/>
    <id>https://blog.032802.xyz/vps-review/zgocloud-los-angeles-global-vps-specials-starter.html</id>
    <published>2025-03-30T05:21:17.000Z</published>
    <updated>2025-03-30T05:21:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>配置信息：</p><ul><li>1 Core AMD EPYC 7002 Series</li><li>1 GB DDR4 RAM</li><li>20G NVMe SSD</li><li>1 IPV4</li><li>2T/Month/1Gbps</li><li>International network, not optimized for China</li></ul><p>购买链接：<span class="exturl" data-url="aHR0cHM6Ly9jbGllbnRzLnpnb3Zwcy5jb20vaW5kZXgucGhwPy9jYXJ0LyZzdGVwPTM=">ZgoCloud -Los Angeles Global VPS - Specials - Starter<i class="fa fa-external-link-alt"></i></span></p><h2 id="测试">2025-03-30测试</h2><h3 id="itdog">ITDOG</h3><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuaXRkb2cuY24vcGluZy8=">https://www.itdog.cn/ping/<i class="fa fa-external-link-alt"></i></span>：</p><p>结果图：<span class="exturl" data-url="aHR0cHM6Ly9pbWcuMDMyODAyLnh5ei92cHMtcmV2aWV3L3pnb2Nsb3VkLWxvcy1hbmdlbGVzLWdsb2JhbC12cHMtc3BlY2lhbHMtc3RhcnRlci8yMDI1LjAzLjMwLTIzLjE1OS53ZWJw">ITDOG：23.159.*.*<i class="fa fa-external-link-alt"></i></span></p><h3 id="ipquality">IPQuality</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3h5a3QvSVBRdWFsaXR5">xykt/IPQuality<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash &lt;(curl -Ls IP.Check.Place)</span><br></pre></td></tr></table></figure><figure><img src="https://report.check.place/IP/188ZI9L6A.svg"alt="IP质量体检报告：23.159.*.*" /><figcaption aria-hidden="true">IP质量体检报告：23.159.*.*</figcaption></figure><h3 id="融合怪">融合怪</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NwaXJpdExITFMvZWNz">spiritLHLS/ecs<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -L https://gitlab.com/spiritysdx/za/-/raw/main/ecs.sh -o ecs.sh &amp;&amp; <span class="built_in">chmod</span> +x ecs.sh &amp;&amp; bash ecs.sh -m 1</span><br></pre></td></tr></table></figure><p><span class="exturl" data-url="aHR0cHM6Ly9wYXN0ZS5zcGlyaXRsaGwubmV0LyMvc2hvdy9Nc2dSWS50eHQ=">https://paste.spiritlhl.net/#/show/MsgRY.txt<i class="fa fa-external-link-alt"></i></span></p>]]></content>
    
    
    <summary type="html">本次测评的服务器是ZgoCloud的Los Angeles Global VPS - Specials - Starter，年付&#92;$15.00USD。</summary>
    
    
    
    <category term="VPS测评" scheme="https://blog.032802.xyz/categories/VPS%E6%B5%8B%E8%AF%84/"/>
    
    
    <category term="ZgoCloud" scheme="https://blog.032802.xyz/tags/ZgoCloud/"/>
    
    <category term="VPS" scheme="https://blog.032802.xyz/tags/VPS/"/>
    
    <category term="VPS测评" scheme="https://blog.032802.xyz/tags/VPS%E6%B5%8B%E8%AF%84/"/>
    
    <category term="美国VPS" scheme="https://blog.032802.xyz/tags/%E7%BE%8E%E5%9B%BDVPS/"/>
    
  </entry>
  
  <entry>
    <title>【VPS测评】Sakura Clouds - HKG1 VPS - 1C512M SP KVM VPS</title>
    <link href="https://blog.032802.xyz/vps-review/sakuraclouds-hkg1-vps-1c512m-sp-kvm-vps.html"/>
    <id>https://blog.032802.xyz/vps-review/sakuraclouds-hkg1-vps-1c512m-sp-kvm-vps.html</id>
    <published>2025-03-29T13:57:11.000Z</published>
    <updated>2025-03-29T13:57:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>配置信息：</p><ul><li>1 Core(s)</li><li>512 MB RAM</li><li>5 GB SSD Storage</li><li>1000 GB Bandwidth (in+out)</li><li>1 Gbps Global Port Shared</li><li>1 x IPv4 + 1 x /64 IPv6</li><li>Virtualizor KVM</li><li>Linux Only</li></ul><p>购买链接：已停售。</p><h2 id="测试">2025-03-29测试</h2><h3 id="itdog">ITDOG</h3><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuaXRkb2cuY24vcGluZy8=">https://www.itdog.cn/ping/<i class="fa fa-external-link-alt"></i></span>：</p><p>结果图：<span class="exturl" data-url="aHR0cHM6Ly9pbWcuMDMyODAyLnh5ei92cHMtcmV2aWV3L3Nha3VyYWNsb3Vkcy1oa2cxLXZwcy0xYzUxMm0tc3Ata3ZtLXZwcy8yMDI1LjAzLjI5LTk1LjEzNS53ZWJw">ITDOG：95.135.*.*<i class="fa fa-external-link-alt"></i></span></p><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuaXRkb2cuY24vcGluZ19pcHY2Lw==">https://www.itdog.cn/ping_ipv6/<i class="fa fa-external-link-alt"></i></span>：</p><p>结果图：<span class="exturl" data-url="aHR0cHM6Ly9pbWcuMDMyODAyLnh5ei92cHMtcmV2aWV3L3Nha3VyYWNsb3Vkcy1oa2cxLXZwcy0xYzUxMm0tc3Ata3ZtLXZwcy8yMDI1LjAzLjI5LTJhMTRfNjdjMV9iMmFhLndlYnA=">ITDOG：2a14:67c1:b2aa:*:*:*:*:*<i class="fa fa-external-link-alt"></i></span></p><h3 id="ipquality">IPQuality</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3h5a3QvSVBRdWFsaXR5">xykt/IPQuality<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash &lt;(curl -Ls IP.Check.Place)</span><br></pre></td></tr></table></figure><figure><img src="https://report.check.place/IP/18ZJ7F8ET.svg"alt="IP质量体检报告：95.135.*.*" /><figcaption aria-hidden="true">IP质量体检报告：95.135.*.*</figcaption></figure><figure><img src="https://report.check.place/IP/2IG65O8J8.svg"alt="IP质量体检报告：2a14:67c1:b2aa:*:*:*:*:*" /><figcaptionaria-hidden="true">IP质量体检报告：2a14:67c1:b2aa:*:*:*:*:*</figcaption></figure><h3 id="融合怪">融合怪</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NwaXJpdExITFMvZWNz">spiritLHLS/ecs<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -L https://gitlab.com/spiritysdx/za/-/raw/main/ecs.sh -o ecs.sh &amp;&amp; <span class="built_in">chmod</span> +x ecs.sh &amp;&amp; bash ecs.sh -m 1</span><br></pre></td></tr></table></figure><p><span class="exturl" data-url="aHR0cHM6Ly9wYXN0ZS5zcGlyaXRsaGwubmV0LyMvc2hvdy9WR3VCeS50eHQ=">https://paste.spiritlhl.net/#/show/VGuBy.txt<i class="fa fa-external-link-alt"></i></span></p>]]></content>
    
    
    <summary type="html">本次测评的服务器是Sakura Clouds的HKG1 VPS-1C512M SP KVM VPS，月付&#92;$1.10USD。</summary>
    
    
    
    <category term="VPS测评" scheme="https://blog.032802.xyz/categories/VPS%E6%B5%8B%E8%AF%84/"/>
    
    
    <category term="VPS" scheme="https://blog.032802.xyz/tags/VPS/"/>
    
    <category term="VPS测评" scheme="https://blog.032802.xyz/tags/VPS%E6%B5%8B%E8%AF%84/"/>
    
    <category term="香港VPS" scheme="https://blog.032802.xyz/tags/%E9%A6%99%E6%B8%AFVPS/"/>
    
    <category term="Sakura Clouds" scheme="https://blog.032802.xyz/tags/Sakura-Clouds/"/>
    
  </entry>
  
  <entry>
    <title>【VPS测评】乐子云 - A1.unlimited-和小埋特殊交易的产物</title>
    <link href="https://blog.032802.xyz/vps-review/leziyun-a1-unlimited.html"/>
    <id>https://blog.032802.xyz/vps-review/leziyun-a1-unlimited.html</id>
    <published>2025-03-28T10:31:13.000Z</published>
    <updated>2025-03-28T10:31:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>配置信息：</p><ul><li>位置：堪萨斯</li><li>架构：KVM</li><li>CPU：1核心</li><li>RAM：300M</li><li>硬盘：20G</li><li>流量：G口无限制</li><li>IP：IPv6 1个</li><li>快照：1</li><li>母鸡：G口</li></ul><h2 id="测试">2025-03-28测试</h2><h3 id="itdog">ITDOG</h3><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuaXRkb2cuY24vcGluZ19pcHY2Lw==">https://www.itdog.cn/ping_ipv6/<i class="fa fa-external-link-alt"></i></span>：</p><p>结果图：<span class="exturl" data-url="aHR0cHM6Ly9pbWcuMDMyODAyLnh5ei92cHMtcmV2aWV3L2xleml5dW4tYTEtdW5saW1pdGVkLzIwMjUuMDMuMjgtMjYwNF80MzAwX2Eud2VicA==">ITDOG：2604:4300:a:*:*:*:*:*<i class="fa fa-external-link-alt"></i></span></p><h3 id="ipquality">IPQuality</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3h5a3QvSVBRdWFsaXR5">xykt/IPQuality<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash &lt;(curl -Ls IP.Check.Place)</span><br></pre></td></tr></table></figure><figure><img src="https://report.check.place/IP/246NZX3AX.svg"alt="IP质量体检报告：104.28.*.*" /><figcaption aria-hidden="true">IP质量体检报告：104.28.*.*</figcaption></figure><figure><img src="https://report.check.place/IP/3NT5NYGTL.svg"alt="IP质量体检报告：2604:4300:a:*:*:*:*:*" /><figcaptionaria-hidden="true">IP质量体检报告：2604:4300:a:*:*:*:*:*</figcaption></figure><h3 id="融合怪">融合怪</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NwaXJpdExITFMvZWNz">spiritLHLS/ecs<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -L https://gitlab.com/spiritysdx/za/-/raw/main/ecs.sh -o ecs.sh &amp;&amp; <span class="built_in">chmod</span> +x ecs.sh &amp;&amp; bash ecs.sh -m 1</span><br></pre></td></tr></table></figure><p><span class="exturl" data-url="aHR0cHM6Ly9wYXN0ZS5zcGlyaXRsaGwubmV0LyMvc2hvdy9yZWJhZy50eHQ=">https://paste.spiritlhl.net/#/show/rebag.txt<i class="fa fa-external-link-alt"></i></span></p>]]></content>
    
    
    <summary type="html">本次测评的服务器是乐子云的A1.unlimited-和小埋特殊交易的产物，月付¥1.00CNY。</summary>
    
    
    
    <category term="VPS测评" scheme="https://blog.032802.xyz/categories/VPS%E6%B5%8B%E8%AF%84/"/>
    
    
    <category term="VPS" scheme="https://blog.032802.xyz/tags/VPS/"/>
    
    <category term="VPS测评" scheme="https://blog.032802.xyz/tags/VPS%E6%B5%8B%E8%AF%84/"/>
    
    <category term="美国VPS" scheme="https://blog.032802.xyz/tags/%E7%BE%8E%E5%9B%BDVPS/"/>
    
    <category term="乐子云" scheme="https://blog.032802.xyz/tags/%E4%B9%90%E5%AD%90%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>【代码阅读】Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation</title>
    <link href="https://blog.032802.xyz/code-reading/Leeiieeo_AG-Pose.html"/>
    <id>https://blog.032802.xyz/code-reading/Leeiieeo_AG-Pose.html</id>
    <published>2025-03-21T03:00:00.000Z</published>
    <updated>2025-03-21T03:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据处理">数据处理</h2><p>在<code>./provider/create_dataloaders.py</code>中创建Dataloader，分别可以使用camera_real，camera和housecat6d三种创建方式，如果使用camera_real方式创建的话，camera和real的比例为3:1。</p><h3 id="深度图加载">深度图加载</h3><p>由于该方法使用到了CAMERA25和REAL25两个数据集，而CAMERA25数据集是一个合成数据集，其深度图为合成深度图，所以需要进行处理，下面是合成深度图的读取方法（注意这里读取的是<code>./data/camera_full_depths/</code>中的图，和<code>./data/camera/</code>中的不一样）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_composed_depth</span>(<span class="params">img_path</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Load depth image from img_path. &quot;&quot;&quot;</span></span><br><span class="line">    img_path_ = img_path.replace(<span class="string">&#x27;/data/camera/&#x27;</span>, <span class="string">&#x27;/data/camera_full_depths/&#x27;</span>)</span><br><span class="line">    depth_path = img_path_ + <span class="string">&#x27;_composed.png&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(depth_path):</span><br><span class="line">        depth = cv2.imread(depth_path, -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(depth.shape) == <span class="number">3</span>:</span><br><span class="line">            <span class="comment"># This is encoded depth image, let&#x27;s convert</span></span><br><span class="line">            <span class="comment"># <span class="doctag">NOTE:</span> RGB is actually BGR in opencv</span></span><br><span class="line">            depth16 = depth[:, :, <span class="number">1</span>]*<span class="number">256</span> + depth[:, :, <span class="number">2</span>]</span><br><span class="line">            depth16 = np.where(depth16==<span class="number">32001</span>, <span class="number">0</span>, depth16)</span><br><span class="line">            depth16 = depth16.astype(np.uint16)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">len</span>(depth.shape) == <span class="number">2</span> <span class="keyword">and</span> depth.dtype == <span class="string">&#x27;uint16&#x27;</span>:</span><br><span class="line">            depth16 = depth</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">assert</span> <span class="literal">False</span>, <span class="string">&#x27;[ Error ]: Unsupported depth type.&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> depth16</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;warning: No data&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure><p>使用OpenCV读取<code>./data/camera/train/00000/0000_depth.png</code>，分别可视化其三个通道：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&#x27;./data/camera/train/00000/0000&#x27;</span></span><br><span class="line"></span><br><span class="line">depth_path = img_path + <span class="string">&#x27;_depth.png&#x27;</span></span><br><span class="line">depth = cv2.imread(depth_path, -<span class="number">1</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">131</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;depth[:, :, 0]&#x27;</span>)</span><br><span class="line">plt.imshow(depth[:, :, <span class="number">0</span>])</span><br><span class="line">plt.colorbar(shrink=<span class="number">0.5</span>)</span><br><span class="line">plt.subplot(<span class="number">132</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;depth[:, :, 1]&#x27;</span>)</span><br><span class="line">plt.imshow(depth[:, :, <span class="number">1</span>])</span><br><span class="line">plt.colorbar(shrink=<span class="number">0.5</span>)</span><br><span class="line">plt.subplot(<span class="number">133</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;depth[:, :, 2]&#x27;</span>)</span><br><span class="line">plt.imshow(depth[:, :, <span class="number">2</span>])</span><br><span class="line">plt.colorbar(shrink=<span class="number">0.5</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><imgsrc="https://img.032802.xyz/code-reading/Leeiieeo_AG-Pose/load_composed_depth_src.webp"alt="load_composed_depth_src" /><figcaption aria-hidden="true">load_composed_depth_src</figcaption></figure><p>合成第1通道和第2通道：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&#x27;./data/camera/train/00000/0000&#x27;</span></span><br><span class="line"></span><br><span class="line">depth_path = img_path + <span class="string">&#x27;_depth.png&#x27;</span></span><br><span class="line">depth = cv2.imread(depth_path, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">depth_camera = depth[:, :, <span class="number">1</span>]*<span class="number">256</span> + depth[:, :, <span class="number">2</span>]</span><br><span class="line">depth_camera = np.where(depth_camera==<span class="number">32001</span>, <span class="number">0</span>, depth_camera)</span><br><span class="line">depth_camera = depth_camera.astype(np.uint16)</span><br><span class="line"></span><br><span class="line">plt.imshow(depth_camera)</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><imgsrc="https://img.032802.xyz/code-reading/Leeiieeo_AG-Pose/load_composed_depth_dist.webp"alt="load_composed_depth_dist" /><figcaption aria-hidden="true">load_composed_depth_dist</figcaption></figure><p>读取<code>./data/camera_full_depths/train/00000/0000_composed.png</code>，直接可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&#x27;./data/camera_full_depths/train/00000/0000&#x27;</span></span><br><span class="line"></span><br><span class="line">depth_path = img_path + <span class="string">&#x27;_composed.png&#x27;</span></span><br><span class="line">depth = cv2.imread(depth_path, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.imshow(depth)</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><imgsrc="https://img.032802.xyz/code-reading/Leeiieeo_AG-Pose/load_composed_depth.webp"alt="load_composed_depth" /><figcaption aria-hidden="true">load_composed_depth</figcaption></figure><p>下面是真实深度图的读取方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_depth</span>(<span class="params">img_path</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Load depth image from img_path. &quot;&quot;&quot;</span></span><br><span class="line">    depth_path = img_path + <span class="string">&#x27;_depth.png&#x27;</span></span><br><span class="line">    depth = cv2.imread(depth_path, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(depth.shape) == <span class="number">3</span>:</span><br><span class="line">        <span class="comment"># This is encoded depth image, let&#x27;s convert</span></span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> RGB is actually BGR in opencv</span></span><br><span class="line">        depth16 = depth[:, :, <span class="number">1</span>]*<span class="number">256</span> + depth[:, :, <span class="number">2</span>]</span><br><span class="line">        depth16 = np.where(depth16==<span class="number">32001</span>, <span class="number">0</span>, depth16)</span><br><span class="line">        depth16 = depth16.astype(np.uint16)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">len</span>(depth.shape) == <span class="number">2</span> <span class="keyword">and</span> depth.dtype == <span class="string">&#x27;uint16&#x27;</span>:</span><br><span class="line">        depth16 = depth</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="literal">False</span>, <span class="string">&#x27;[ Error ]: Unsupported depth type.&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> depth16</span><br></pre></td></tr></table></figure><p>直接读取<code>./data/real/train/scene_1/0000_depth.png</code>并可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&#x27;./data/real/train/scene_1/0000&#x27;</span></span><br><span class="line"></span><br><span class="line">depth_path = img_path + <span class="string">&#x27;_depth.png&#x27;</span></span><br><span class="line">depth_real = cv2.imread(depth_path, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.imshow(depth_real)</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><imgsrc="https://img.032802.xyz/code-reading/Leeiieeo_AG-Pose/load_depth.webp"alt="load_depth" /><figcaption aria-hidden="true">load_depth</figcaption></figure><p>简单来说就是合成数据集读取的是<code>./data/camera_full_depths/</code>中的深度图，而真实数据集读取的是<code>./data/real/</code>中的深度图。</p><h3 id="深度图补全">深度图补全</h3><p>然后对深度图进行补全，分别对合成深度图和真实深度图进行补全：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> utils.data_utils <span class="keyword">import</span> fill_missing, load_depth, load_composed_depth</span><br><span class="line"></span><br><span class="line">camera_img_path = <span class="string">&#x27;./data/camera/train/00000/0000&#x27;</span></span><br><span class="line">real_img_path = <span class="string">&#x27;./data/real/train/scene_1/0000&#x27;</span></span><br><span class="line"></span><br><span class="line">camera_depth = load_composed_depth(camera_img_path)</span><br><span class="line">real_depth = load_depth(real_img_path)</span><br><span class="line"></span><br><span class="line">fill_missing_camera = fill_missing(camera_depth, <span class="number">1000.0</span>, <span class="number">1</span>)</span><br><span class="line">fill_missing_real = fill_missing(real_depth, <span class="number">1000.0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">plt.imshow(fill_missing_camera)</span><br><span class="line">plt.colorbar(shrink=<span class="number">0.5</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">plt.imshow(fill_missing_real)</span><br><span class="line">plt.colorbar(shrink=<span class="number">0.5</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><imgsrc="https://img.032802.xyz/code-reading/Leeiieeo_AG-Pose/fill_missing_depth.webp"alt="fill_missing_depth" /><figcaption aria-hidden="true">fill_missing_depth</figcaption></figure><h3 id="mask加载">mask加载</h3><p>以下读取<code>./data/camera/train/00000/0000_label.pkl</code>中的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(img_path + <span class="string">&#x27;_label.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    gts = cPickle.load(f)</span><br></pre></td></tr></table></figure><p><code>gts</code>的内容为：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;class_ids&#x27;: [1, 2, 6, 1],</span><br><span class="line"> &#x27;bboxes&#x27;: array([[ 50, 379, 222, 438],</span><br><span class="line">        [224, 107, 325, 222],</span><br><span class="line">        [ 21, 303,  83, 344],</span><br><span class="line">        [  0,  78, 158, 187]], dtype=int32),</span><br><span class="line"> &#x27;scales&#x27;: array([0.3005802 , 0.21199934, 0.17190282, 0.42889243], dtype=float32),</span><br><span class="line"> &#x27;sizes&#x27;: array([[0.238512, 0.941394, 0.238512],</span><br><span class="line">        [0.691974, 0.20577 , 0.691974],</span><br><span class="line">        [0.685634, 0.549036, 0.477982],</span><br><span class="line">        [0.210626, 0.926818, 0.31088 ]], dtype=float32),</span><br><span class="line"> &#x27;rotations&#x27;: array([[[ 0.98163134,  0.01207083, -0.19040541],</span><br><span class="line">         [ 0.14415757, -0.7006575 ,  0.69878304],</span><br><span class="line">         [-0.12497408, -0.7133957 , -0.6895274 ]],</span><br><span class="line"> </span><br><span class="line">        [[ 0.1414129 ,  0.01162317,  0.98988247],</span><br><span class="line">         [-0.705418  , -0.7003605 ,  0.10899841],</span><br><span class="line">         [ 0.69454145, -0.7136947 , -0.09084082]],</span><br><span class="line"> </span><br><span class="line">        [[ 0.02342262,  0.01182115,  0.9996558 ],</span><br><span class="line">         [-0.71319616, -0.7005187 ,  0.02499446],</span><br><span class="line">         [ 0.700573  , -0.7135361 , -0.00797719]],</span><br><span class="line"> </span><br><span class="line">        [[-0.98874557,  0.01170614,  0.14914814],</span><br><span class="line">         [-0.11453401, -0.70061237, -0.7042899 ],</span><br><span class="line">         [ 0.09625051, -0.7134461 ,  0.69406813]]], dtype=float32),</span><br><span class="line"> &#x27;translations&#x27;: array([[ 0.1340191 , -0.14495842,  0.8599784 ],</span><br><span class="line">        [-0.20857327,  0.04460018,  0.7819705 ],</span><br><span class="line">        [ 0.0061691 , -0.40006354,  1.2049764 ],</span><br><span class="line">        [-0.31816682, -0.3390898 ,  0.9828115 ]], dtype=float32),</span><br><span class="line"> &#x27;instance_ids&#x27;: [2, 3, 4, 9],</span><br><span class="line"> &#x27;model_list&#x27;: [&#x27;ab6792cddc7c4c83afbf338b16b43f53&#x27;,</span><br><span class="line">  &#x27;7d7bdea515818eb844638317e9e4ff18&#x27;,</span><br><span class="line">  &#x27;73b8b6456221f4ea20d3c05c08e26f&#x27;,</span><br><span class="line">  &#x27;a1275bd03ab15100f6dbe3dc17d6cdf7&#x27;]&#125;</span><br></pre></td></tr></table></figure><p>读取<code>mask</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mask = cv2.imread(img_path + <span class="string">&#x27;_mask.png&#x27;</span>)[:, :, <span class="number">2</span>]</span><br></pre></td></tr></table></figure><p>这里只取了第2通道，因为第2通道携带了类别信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mask = cv2.imread(<span class="string">&#x27;./data/camera/train/00000/0000_mask.png&#x27;</span>)</span><br><span class="line"></span><br><span class="line">unique_ids_0 = np.unique(mask[:, :, <span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第0通道实例ID列表：&quot;</span>, unique_ids_0)</span><br><span class="line"><span class="comment"># 第0通道实例ID列表： [  0 255]</span></span><br><span class="line"></span><br><span class="line">unique_ids_1 = np.unique(mask[:, :, <span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第1通道实例ID列表：&quot;</span>, unique_ids_1)</span><br><span class="line"><span class="comment"># 第1通道实例ID列表： [  0 255]</span></span><br><span class="line"></span><br><span class="line">unique_ids_2 = np.unique(mask[:, :, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第2通道实例ID列表：&quot;</span>, unique_ids_2)</span><br><span class="line"><span class="comment"># 第2通道实例ID列表： [  1   2   3   4   9 255]</span></span><br></pre></td></tr></table></figure><p>与上方的<code>instance_ids</code>对应。</p><h4 id="去除非目标物体的mask">去除非目标物体的mask</h4><p>首先随机选取一个物体（注意，训练时会从图像中所有物体中随机返回一个，而测试时则会全部返回）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">idx = np.random.randint(<span class="number">0</span>, num_instance)</span><br></pre></td></tr></table></figure><p>使用<code>get_bbox</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rmin, rmax, cmin, cmax = get_bbox(gts[<span class="string">&#x27;bboxes&#x27;</span>][idx])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_bbox</span>(<span class="params">bbox</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Compute square image crop window. &quot;&quot;&quot;</span></span><br><span class="line">    y1, x1, y2, x2 = bbox</span><br><span class="line">    img_width = <span class="number">480</span></span><br><span class="line">    img_length = <span class="number">640</span></span><br><span class="line">    window_size = (<span class="built_in">max</span>(y2 - y1, x2 - x1) // <span class="number">40</span> + <span class="number">1</span>) * <span class="number">40</span></span><br><span class="line">    window_size = <span class="built_in">min</span>(window_size, <span class="number">440</span>)</span><br><span class="line">    center = [(y1 + y2) // <span class="number">2</span>, (x1 + x2) // <span class="number">2</span>]</span><br><span class="line">    rmin = center[<span class="number">0</span>] - <span class="built_in">int</span>(window_size / <span class="number">2</span>)</span><br><span class="line">    rmax = center[<span class="number">0</span>] + <span class="built_in">int</span>(window_size / <span class="number">2</span>)</span><br><span class="line">    cmin = center[<span class="number">1</span>] - <span class="built_in">int</span>(window_size / <span class="number">2</span>)</span><br><span class="line">    cmax = center[<span class="number">1</span>] + <span class="built_in">int</span>(window_size / <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">if</span> rmin &lt; <span class="number">0</span>:</span><br><span class="line">        delt = -rmin</span><br><span class="line">        rmin = <span class="number">0</span></span><br><span class="line">        rmax += delt</span><br><span class="line">    <span class="keyword">if</span> cmin &lt; <span class="number">0</span>:</span><br><span class="line">        delt = -cmin</span><br><span class="line">        cmin = <span class="number">0</span></span><br><span class="line">        cmax += delt</span><br><span class="line">    <span class="keyword">if</span> rmax &gt; img_width:</span><br><span class="line">        delt = rmax - img_width</span><br><span class="line">        rmax = img_width</span><br><span class="line">        rmin -= delt</span><br><span class="line">    <span class="keyword">if</span> cmax &gt; img_length:</span><br><span class="line">        delt = cmax - img_length</span><br><span class="line">        cmax = img_length</span><br><span class="line">        cmin -= delt</span><br><span class="line">    <span class="keyword">return</span> rmin, rmax, cmin, cmax</span><br></pre></td></tr></table></figure><p>以原boundingbox的中心为中心，生成长宽为40的倍数的boundingbox，并考虑了结果boundingbox超出图像范围的情况。</p><p>使用与操作去除其余物体的mask：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mask = np.equal(mask, gts[<span class="string">&#x27;instance_ids&#x27;</span>][idx])</span><br><span class="line">mask = np.logical_and(mask , depth &gt; <span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="从物体上采样">从物体上采样</h3><p>在后续得到物体点云后，需要从点云中采样固定数量的点以调整为网络需要的输入维度，在这一步实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">choose = mask[rmin:rmax, cmin:cmax].flatten().nonzero()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>从mask中截取目标物体的一部分，然后展平，得到其非零值的下标。</p><p>采样到固定数量（配置文件中为1024）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(choose) &lt;= <span class="variable language_">self</span>.sample_num: <span class="comment"># 1024</span></span><br><span class="line">    choose_idx = np.random.choice(<span class="built_in">len</span>(choose), <span class="variable language_">self</span>.sample_num)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    choose_idx = np.random.choice(<span class="built_in">len</span>(choose), <span class="variable language_">self</span>.sample_num, replace=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">choose = choose[choose_idx]</span><br></pre></td></tr></table></figure><h3 id="将深度图转换为点云">将深度图转换为点云</h3><p>获取内参：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cam_fx, cam_fy, cam_cx, cam_cy = <span class="variable language_">self</span>.intrinsics</span><br></pre></td></tr></table></figure><p>（深度）归一化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pts2 = depth.copy() / <span class="variable language_">self</span>.norm_scale</span><br></pre></td></tr></table></figure><p>将像素坐标系中的xy坐标转换为相机坐标系中的xy坐标：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.xmap = np.array([[i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">640</span>)] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">480</span>)])</span><br><span class="line"><span class="variable language_">self</span>.ymap = np.array([[j <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">640</span>)] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">480</span>)])</span><br><span class="line">pts0 = (<span class="variable language_">self</span>.xmap - cam_cx) * pts2 / cam_fx</span><br><span class="line">pts1 = (<span class="variable language_">self</span>.ymap - cam_cy) * pts2 / cam_fy</span><br></pre></td></tr></table></figure><p>具体原理见：<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vc2lsZW5jZS1jaG8vcC8xNTAyMzgyMi5odG1s">针孔相机成像模型<i class="fa fa-external-link-alt"></i></span></p><p>合并并裁剪：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pts = np.transpose(np.stack([pts0, pts1, pts2]), (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)).astype(np.float32)</span><br><span class="line">pts = pts[rmin:rmax, cmin:cmax, :].reshape((-<span class="number">1</span>, <span class="number">3</span>))[choose, :]</span><br></pre></td></tr></table></figure><h3 id="rgb">RGB</h3><p>这里会将目标图像使用<code>rmin, rmax, cmin, cmax</code>进行裁剪，然后resize（代码中为<spanclass="math inline">\(224 \times224\)</span>），那么相应的采样下标也需要修改。</p><p>除此之外，使用OpenCV读入RGB时，其维度顺序为HWC，在经过<code>transforms.ToTensor()</code>后，维度顺序会变为CHW。</p><h3 id="修改采样下标">修改采样下标</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">crop_w = rmax - rmin <span class="comment"># 原本crop mask的宽度</span></span><br><span class="line">ratio = <span class="variable language_">self</span>.img_size / crop_w <span class="comment"># 缩放比例</span></span><br><span class="line">col_idx = choose % crop_w <span class="comment"># 原本列索引的相对位置</span></span><br><span class="line">row_idx = choose // crop_w <span class="comment"># 原本行索引的相对位置</span></span><br><span class="line">choose = (np.floor(row_idx * ratio) * <span class="variable language_">self</span>.img_size + np.floor(col_idx * ratio)).astype(np.int64)</span><br><span class="line"><span class="comment"># np.floor(row_idx * ratio) 新行所在的位置</span></span><br><span class="line"><span class="comment"># np.floor(col_idx * ratio) 新列所在的位置</span></span><br><span class="line"><span class="comment"># np.floor(row_idx * ratio) * self.img_size + np.floor(col_idx * ratio) 新位置的索引</span></span><br></pre></td></tr></table></figure><h3 id="读取物体模型点云">读取物体模型点云</h3><p>首先是全部物体的模型点云，这里读取的是<code>./data/obj_models/camera_train.pkl</code>中的数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.models = &#123;&#125;</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(<span class="variable language_">self</span>.data_dir, model_path), <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="variable language_">self</span>.models.update(cPickle.load(f))</span><br></pre></td></tr></table></figure><p>该文件以字典的形式存储了物体的点云信息，可以通过键取出对应的点云：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = <span class="variable language_">self</span>.models[gts[<span class="string">&#x27;model_list&#x27;</span>][idx]].astype(np.float32)</span><br></pre></td></tr></table></figure><p>取出物体的平移、旋转和大小：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">translation = gts[<span class="string">&#x27;translations&#x27;</span>][idx].astype(np.float32) <span class="comment"># 3 物体坐标系到相机坐标系的平移</span></span><br><span class="line">rotation = gts[<span class="string">&#x27;rotations&#x27;</span>][idx].astype(np.float32) <span class="comment"># 3, 3 物体坐标系到相机坐标系的旋转</span></span><br><span class="line">size = gts[<span class="string">&#x27;scales&#x27;</span>][idx] * gts[<span class="string">&#x27;sizes&#x27;</span>][idx].astype(np.float32) <span class="comment"># 3</span></span><br><span class="line"><span class="comment"># gts[&#x27;scales&#x27;][idx] 1 物体缩放比例</span></span><br><span class="line"><span class="comment"># gts[&#x27;sizes&#x27;][idx] 3 物体长宽高</span></span><br></pre></td></tr></table></figure><h3 id="处理对称物体">处理对称物体</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> cat_id <span class="keyword">in</span> <span class="variable language_">self</span>.sym_ids:</span><br><span class="line">    theta_x = rotation[<span class="number">0</span>, <span class="number">0</span>] + rotation[<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">    theta_y = rotation[<span class="number">0</span>, <span class="number">2</span>] - rotation[<span class="number">2</span>, <span class="number">0</span>]</span><br><span class="line">    r_norm = math.sqrt(theta_x**<span class="number">2</span> + theta_y**<span class="number">2</span>)</span><br><span class="line">    s_map = np.array([[theta_x/r_norm, <span class="number">0.0</span>, -theta_y/r_norm],</span><br><span class="line">                        [<span class="number">0.0</span>,            <span class="number">1.0</span>,  <span class="number">0.0</span>           ],</span><br><span class="line">                        [theta_y/r_norm, <span class="number">0.0</span>,  theta_x/r_norm]])</span><br><span class="line">    rotation = rotation @ s_map</span><br><span class="line"><span class="comment"># 绕Y轴旋转，这里假设Y轴朝上，Z轴朝前，X轴朝右，为右手系</span></span><br></pre></td></tr></table></figure><p>Y轴朝上，所以第2列为<span class="math inline">\([0, 1,0]\)</span>。</p><h2 id="网络架构">网络架构</h2><h3 id="rgb特征">RGB特征</h3><p>使用DINOv2提取RGB特征：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.rgb_extractor = torch.hub.load(<span class="string">&#x27;facebookresearch/dinov2&#x27;</span>,<span class="string">&#x27;dinov2_vits14&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> <span class="variable language_">self</span>.rgb_extractor.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure><p>还使用了一个1d卷积：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.feature_mlp = nn.Sequential(</span><br><span class="line">    nn.Conv1d(<span class="number">384</span>, <span class="number">128</span>, <span class="number">1</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">rgb_local = <span class="variable language_">self</span>.feature_mlp(dino_feature)</span><br></pre></td></tr></table></figure><h3 id="挑选rgb特征">挑选RGB特征</h3><p>在数据处理时，生成了一个<code>choose</code>变量，这里要用该变量将RGB特征从<spanclass="math inline">\(\mathbb{R}^{b \times 128 \times (196 \times196)}\)</span>采样为<span class="math inline">\(\mathbb{R}^{b \times 128\times 1024}\)</span>。</p><h3 id="加噪">加噪</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.training:</span><br><span class="line">    delta_r, delta_t, delta_s = generate_augmentation(b)</span><br><span class="line">    pts = (pts - delta_t) / delta_s.unsqueeze(<span class="number">2</span>) @ delta_r</span><br></pre></td></tr></table></figure><p>其中，生成噪声的函数为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_augmentation</span>(<span class="params">batchsize</span>):</span><br><span class="line">    delta_t = torch.rand(batchsize, <span class="number">1</span>, <span class="number">3</span>).cuda() <span class="comment"># b, 1, 3</span></span><br><span class="line">    delta_t = delta_t.uniform_(-<span class="number">0.02</span>, <span class="number">0.02</span>) <span class="comment"># 将值重新采样到[-0.02, 0.02]范围</span></span><br><span class="line"></span><br><span class="line">    angle_r = torch.randn(batchsize, <span class="number">3</span>) <span class="comment"># b, 3</span></span><br><span class="line">    angle_r.uniform_(-<span class="number">20</span>, <span class="number">20</span>) <span class="comment"># 将值重新采样到[-20, 20]范围</span></span><br><span class="line">    angle_r = angle_r / <span class="number">180</span> * torch.pi <span class="comment"># b, 3 将角度转换为弧度</span></span><br><span class="line"></span><br><span class="line">    delta_r_x = torch.eye(<span class="number">3</span>).unsqueeze(<span class="number">0</span>).repeat(batchsize, <span class="number">1</span>, <span class="number">1</span>) <span class="comment"># b, 3, 3</span></span><br><span class="line">    delta_r_y = torch.eye(<span class="number">3</span>).unsqueeze(<span class="number">0</span>).repeat(batchsize, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    delta_r_z = torch.eye(<span class="number">3</span>).unsqueeze(<span class="number">0</span>).repeat(batchsize, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绕X轴旋转</span></span><br><span class="line">    delta_r_x[:, <span class="number">1</span>, <span class="number">1</span>] = torch.cos(angle_r[:, <span class="number">0</span>])</span><br><span class="line">    delta_r_x[:, <span class="number">1</span>, <span class="number">2</span>] = -torch.sin(angle_r[:, <span class="number">0</span>])</span><br><span class="line">    delta_r_x[:, <span class="number">2</span>, <span class="number">1</span>] = torch.sin(angle_r[:, <span class="number">0</span>])</span><br><span class="line">    delta_r_x[:, <span class="number">2</span>, <span class="number">2</span>] = torch.cos(angle_r[:, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绕Y轴旋转</span></span><br><span class="line">    delta_r_y[:, <span class="number">0</span>, <span class="number">0</span>] = torch.cos(angle_r[:, <span class="number">1</span>])</span><br><span class="line">    delta_r_y[:, <span class="number">0</span>, <span class="number">2</span>] = torch.sin(angle_r[:, <span class="number">1</span>])</span><br><span class="line">    delta_r_y[:, <span class="number">2</span>, <span class="number">0</span>] = -torch.sin(angle_r[:, <span class="number">1</span>])</span><br><span class="line">    delta_r_y[:, <span class="number">2</span>, <span class="number">2</span>] = torch.cos(angle_r[:, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绕Z轴旋转</span></span><br><span class="line">    delta_r_z[:, <span class="number">0</span>, <span class="number">0</span>] = torch.cos(angle_r[:, <span class="number">2</span>])</span><br><span class="line">    delta_r_z[:, <span class="number">0</span>, <span class="number">1</span>] = -torch.sin(angle_r[:, <span class="number">2</span>])</span><br><span class="line">    delta_r_z[:, <span class="number">1</span>, <span class="number">0</span>] = torch.sin(angle_r[:, <span class="number">2</span>])</span><br><span class="line">    delta_r_z[:, <span class="number">1</span>, <span class="number">1</span>] = torch.cos(angle_r[:, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 组合旋转矩阵</span></span><br><span class="line">    delta_r = torch.bmm(torch.bmm(delta_r_x, delta_r_y), delta_r_z).cuda()</span><br><span class="line"></span><br><span class="line">    delta_s = torch.rand(batchsize, <span class="number">1</span>).cuda() <span class="comment"># b, 1</span></span><br><span class="line">    delta_s = delta_s.uniform_(<span class="number">0.8</span>, <span class="number">1.2</span>) <span class="comment"># 将值重新采样到[0.8, 1.2]范围</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> delta_r, delta_t, delta_s</span><br></pre></td></tr></table></figure><p>模型会预测加噪后的位姿，然后在Loss阶段，会使用生成的噪声去除噪声，以实现数据增强。</p><h3 id="点云特征">点云特征</h3><p>原文中说使用PointNet++来提取点云特征，但是暂时没有阅读过和PointNet++相关的论文，所以暂时不详细写。</p><h3 id="iakd">IAKD</h3><p>该模块以RGB特征和点云特征为输入，将点云特征和RGB特征进行拼接后作为<spanclass="math inline">\(KV\)</span>、将可训练的一个查询向量作为<spanclass="math inline">\(Q\)</span>，执行交叉注意力，返回处理后的查询向量和注意力图。</p><p>然后使用查询向量和输入特征做矩阵乘法，得到热图，最后返回查询向量和热图。</p><p>热图将拼接后的点云特征和RGB特征进一步压缩。</p><h3 id="gafa">GAFA</h3><p>首先使用一堆卷积和一堆全连接堆叠成GAFA块，然后使用两个GAFA块组成GAFA模块，最后返回关键点特征。</p><h2 id="训练">训练</h2><p>这里训练和测试都使用的是<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0dvcmlsbGEtTGFiLVNDVVQvZ29yaWxsYS1jb3Jl">Gorilla-Lab-SCUT/gorilla-core<i class="fa fa-external-link-alt"></i></span>中的包。</p><h2 id="测试">测试</h2>]]></content>
    
    
    <summary type="html">类别级6D对象姿态估计旨在估计特定类别中看不见的实例的旋转、平移和大小。在这一领域，基于密集对应的方法取得了领先的性能。但是，它们没有明确考虑不同实例的局部和全局几何信息，导致对具有显著形状变化的不可见实例的泛化能力较差。针对这个问题，我们提出了一种新的用于类别级6D物体姿态估计（AG-Pose）的实例自适应和几何感知关键点学习方法，该方法包括两个关键设计：（1）第一个设计是实例自适应关键点检测模块，它可以自适应地检测各种实例的一组稀疏关键点来表示它们的几何结构。（2）第二种设计是GeometricAware Feature Aggregation模块，可以高效地将局部和全局几何信息集成到关键点特征中。这两个模块可以协同工作，为看不见的实例建立健壮的关键点级对应关系，从而增强模型的泛化能力。在CAMERA25和REAL275数据集上的实验结果表明，所提出的AG-Pose在没有特定类别形状先验的情况下，其性能大大优于最先进的方法。代码将于https://github.com/Leeiieeo/AG-Pose发布。</summary>
    
    
    
    <category term="代码阅读" scheme="https://blog.032802.xyz/categories/%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="2024CVPR" scheme="https://blog.032802.xyz/tags/2024CVPR/"/>
    
    <category term="Object Pose Estimation" scheme="https://blog.032802.xyz/tags/Object-Pose-Estimation/"/>
    
  </entry>
  
  <entry>
    <title>【VPS测评】RackNerd - 4.5 GB KVM VPS (Black Friday 2024)</title>
    <link href="https://blog.032802.xyz/vps-review/racknerd-4.5gb-kvm-vps-black-friday-2024.html"/>
    <id>https://blog.032802.xyz/vps-review/racknerd-4.5gb-kvm-vps-black-friday-2024.html</id>
    <published>2025-03-11T10:28:11.000Z</published>
    <updated>2025-03-11T10:28:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>配置信息：</p><ul><li>3 CPU Cores</li><li>4.5 GB RAM</li><li>16.6 TB Bandwidth</li></ul><p>购买链接：黑五款，已停售。</p><h2 id="测试">2025-03-11测试</h2><h3 id="itdog">ITDOG</h3><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuaXRkb2cuY24vcGluZy8=">https://www.itdog.cn/ping/<i class="fa fa-external-link-alt"></i></span>：</p><p>结果图：<span class="exturl" data-url="aHR0cHM6Ly9pbWcuMDMyODAyLnh5ei92cHMtcmV2aWV3L3JhY2tuZXJkLTQuNWdiLWt2bS12cHMtYmxhY2stZnJpZGF5LTIwMjQvMjAyNS4wMy4xMS03NC40OC53ZWJw">ITDOG：74.48.*.*<i class="fa fa-external-link-alt"></i></span></p><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuaXRkb2cuY24vcGluZ19pcHY2Lw==">https://www.itdog.cn/ping_ipv6/<i class="fa fa-external-link-alt"></i></span>：</p><p>结果图：<span class="exturl" data-url="aHR0cHM6Ly9pbWcuMDMyODAyLnh5ei92cHMtcmV2aWV3L3JhY2tuZXJkLTQuNWdiLWt2bS12cHMtYmxhY2stZnJpZGF5LTIwMjQvMjAyNS4wMy4xMS0yNjA3X2YxMzBfMC53ZWJw">ITDOG：2607:f130:0:*:*:*:*:*<i class="fa fa-external-link-alt"></i></span></p><h3 id="ipquality">IPQuality</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3h5a3QvSVBRdWFsaXR5">xykt/IPQuality<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash &lt;(curl -Ls IP.Check.Place)</span><br></pre></td></tr></table></figure><figure><img src="https://report.check.place/IP/MX40Q96GN.svg"alt="IP质量体检报告：74.48.*.*" /><figcaption aria-hidden="true">IP质量体检报告：74.48.*.*</figcaption></figure><figure><img src="https://report.check.place/IP/17BGIB935.svg"alt="IP质量体检报告：2607:f130:0:*:*:*:*:*" /><figcaptionaria-hidden="true">IP质量体检报告：2607:f130:0:*:*:*:*:*</figcaption></figure><h3 id="融合怪">融合怪</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NwaXJpdExITFMvZWNz">spiritLHLS/ecs<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -L https://gitlab.com/spiritysdx/za/-/raw/main/ecs.sh -o ecs.sh &amp;&amp; <span class="built_in">chmod</span> +x ecs.sh &amp;&amp; bash ecs.sh -m 1</span><br></pre></td></tr></table></figure><p><span class="exturl" data-url="aHR0cHM6Ly9wYXN0ZS5zcGlyaXRsaGwubmV0LyMvc2hvdy9sYlMydS50eHQ=">https://paste.spiritlhl.net/#/show/lbS2u.txt<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br></pre></td><td class="code"><pre><span class="line">--------------------- A Bench Script By spiritlhl ----------------------</span><br><span class="line">                   测评频道: https://t.me/vps_reviews</span><br><span class="line">VPS融合怪版本：2025.02.12</span><br><span class="line">Shell项目地址：https://github.com/spiritLHLS/ecs</span><br><span class="line">Go项目地址：https://github.com/oneclickvirt/ecs</span><br><span class="line">---------------------基础信息查询--感谢所有开源项目---------------------</span><br><span class="line"> CPU 型号          : Intel(R) Xeon(R) CPU E5-2695 v2 @ 2.40GHz</span><br><span class="line"> CPU 核心数        : 3</span><br><span class="line"> CPU 频率          : 2399.998 MHz</span><br><span class="line"> CPU 缓存          : L1: 192.00 KB / L2: 12.00 MB / L3: 48.00 MB</span><br><span class="line"> AES-NI指令集      : ✔ Enabled</span><br><span class="line"> VM-x/AMD-V支持    : ❌ Disabled</span><br><span class="line"> 内存              : 125.72 MiB / 4.33 GiB</span><br><span class="line"> Swap              : 0 KiB / 2.25 GiB</span><br><span class="line"> 硬盘空间          : 1.86 GiB / 96.23 GiB</span><br><span class="line"> 启动盘路径        : /dev/vda1</span><br><span class="line"> 系统在线时间      : 0 days, 0 hour 18 min</span><br><span class="line"> 负载              : 0.26, 0.27, 0.18</span><br><span class="line"> 系统              : Ubuntu 20.04 LTS (x86_64)</span><br><span class="line"> 架构              : x86_64 (64 Bit)</span><br><span class="line"> 内核              : 5.4.0-28-generic</span><br><span class="line"> TCP加速方式       : cubic</span><br><span class="line"> 虚拟化架构        : KVM</span><br><span class="line"> NAT类型           : Full Cone</span><br><span class="line"> IPV4 ASN          : AS35916 MULTACOM CORPORATION</span><br><span class="line"> IPV4 位置         : Los Angeles / California / US</span><br><span class="line"> IPV6 ASN          : AS35916 Multacom Corporation</span><br><span class="line"> IPV6 位置         : Hedwig Village / Texas / United States</span><br><span class="line"> IPV6 子网掩码     : 64</span><br><span class="line">----------------------CPU测试--通过sysbench测试-------------------------</span><br><span class="line"> -&gt; CPU 测试中 (Fast Mode, 1-Pass @ 5sec)</span><br><span class="line"> 1 线程测试(单核)得分:          740 Scores</span><br><span class="line"> 3 线程测试(多核)得分:          2279 Scores</span><br><span class="line">---------------------内存测试--感谢lemonbench开源-----------------------</span><br><span class="line"> -&gt; 内存测试 Test (Fast Mode, 1-Pass @ 5sec)</span><br><span class="line"> 单线程读测试:          15032.99 MB/s</span><br><span class="line"> 单线程写测试:          12425.39 MB/s</span><br><span class="line">------------------磁盘dd读写测试--感谢lemonbench开源--------------------</span><br><span class="line"> -&gt; 磁盘IO测试中 (4K Block/1M Block, Direct Mode)</span><br><span class="line"> 测试操作               写速度                                  读速度</span><br><span class="line"> 100MB-4K Block         34.8 MB/s (8500 IOPS, 3.01s)            40.5 MB/s (9889 IOPS, 2.59s)</span><br><span class="line"> 1GB-1M Block           1.2 GB/s (1127 IOPS, 0.89s)             848 MB/s (808 IOPS, 1.24s)</span><br><span class="line">---------------------磁盘fio读写测试--感谢yabs开源----------------------</span><br><span class="line">Block Size | 4k            (IOPS) | 64k           (IOPS)</span><br><span class="line">  ------   | ---            ----  | ----           ----</span><br><span class="line">Read       | 76.91 MB/s   (19.2k) | 1.06 GB/s    (16.5k)</span><br><span class="line">Write      | 77.12 MB/s   (19.2k) | 1.06 GB/s    (16.6k)</span><br><span class="line">Total      | 154.03 MB/s  (38.5k) | 2.13 GB/s    (33.2k)</span><br><span class="line">           |                      |</span><br><span class="line">Block Size | 512k          (IOPS) | 1m            (IOPS)</span><br><span class="line">  ------   | ---            ----  | ----           ----</span><br><span class="line">Read       | 1.18 GB/s     (2.3k) | 1.19 GB/s     (1.1k)</span><br><span class="line">Write      | 1.24 GB/s     (2.4k) | 1.26 GB/s     (1.2k)</span><br><span class="line">Total      | 2.43 GB/s     (4.7k) | 2.46 GB/s     (2.4k)</span><br><span class="line">------------流媒体解锁--基于oneclickvirt/CommonMediaTests开源-----------</span><br><span class="line">以下测试的解锁地区是准确的，但是不是完整解锁的判断可能有误，这方面仅作参考使用</span><br><span class="line">----------------Netflix-----------------</span><br><span class="line">[IPV4]</span><br><span class="line">您的出口IP完整解锁Netflix，支持非自制剧的观看</span><br><span class="line">NF所识别的IP地域信息：美国</span><br><span class="line">[IPV6]</span><br><span class="line">您的出口IP可以使用Netflix，但仅可看Netflix自制剧</span><br><span class="line">NF所识别的IP地域信息：美国</span><br><span class="line">----------------Youtube-----------------</span><br><span class="line">[IPV4]</span><br><span class="line">连接方式: Youtube Video Server</span><br><span class="line">视频缓存节点地域: 美国  洛杉机(LAX17S56)</span><br><span class="line">[IPV6]</span><br><span class="line">连接方式: Youtube Video Server</span><br><span class="line">视频缓存节点地域: 美国  洛杉机(LAX31S13)</span><br><span class="line">---------------DisneyPlus---------------</span><br><span class="line">[IPV4]</span><br><span class="line">当前IPv4出口所在地区即将开通DisneyPlus</span><br><span class="line">[IPV6]</span><br><span class="line">当前IPv4出口所在地区即将开通DisneyPlus</span><br><span class="line">解锁Netflix，Youtube，DisneyPlus上面和下面进行比较，不同之处自行判断</span><br><span class="line">----------------流媒体解锁--感谢RegionRestrictionCheck开源--------------</span><br><span class="line"> 以下为IPV4网络测试，若无IPV4网络则无输出</span><br><span class="line">============[ Multination ]============</span><br><span class="line"> Dazn:                                  Yes (Region: US)</span><br><span class="line"> Disney+:                               Yes (Region: US)</span><br><span class="line"> Netflix:                               Yes (Region: US)</span><br><span class="line"> YouTube Premium:                       Yes (Region: US)</span><br><span class="line"> Amazon Prime Video:                    Yes (Region: US)</span><br><span class="line"> TVBAnywhere+:                          Yes</span><br><span class="line"> Spotify Registration:                  No</span><br><span class="line"> OneTrust Region:                       US [California]</span><br><span class="line"> iQyi Oversea Region:                   US</span><br><span class="line"> Bing Region:                           US (Risky)</span><br><span class="line"> Apple Region:                          US</span><br><span class="line"> YouTube CDN:                           Los Angeles, CA</span><br><span class="line"> Netflix Preferred CDN:                 Los Angeles, CA</span><br><span class="line"> ChatGPT:                               Yes</span><br><span class="line"> Google Gemini:                         Yes (Region: USA)</span><br><span class="line"> Claude:                                Yes</span><br><span class="line"> Wikipedia Editability:                 No</span><br><span class="line"> Google Play Store:                     United States</span><br><span class="line"> Google Search CAPTCHA Free:            Yes</span><br><span class="line"> Steam Currency:                        USD</span><br><span class="line"> ---Forum---</span><br><span class="line"> Reddit:                                Yes</span><br><span class="line">=======================================</span><br><span class="line"> 以下为IPV6网络测试，若无IPV6网络则无输出</span><br><span class="line">============[ Multination ]============</span><br><span class="line"> Dazn:                                  IPv6 Is Not Currently Supported</span><br><span class="line"> Disney+:                               IPv6 Is Not Currently Supported</span><br><span class="line"> Netflix:                               Originals Only</span><br><span class="line"> YouTube Premium:                       Yes (Region: US)</span><br><span class="line"> Amazon Prime Video:                    IPv6 Is Not Currently Supported</span><br><span class="line"> TVBAnywhere+:                          IPv6 Is Not Currently Supported</span><br><span class="line"> Spotify Registration:                  No</span><br><span class="line"> OneTrust Region:                       US [Texas]</span><br><span class="line"> iQyi Oversea Region:                   IPv6 Is Not Currently Supported</span><br><span class="line"> Bing Region:                           US (Risky)</span><br><span class="line"> Apple Region:                          US</span><br><span class="line"> YouTube CDN:                           Los Angeles, CA</span><br><span class="line"> Netflix Preferred CDN:                 Los Angeles, CA</span><br><span class="line"> ChatGPT:                               Failed (Network Connection)</span><br><span class="line"> Google Gemini:                         Yes (Region: USA)</span><br><span class="line"> Claude:                                Yes</span><br><span class="line"> Wikipedia Editability:                 No</span><br><span class="line"> Google Play Store:                     United States</span><br><span class="line"> Google Search CAPTCHA Free:            Yes</span><br><span class="line"> Steam Currency:                        IPv6 Is Not Currently Supported</span><br><span class="line"> ---Forum---</span><br><span class="line"> Reddit:                                IPv6 Is Not Currently Supported</span><br><span class="line">=======================================</span><br><span class="line">---------------TikTok解锁--感谢lmc999的源脚本及fscarmen PR--------------</span><br><span class="line"> Tiktok Region:         【US】</span><br><span class="line">-------------IP质量检测--基于oneclickvirt/securityCheck使用-------------</span><br><span class="line">数据仅作参考，不代表100%准确，如果和实际情况不一致请手动查询多个数据库比对</span><br><span class="line">以下为各数据库编号，输出结果后将自带数据库来源对应的编号</span><br><span class="line">ipinfo数据库  [0] | scamalytics数据库 [1] | virustotal数据库   [2] | abuseipdb数据库   [3] | ip2location数据库    [4]</span><br><span class="line">ip-api数据库  [5] | ipwhois数据库     [6] | ipregistry数据库   [7] | ipdata数据库      [8] | db-ip数据库          [9]</span><br><span class="line">ipapiis数据库 [A] | ipapicom数据库    [B] | bigdatacloud数据库 [C] | cheervision数据库 [D] | ipqualityscore数 据库 [E]</span><br><span class="line">IPV4:</span><br><span class="line">安全得分:</span><br><span class="line">声誉(越高越好): 0 [2]</span><br><span class="line">信任得分(越高越好): 0 [8]</span><br><span class="line">VPN得分(越低越好): 100 [8]</span><br><span class="line">代理得分(越低越好): 100 [8]</span><br><span class="line">社区投票-无害: 0 [2]</span><br><span class="line">社区投票-恶意: 0 [2]</span><br><span class="line">威胁得分(越低越好): 100 [8]</span><br><span class="line">欺诈得分(越低越好): 39 [1] 65 [E]</span><br><span class="line">滥用得分(越低越好): 0 [3]</span><br><span class="line">ASN滥用得分(越低越好): 0.0001 (Very Low) [A]</span><br><span class="line">公司滥用得分(越低越好): 0.0011 (Low) [A]</span><br><span class="line">威胁级别: low [B]</span><br><span class="line">黑名单记录统计:(有多少黑名单网站有记录):</span><br><span class="line">无害记录数: 0 [2]  恶意记录数: 0 [2]  可疑记录数: 0 [2]  无记录数: 94 [2]</span><br><span class="line">安全信息:</span><br><span class="line">使用类型: isp [A] hosting [0 7] hosting - moderate probability [C] DataCenter/WebHosting/Transit [3] business [8]</span><br><span class="line">公司类型: isp [A] hosting [0 7]</span><br><span class="line">是否云提供商: Yes [7 D]</span><br><span class="line">是否数据中心: No [8 A C] Yes [0 1 5 6]</span><br><span class="line">是否移动设备: Yes [E] No [5 A C]</span><br><span class="line">是否代理: No [0 1 4 5 6 7 8 A B C D] Yes [E]</span><br><span class="line">是否VPN: Yes [A E] No [0 1 6 7 C D]</span><br><span class="line">是否Tor: No [0 1 3 6 7 8 A B C D E]</span><br><span class="line">是否Tor出口: No [1 7 D]</span><br><span class="line">是否网络爬虫: No [A B E]</span><br><span class="line">是否匿名: No [1 6 7 8 D]</span><br><span class="line">是否攻击者: No [7 8 D]</span><br><span class="line">是否滥用者: No [7 8 A C D E]</span><br><span class="line">是否威胁: No [7 8 C D]</span><br><span class="line">是否中继: No [0 7 8 C D]</span><br><span class="line">是否Bogon: No [7 8 A C D]</span><br><span class="line">是否机器人: No [E]</span><br><span class="line">DNS-黑名单: 313(Total_Check) 0(Clean) 4(Blacklisted) 18(Other)</span><br><span class="line">IPV6:</span><br><span class="line">安全得分:</span><br><span class="line">欺诈得分(越低越好): 21 [1]</span><br><span class="line">滥用得分(越低越好): 0 [3]</span><br><span class="line">ASN滥用得分(越低越好): 0.0001 (Very Low) [A]</span><br><span class="line">公司滥用得分(越低越好): 0 (Very Low) [A]</span><br><span class="line">威胁级别: low [B]</span><br><span class="line">安全信息:</span><br><span class="line">使用类型: DataCenter/WebHosting/Transit [3] isp [A]</span><br><span class="line">公司类型: isp [A]</span><br><span class="line">是否云提供商: Yes [D]</span><br><span class="line">是否数据中心: Yes [1] No [A]</span><br><span class="line">是否移动设备: No [A]</span><br><span class="line">是否代理: No [1 A B D]</span><br><span class="line">是否VPN: No [1 A D]</span><br><span class="line">是否TorExit: No [1 D]</span><br><span class="line">是否Tor出口: No [1 D]</span><br><span class="line">是否网络爬虫: No [A B]</span><br><span class="line">是否匿名: No [1 D]</span><br><span class="line">是否攻击者: No [D]</span><br><span class="line">是否滥用者: No [A D]</span><br><span class="line">是否威胁: No [D]</span><br><span class="line">是否中继: No [D]</span><br><span class="line">是否Bogon: No [A D]</span><br><span class="line">DNS-黑名单: 313(Total_Check) 0(Clean) 0(Blacklisted) 313(Other)</span><br><span class="line">Google搜索可行性：NO</span><br><span class="line">-------------邮件端口检测--基于oneclickvirt/portchecker开源-------------</span><br><span class="line">Platform  SMTP  SMTPS POP3  POP3S IMAP  IMAPS</span><br><span class="line">LocalPort ✔     ✔     ✔     ✔     ✔     ✔</span><br><span class="line">QQ        ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">163       ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">Sohu      ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">Yandex    ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">Gmail     ✔     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">Outlook   ✔     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Office365 ✔     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Yahoo     ✔     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">MailCOM   ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">MailRU    ✔     ✔     ✘     ✘     ✔     ✘</span><br><span class="line">AOL       ✔     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">GMX       ✔     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Sina      ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">Apple     ✘     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">FastMail  ✘     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">ProtonMail✘     ✘     ✘     ✘     ✘     ✘</span><br><span class="line">MXRoute   ✔     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Namecrane ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">XYAMail   ✘     ✘     ✘     ✘     ✘     ✘</span><br><span class="line">ZohoMail  ✘     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">Inbox_eu  ✔     ✔     ✔     ✘     ✘     ✘</span><br><span class="line">Free_fr   ✘     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">----------------三网回程--基于oneclickvirt/backtrace开源----------------</span><br><span class="line">北京电信 219.141.140.10  检测不到回程路由节点的IP地址</span><br><span class="line">北京联通 202.106.195.68  联通4837   [普通线路]</span><br><span class="line">北京移动 221.179.155.161 移动CMI    [普通线路]</span><br><span class="line">上海电信 202.96.209.133  电信163    [普通线路]</span><br><span class="line">上海联通 210.22.97.1     联通4837   [普通线路]</span><br><span class="line">上海移动 211.136.112.200 移动CMI    [普通线路]</span><br><span class="line">广州电信 58.60.188.222   电信163    [普通线路]</span><br><span class="line">广州联通 210.21.196.6    联通4837   [普通线路]</span><br><span class="line">广州移动 120.196.165.24  移动CMI    [普通线路]</span><br><span class="line">成都电信 61.139.2.69     电信163    [普通线路]</span><br><span class="line">成都联通 119.6.6.6       联通4837   [普通线路]</span><br><span class="line">成都移动 211.137.96.205  移动CMI    [普通线路]</span><br><span class="line">准确线路自行查看详细路由，本测试结果仅作参考</span><br><span class="line">同一目标地址多个线路时，可能检测已越过汇聚层，除了第一个线路外，后续信息可能无效</span><br><span class="line">---------------------回程路由--感谢fscarmen开源及PR---------------------</span><br><span class="line">依次测试电信/联通/移动经过的地区及线路，核心程序来自nexttrace，请知悉!</span><br><span class="line">广州电信 58.60.188.222</span><br><span class="line">0.86 ms         AS35916 美国 加利福尼亚 洛杉矶 multacom.com</span><br><span class="line">1.05 ms         AS35916 美国 加利福尼亚州 洛杉矶 multacom.com</span><br><span class="line">7.05 ms         AS2914 [NTTA-128] 美国 加利福尼亚 洛杉矶 gin.ntt.net</span><br><span class="line">0.68 ms         AS2914 [NTT-BACKBONE] 美国 加利福尼亚 洛杉矶 gin.ntt.net</span><br><span class="line">9.11 ms         AS2914 [NTT-BACKBONE] 美国 加利福尼亚 圣何塞 gin.ntt.net</span><br><span class="line">9.28 ms         AS2914 [NTT-BACKBONE] 美国 加利福尼亚 圣何塞 gin.ntt.net</span><br><span class="line">13.49 ms        AS4134 [CHINANET-US] 美国 加利福尼亚 圣克拉拉 www.chinatelecom.com.cn 电信</span><br><span class="line">161.59 ms       AS4134 [CHINANET-BB] 中国 广东 广州 www.chinatelecom.com.cn 电信</span><br><span class="line">160.32 ms       AS4134 [CHINANET-BB] 中国 广东 广州 www.chinatelecom.com.cn 电信</span><br><span class="line">165.19 ms       AS4134 中国 广东 深圳 福田区 www.chinatelecom.com.cn 电信</span><br><span class="line">广州联通 210.21.196.6</span><br><span class="line">1.16 ms         AS35916 美国 加利福尼亚 洛杉矶 multacom.com</span><br><span class="line">2.36 ms         AS35916 美国 加利福尼亚州 洛杉矶 multacom.com</span><br><span class="line">0.90 ms         AS64050 [BCPL-SG] 新加坡 bgp.net</span><br><span class="line">14.88 ms        AS4837 [CU169-BACKBONE] 美国 加利福尼亚 圣何塞 chinaunicom.cn 联通</span><br><span class="line">157.18 ms       AS4837 [CU169-BACKBONE] 中国 北京 chinaunicom.cn 联通</span><br><span class="line">178.71 ms       AS4837 [CU169-BACKBONE] 中国 北京 chinaunicom.cn 联通</span><br><span class="line">193.06 ms       AS17816 [UNICOM-GD] 中国 广东 深圳 chinaunicom.cn 联通</span><br><span class="line">195.11 ms       AS17623 [APNIC-AP] 中国 广东 深圳 chinaunicom.cn 联通</span><br><span class="line">214.31 ms       AS17623 中国 广东 深圳 宝安区 chinaunicom.cn 联通</span><br><span class="line">广州移动 120.196.165.24</span><br><span class="line">0.88 ms         AS35916 美国 加利福尼亚 洛杉矶 multacom.com</span><br><span class="line">1.06 ms         AS35916 美国 加利福尼亚州 洛杉矶 multacom.com</span><br><span class="line">5.97 ms         AS2914 [NTTA-128] 美国 加利福尼亚 洛杉矶 gin.ntt.net</span><br><span class="line">0.72 ms         AS2914 [NTT-BACKBONE] 美国 加利福尼亚 洛杉矶 gin.ntt.net</span><br><span class="line">1.10 ms         AS2914 [NTT-BACKBONE] 美国 加利福尼亚 洛杉矶 gin.ntt.net</span><br><span class="line">210.30 ms       AS2914 [NTT-BACKBONE] 日本 大阪府 大阪 gin.ntt.net</span><br><span class="line">264.74 ms       AS2914 [NTT-BACKBONE] 中国 香港 gin.ntt.net</span><br><span class="line">162.02 ms       AS2914 [NTT-BACKBONE] 中国 香港 gin.ntt.net</span><br><span class="line">154.15 ms       AS58453 [CMI-INT] 中国 香港 cmi.chinamobile.com 移动</span><br><span class="line">148.17 ms       AS58453 [CMI-INT] 中国 广东 广州 cmi.chinamobile.com 移动</span><br><span class="line">162.94 ms       AS9808 [CMNET] 中国 广东 广州 chinamobileltd.com 移动</span><br><span class="line">163.22 ms       AS9808 [CMNET] 中国 广东 广州 I-C chinamobileltd.com 移动</span><br><span class="line">166.40 ms       AS9808 [CMNET] 中国 广东 广州 chinamobileltd.com 移动</span><br><span class="line">209.93 ms       AS9808 [CMNET] 中国 广东 广州 chinamobileltd.com 移动</span><br><span class="line">157.25 ms       AS56040 [APNIC-AP] 中国 广东 深圳 gd.10086.cn 移动</span><br><span class="line">--------------------自动更新测速节点列表--本脚本原创--------------------</span><br><span class="line">位置             上传速度        下载速度        延迟     丢包率</span><br><span class="line">Speedtest.net    847.28 Mbps     849.00 Mbps     0.52     0.0%</span><br><span class="line">洛杉矶           804.29 Mbps     836.19 Mbps     1.00     0.0%</span><br><span class="line">日本东京         841.07 Mbps     146.15 Mbps     102.80   0.0%</span><br><span class="line">联通上海5G       475.84 Mbps     931.15 Mbps     179.97   0.0%</span><br><span class="line">联通Beijing      700.53 Mbps     151.27 Mbps     158.41   NULL</span><br><span class="line">电信Zhenjiang5G  36.45 Mbps      220.96 Mbps     148.83   NULL</span><br><span class="line">电信浙江         4.87 Mbps       34.35 Mbps      150.15   NULL</span><br><span class="line">移动Fujian       412.00 Mbps     49.55 Mbps      212.57   NULL</span><br><span class="line">------------------------------------------------------------------------</span><br><span class="line"> 总共花费      : 7 分 27 秒</span><br><span class="line"> 时间          : Tue Mar 11 10:57:31 GMT 2025</span><br><span class="line">------------------------------------------------------------------------</span><br><span class="line">  短链:</span><br><span class="line">    https://paste.spiritlhl.net/#/show/lbS2u.txt</span><br><span class="line">    http://hpaste.spiritlhl.net/#/show/lbS2u.txt</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">本次测评的服务器是RackNerd的4.5 GB KVM VPS (Black Friday 2024)，年付&#92;$39.88USD。</summary>
    
    
    
    <category term="VPS测评" scheme="https://blog.032802.xyz/categories/VPS%E6%B5%8B%E8%AF%84/"/>
    
    
    <category term="VPS" scheme="https://blog.032802.xyz/tags/VPS/"/>
    
    <category term="RackNerd" scheme="https://blog.032802.xyz/tags/RackNerd/"/>
    
    <category term="VPS测评" scheme="https://blog.032802.xyz/tags/VPS%E6%B5%8B%E8%AF%84/"/>
    
    <category term="美国VPS" scheme="https://blog.032802.xyz/tags/%E7%BE%8E%E5%9B%BDVPS/"/>
    
  </entry>
  
  <entry>
    <title>【VPS测评】Sakura Clouds - LAX2 VPS - 2C1G KVM VPS</title>
    <link href="https://blog.032802.xyz/vps-review/sakuraclouds-lax2-vps-2c1g-kvm-vps.html"/>
    <id>https://blog.032802.xyz/vps-review/sakuraclouds-lax2-vps-2c1g-kvm-vps.html</id>
    <published>2025-03-11T06:49:32.000Z</published>
    <updated>2025-03-11T06:49:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>配置信息：</p><ul><li>2 x E3 1240 V6 Core(s)</li><li>1 GB RAM</li><li>15 GB SSD Storage</li><li>5000 GB Bandwidth (in+out)</li><li>1 Gbps Global Port Shared</li><li>1 x IPv4 + 1 x /64 IPv6</li><li>Virtualizor KVM</li><li>Linux Only</li></ul><p>购买链接：<span class="exturl" data-url="aHR0cHM6Ly9wb3J0YWwuc2FrdXJhY2xvdWRzLmNvbS9hZmYucGhwP2FmZj03OQ==">Sakura Clouds -LAX2 VPS-2C1G KVM VPS<i class="fa fa-external-link-alt"></i></span></p><h2 id="测试">2025-03-11测试</h2><h3 id="itdog">ITDOG</h3><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuaXRkb2cuY24vcGluZy8=">https://www.itdog.cn/ping/<i class="fa fa-external-link-alt"></i></span>：</p><p>结果图：<span class="exturl" data-url="aHR0cHM6Ly9pbWcuMDMyODAyLnh5ei92cHMtcmV2aWV3L3Nha3VyYWNsb3Vkcy1sYXgyLXZwcy0yYzFnLWt2bS12cHMvMjAyNS4wMy4xMS03Ny4xMTEud2VicA==">ITDOG：77.111.*.*<i class="fa fa-external-link-alt"></i></span></p><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuaXRkb2cuY24vcGluZ19pcHY2Lw==">https://www.itdog.cn/ping_ipv6/<i class="fa fa-external-link-alt"></i></span>：</p><p>结果图：<span class="exturl" data-url="aHR0cHM6Ly9pbWcuMDMyODAyLnh5ei92cHMtcmV2aWV3L3Nha3VyYWNsb3Vkcy1sYXgyLXZwcy0yYzFnLWt2bS12cHMvMjAyNS4wMy4xMS0yYTBmXzg1YzFfODYxLndlYnA=">ITDOG：2a0f:85c1:861:*:*:*:*:*<i class="fa fa-external-link-alt"></i></span></p><h3 id="ipquality">IPQuality</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3h5a3QvSVBRdWFsaXR5">xykt/IPQuality<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash &lt;(curl -Ls IP.Check.Place)</span><br></pre></td></tr></table></figure><figure><img src="https://report.check.place/IP/VUYLPDN78.svg"alt="IP质量体检报告：77.111.*.*" /><figcaption aria-hidden="true">IP质量体检报告：77.111.*.*</figcaption></figure><figure><img src="https://report.check.place/IP/TJZ0RT0SM.svg"alt="IP质量体检报告：2a0f:85c1:861:*:*:*:*:*" /><figcaptionaria-hidden="true">IP质量体检报告：2a0f:85c1:861:*:*:*:*:*</figcaption></figure><h3 id="融合怪">融合怪</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NwaXJpdExITFMvZWNz">spiritLHLS/ecs<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -L https://gitlab.com/spiritysdx/za/-/raw/main/ecs.sh -o ecs.sh &amp;&amp; <span class="built_in">chmod</span> +x ecs.sh &amp;&amp; bash ecs.sh -m 1</span><br></pre></td></tr></table></figure><p><span class="exturl" data-url="aHR0cHM6Ly9wYXN0ZS5zcGlyaXRsaGwubmV0LyMvc2hvdy9TTE5TYy50eHQ=">https://paste.spiritlhl.net/#/show/SLNSc.txt<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br></pre></td><td class="code"><pre><span class="line">--------------------- A Bench Script By spiritlhl ----------------------</span><br><span class="line">                   测评频道: https://t.me/vps_reviews</span><br><span class="line">VPS融合怪版本：2025.02.12</span><br><span class="line">Shell项目地址：https://github.com/spiritLHLS/ecs</span><br><span class="line">Go项目地址：https://github.com/oneclickvirt/ecs</span><br><span class="line">---------------------基础信息查询--感谢所有开源项目---------------------</span><br><span class="line"> CPU 型号          : Intel(R) Xeon(R) CPU E3-1240 v6 @ 3.70GHz</span><br><span class="line"> CPU 核心数        : 2</span><br><span class="line"> CPU 频率          : 3695.962 MHz</span><br><span class="line"> CPU 缓存          : L1: 64.00 KB / L2: 512.00 KB / L3: 16.00 MB</span><br><span class="line"> AES-NI指令集      : ✔ Enabled</span><br><span class="line"> VM-x/AMD-V支持    : ✔ Enabled</span><br><span class="line"> 内存              : 139.70 MiB / 960.77 MiB</span><br><span class="line"> Swap              : 0 KiB / 512.00 MiB</span><br><span class="line"> 硬盘空间          : 1.79 GiB / 14.66 GiB</span><br><span class="line"> 启动盘路径        : /dev/vda1</span><br><span class="line"> 系统在线时间      : 0 days, 0 hour 15 min</span><br><span class="line"> 负载              : 0.30, 0.19, 0.10</span><br><span class="line"> 系统              : Debian GNU/Linux 12 (bookworm) (x86_64)</span><br><span class="line"> 架构              : x86_64 (64 Bit)</span><br><span class="line"> 内核              : 6.1.0-9-amd64</span><br><span class="line"> TCP加速方式       : cubic</span><br><span class="line"> 虚拟化架构        : KVM</span><br><span class="line"> NAT类型           : Full Cone</span><br><span class="line"> IPV4 ASN          : AS214478 Sakura Clouds LLC</span><br><span class="line"> IPV4 位置         : Los Angeles / California / US</span><br><span class="line"> IPV6 ASN          : AS214478 Sakura Clouds</span><br><span class="line"> IPV6 位置         : Los Angeles / California / United States</span><br><span class="line"> IPV6 子网掩码     : 64</span><br><span class="line">----------------------CPU测试--通过sysbench测试-------------------------</span><br><span class="line"> -&gt; CPU 测试中 (Fast Mode, 1-Pass @ 5sec)</span><br><span class="line"> 1 线程测试(单核)得分:          1090 Scores</span><br><span class="line"> 2 线程测试(多核)得分:          2294 Scores</span><br><span class="line">---------------------内存测试--感谢lemonbench开源-----------------------</span><br><span class="line"> -&gt; 内存测试 Test (Fast Mode, 1-Pass @ 5sec)</span><br><span class="line"> 单线程读测试:          21966.35 MB/s</span><br><span class="line"> 单线程写测试:          18198.81 MB/s</span><br><span class="line">------------------磁盘dd读写测试--感谢lemonbench开源--------------------</span><br><span class="line"> -&gt; 磁盘IO测试中 (4K Block/1M Block, Direct Mode)</span><br><span class="line"> 测试操作               写速度                                  读速度</span><br><span class="line"> 100MB-4K Block         4.5 MB/s (1099 IOPS, 23.29s)            4.5 MB/s (1104 IOPS, 23.18s)</span><br><span class="line"> 1GB-1M Block           300 MB/s (286 IOPS, 3.49s)              302 MB/s (288 IOPS, 3.47s)</span><br><span class="line">---------------------磁盘fio读写测试--感谢yabs开源----------------------</span><br><span class="line">Block Size | 4k            (IOPS) | 64k           (IOPS)</span><br><span class="line">  ------   | ---            ----  | ----           ----</span><br><span class="line">Read       | 2.18 MB/s      (547) | 35.06 MB/s     (547)</span><br><span class="line">Write      | 2.21 MB/s      (552) | 35.41 MB/s     (553)</span><br><span class="line">Total      | 4.40 MB/s     (1.0k) | 70.48 MB/s    (1.1k)</span><br><span class="line">           |                      |</span><br><span class="line">Block Size | 512k          (IOPS) | 1m            (IOPS)</span><br><span class="line">  ------   | ---            ----  | ----           ----</span><br><span class="line">Read       | 226.73 MB/s    (442) | 233.84 MB/s    (228)</span><br><span class="line">Write      | 238.78 MB/s    (466) | 249.42 MB/s    (243)</span><br><span class="line">Total      | 465.51 MB/s    (908) | 483.27 MB/s    (471)</span><br><span class="line">------------流媒体解锁--基于oneclickvirt/CommonMediaTests开源-----------</span><br><span class="line">以下测试的解锁地区是准确的，但是不是完整解锁的判断可能有误，这方面仅作参考使用</span><br><span class="line">----------------Netflix-----------------</span><br><span class="line">[IPV4]</span><br><span class="line">您的出口IP完整解锁Netflix，支持非自制剧的观看</span><br><span class="line">NF所识别的IP地域信息：美国</span><br><span class="line">[IPV6]</span><br><span class="line">您的出口IP完整解锁Netflix，支持非自制剧的观看</span><br><span class="line">NF所识别的IP地域信息：美国</span><br><span class="line">----------------Youtube-----------------</span><br><span class="line">[IPV4]</span><br><span class="line">连接方式: Youtube Video Server</span><br><span class="line">视频缓存节点地域: 美国  洛杉机(LAX17S56)</span><br><span class="line">[IPV6]</span><br><span class="line">连接方式: Youtube Video Server</span><br><span class="line">视频缓存节点地域: 美国  洛杉机(LAX17S56)</span><br><span class="line">---------------DisneyPlus---------------</span><br><span class="line">[IPV4]</span><br><span class="line">当前IPv4出口所在地区即将开通DisneyPlus</span><br><span class="line">[IPV6]</span><br><span class="line">当前IPv4出口所在地区即将开通DisneyPlus</span><br><span class="line">解锁Netflix，Youtube，DisneyPlus上面和下面进行比较，不同之处自行判断</span><br><span class="line">----------------流媒体解锁--感谢RegionRestrictionCheck开源--------------</span><br><span class="line"> 以下为IPV4网络测试，若无IPV4网络则无输出</span><br><span class="line">============[ Multination ]============</span><br><span class="line"> Dazn:                                  Yes (Region: US)</span><br><span class="line"> Disney+:                               Yes (Region: US)</span><br><span class="line"> Netflix:                               Yes (Region: US)</span><br><span class="line"> YouTube Premium:                       Yes (Region: US)</span><br><span class="line"> Amazon Prime Video:                    Yes (Region: US)</span><br><span class="line"> TVBAnywhere+:                          Yes</span><br><span class="line"> Spotify Registration:                  No</span><br><span class="line"> OneTrust Region:                       US [California]</span><br><span class="line"> iQyi Oversea Region:                   US</span><br><span class="line"> Bing Region:                           US (Risky)</span><br><span class="line"> Apple Region:                          US</span><br><span class="line"> YouTube CDN:                           Los Angeles, CA</span><br><span class="line"> Netflix Preferred CDN:                 [Sakura Clouds LLC] in [Los Angeles, CA]</span><br><span class="line"> ChatGPT:                               Yes</span><br><span class="line"> Google Gemini:                         Yes (Region: USA)</span><br><span class="line"> Claude:                                Yes</span><br><span class="line"> Wikipedia Editability:                 Yes</span><br><span class="line"> Google Play Store:                     United States</span><br><span class="line"> Google Search CAPTCHA Free:            Yes</span><br><span class="line"> Steam Currency:                        USD</span><br><span class="line"> ---Forum---</span><br><span class="line"> Reddit:                                No</span><br><span class="line">=======================================</span><br><span class="line"> 以下为IPV6网络测试，若无IPV6网络则无输出</span><br><span class="line">============[ Multination ]============</span><br><span class="line"> Dazn:                                  IPv6 Is Not Currently Supported</span><br><span class="line"> Disney+:                               IPv6 Is Not Currently Supported</span><br><span class="line"> Netflix:                               Yes (Region: US)</span><br><span class="line"> YouTube Premium:                       Yes (Region: US)</span><br><span class="line"> Amazon Prime Video:                    IPv6 Is Not Currently Supported</span><br><span class="line"> TVBAnywhere+:                          IPv6 Is Not Currently Supported</span><br><span class="line"> Spotify Registration:                  Yes (Region: US)</span><br><span class="line"> OneTrust Region:                       US [California]</span><br><span class="line"> iQyi Oversea Region:                   IPv6 Is Not Currently Supported</span><br><span class="line"> Bing Region:                           US (Risky)</span><br><span class="line"> Apple Region:                          US</span><br><span class="line"> YouTube CDN:                           Los Angeles, CA</span><br><span class="line"> Netflix Preferred CDN:                 [Sakura Clouds] in [Los Angeles, CA]</span><br><span class="line"> ChatGPT:                               Failed (Network Connection)</span><br><span class="line"> Google Gemini:                         Yes (Region: USA)</span><br><span class="line"> Claude:                                Yes</span><br><span class="line"> Wikipedia Editability:                 Yes</span><br><span class="line"> Google Play Store:                     United States</span><br><span class="line"> Google Search CAPTCHA Free:            Yes</span><br><span class="line"> Steam Currency:                        IPv6 Is Not Currently Supported</span><br><span class="line"> ---Forum---</span><br><span class="line"> Reddit:                                IPv6 Is Not Currently Supported</span><br><span class="line">=======================================</span><br><span class="line">---------------TikTok解锁--感谢lmc999的源脚本及fscarmen PR--------------</span><br><span class="line"> Tiktok Region:         【US】</span><br><span class="line">-------------IP质量检测--基于oneclickvirt/securityCheck使用-------------</span><br><span class="line">数据仅作参考，不代表100%准确，如果和实际情况不一致请手动查询多个数据库比对</span><br><span class="line">以下为各数据库编号，输出结果后将自带数据库来源对应的编号</span><br><span class="line">ipinfo数据库  [0] | scamalytics数据库 [1] | virustotal数据库   [2] | abuseipdb数据库   [3] | ip2location数据库    [4]</span><br><span class="line">ip-api数据库  [5] | ipwhois数据库     [6] | ipregistry数据库   [7] | ipdata数据库      [8] | db-ip数据库          [9]</span><br><span class="line">ipapiis数据库 [A] | ipapicom数据库    [B] | bigdatacloud数据库 [C] | cheervision数据库 [D] | ipqualityscore数据库 [E]</span><br><span class="line">IPV4:</span><br><span class="line">安全得分:</span><br><span class="line">声誉(越高越好): 0 [2]</span><br><span class="line">信任得分(越高越好): 9 [8]</span><br><span class="line">VPN得分(越低越好): 78 [8]</span><br><span class="line">代理得分(越低越好): 99 [8]</span><br><span class="line">社区投票-无害: 0 [2]</span><br><span class="line">社区投票-恶意: 0 [2]</span><br><span class="line">威胁得分(越低越好): 96 [8]</span><br><span class="line">欺诈得分(越低越好): 0 [1] 65 [E]</span><br><span class="line">滥用得分(越低越好): 0 [3]</span><br><span class="line">ASN滥用得分(越低越好): 0.0029 (Low) [A]</span><br><span class="line">公司滥用得分(越低越好): 0.001 (Low) [A]</span><br><span class="line">威胁级别: low [9 B]</span><br><span class="line">黑名单记录统计:(有多少黑名单网站有记录):</span><br><span class="line">无害记录数: 0 [2]  恶意记录数: 0 [2]  可疑记录数: 0 [2]  无记录数: 94 [2]</span><br><span class="line">安全信息:</span><br><span class="line">使用类型: DataCenter/WebHosting/Transit [3] hosting - high probability [C] business [8 A] hosting [0 7 9]</span><br><span class="line">公司类型: isp [A] hosting [0 7]</span><br><span class="line">是否云提供商: Yes [7 D]</span><br><span class="line">是否数据中心: No [5 6 8] Yes [0 1 A C]</span><br><span class="line">是否移动设备: No [5 A C] Yes [E]</span><br><span class="line">是否代理: Yes [E] No [0 1 4 5 6 7 8 9 A B C D]</span><br><span class="line">是否VPN: No [0 1 6 7 A C D] Yes [E]</span><br><span class="line">是否Tor: No [0 1 3 6 7 8 A B C D E]</span><br><span class="line">是否Tor出口: No [1 7 D]</span><br><span class="line">是否网络爬虫: No [9 A B E]</span><br><span class="line">是否匿名: No [1 6 7 8 D]</span><br><span class="line">是否攻击者: No [7 8 D]</span><br><span class="line">是否滥用者: No [7 8 A C D E]</span><br><span class="line">是否威胁: No [7 8 C D]</span><br><span class="line">是否中继: No [0 7 8 C D]</span><br><span class="line">是否Bogon: No [7 8 A C D]</span><br><span class="line">是否机器人: No [E]</span><br><span class="line">DNS-黑名单: 313(Total_Check) 0(Clean) 5(Blacklisted) 19(Other)</span><br><span class="line">IPV6:</span><br><span class="line">安全得分:</span><br><span class="line">欺诈得分(越低越好): 0 [1]</span><br><span class="line">滥用得分(越低越好): 0 [3]</span><br><span class="line">ASN滥用得分(越低越好): 0.0029 (Low) [A]</span><br><span class="line">公司滥用得分(越低越好): 0 (Very Low) [A]</span><br><span class="line">威胁级别: low [B]</span><br><span class="line">安全信息:</span><br><span class="line">使用类型: DataCenter/WebHosting/Transit [3] business [A]</span><br><span class="line">公司类型: business [A]</span><br><span class="line">是否云提供商: Yes [D]</span><br><span class="line">是否数据中心: Yes [1 A]</span><br><span class="line">是否移动设备: No [A]</span><br><span class="line">是否代理: No [1 A B D]</span><br><span class="line">是否VPN: No [1 A D]</span><br><span class="line">是否TorExit: No [1 D]</span><br><span class="line">是否Tor出口: No [1 D]</span><br><span class="line">是否网络爬虫: No [A B]</span><br><span class="line">是否匿名: No [1 D]</span><br><span class="line">是否攻击者: No [D]</span><br><span class="line">是否滥用者: No [A D]</span><br><span class="line">是否威胁: No [D]</span><br><span class="line">是否中继: No [D]</span><br><span class="line">是否Bogon: No [A D]</span><br><span class="line">DNS-黑名单: 313(Total_Check) 0(Clean) 0(Blacklisted) 313(Other)</span><br><span class="line">Google搜索可行性：NO</span><br><span class="line">-------------邮件端口检测--基于oneclickvirt/portchecker开源-------------</span><br><span class="line">Platform  SMTP  SMTPS POP3  POP3S IMAP  IMAPS</span><br><span class="line">LocalPort ✔     ✔     ✔     ✔     ✔     ✔</span><br><span class="line">QQ        ✘     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">163       ✘     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">Sohu      ✘     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">Yandex    ✘     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">Gmail     ✘     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">Outlook   ✘     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Office365 ✘     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Yahoo     ✘     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">MailCOM   ✘     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">MailRU    ✘     ✔     ✘     ✘     ✔     ✘</span><br><span class="line">AOL       ✘     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">GMX       ✘     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Sina      ✘     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">Apple     ✘     ✘     ✘     ✘     ✘     ✘</span><br><span class="line">FastMail  ✘     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">ProtonMail✘     ✘     ✘     ✘     ✘     ✘</span><br><span class="line">MXRoute   ✘     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Namecrane ✘     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">XYAMail   ✘     ✘     ✘     ✘     ✘     ✘</span><br><span class="line">ZohoMail  ✘     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">Inbox_eu  ✘     ✔     ✔     ✘     ✘     ✘</span><br><span class="line">Free_fr   ✘     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">----------------三网回程--基于oneclickvirt/backtrace开源----------------</span><br><span class="line">北京电信 219.141.140.10  电信163    [普通线路]</span><br><span class="line">北京联通 202.106.195.68  联通4837   [普通线路]</span><br><span class="line">北京移动 221.179.155.161 移动CMI    [普通线路]</span><br><span class="line">上海电信 202.96.209.133  检测不到回程路由节点的IP地址</span><br><span class="line">上海联通 210.22.97.1     联通4837   [普通线路]</span><br><span class="line">上海移动 211.136.112.200 检测不到回程路由节点的IP地址</span><br><span class="line">广州电信 58.60.188.222   电信163    [普通线路]</span><br><span class="line">广州联通 210.21.196.6    联通4837   [普通线路]</span><br><span class="line">广州移动 120.196.165.24  移动CMI    [普通线路]</span><br><span class="line">成都电信 61.139.2.69     电信163    [普通线路]</span><br><span class="line">成都联通 119.6.6.6       联通4837   [普通线路]</span><br><span class="line">成都移动 211.137.96.205  移动CMI    [普通线路]</span><br><span class="line">准确线路自行查看详细路由，本测试结果仅作参考</span><br><span class="line">同一目标地址多个线路时，可能检测已越过汇聚层，除了第一个线路外，后续信息可能无效</span><br><span class="line">---------------------回程路由--感谢fscarmen开源及PR---------------------</span><br><span class="line">依次测试电信/联通/移动经过的地区及线路，核心程序来自nexttrace，请知悉!</span><br><span class="line">广州电信 58.60.188.222</span><br><span class="line">0.53 ms         AS214478 美国 加利福尼亚州 洛杉矶 Dravanet</span><br><span class="line">1.06 ms         AS3257 [GTT-GTT] 美国 加利福尼亚 洛杉矶 gtt.net</span><br><span class="line">92.34 ms        AS3257 [GTT-BACKBONE] 美国 加利福尼亚 洛杉矶 gtt.net</span><br><span class="line">2.07 ms         AS3257 美国 gtt.net</span><br><span class="line">156.46 ms       AS4134 [CHINANET-BB] 中国 广东 广州 www.chinatelecom.com.cn 电信</span><br><span class="line">206.85 ms       AS4134 [CHINANET-BB] 中国 广东 广州 www.chinatelecom.com.cn 电信</span><br><span class="line">159.97 ms       AS4134 中国 广东 深圳 福田区 www.chinatelecom.com.cn 电信</span><br><span class="line">广州联通 210.21.196.6</span><br><span class="line">5.67 ms         AS214478 美国 加利福尼亚州 洛杉矶 Dravanet</span><br><span class="line">0.47 ms         AS3257 [GTT-GTT] 美国 加利福尼亚 洛杉矶 gtt.net</span><br><span class="line">10.56 ms        AS3257 [GTT-BACKBONE] 美国 加利福尼亚 圣何塞 gtt.net</span><br><span class="line">158.53 ms       AS3257 美国 加利福尼亚 圣何塞 gtt.net</span><br><span class="line">179.18 ms       AS4837 [CU169-BACKBONE] 中国 上海 chinaunicom.cn 联通</span><br><span class="line">280.12 ms       AS4837 [CU169-BACKBONE] 中国 上海 chinaunicom.cn 联通</span><br><span class="line">175.90 ms       AS17816 [UNICOM-GD] 中国 广东 深圳 chinaunicom.cn 联通</span><br><span class="line">181.50 ms       AS17623 [APNIC-AP] 中国 广东 深圳 chinaunicom.cn 联通</span><br><span class="line">176.13 ms       AS17623 中国 广东 深圳 宝安区 chinaunicom.cn 联通</span><br><span class="line">广州移动 120.196.165.24</span><br><span class="line">3.97 ms         AS214478 美国 加利福尼亚州 洛杉矶 Dravanet</span><br><span class="line">0.66 ms         AS2914 [NTTA-128] 美国 加利福尼亚 洛杉矶 gin.ntt.net</span><br><span class="line">3.02 ms         AS2914 [NTT-BACKBONE] 美国 加利福尼亚 洛杉矶 gin.ntt.net</span><br><span class="line">253.71 ms       AS2914 [NTT-BACKBONE] 日本 东京都 东京 gin.ntt.net</span><br><span class="line">198.68 ms       AS2914 [NTT-BACKBONE] 中国 香港 gin.ntt.net</span><br><span class="line">147.12 ms       AS2914 [NTT-BACKBONE] 中国 香港 gin.ntt.net</span><br><span class="line">156.60 ms       AS2914 [NTT-GLOBAL] 中国 香港 gin.ntt.net</span><br><span class="line">149.50 ms       AS58453 [CMI-INT] 中国 香港 cmi.chinamobile.com 移动</span><br><span class="line">166.43 ms       AS58453 [CMI-INT] 中国 广东 广州 cmi.chinamobile.com 移动</span><br><span class="line">153.17 ms       AS9808 [CMNET] 中国 广东 广州 chinamobileltd.com 移动</span><br><span class="line">155.66 ms       AS9808 [CMNET] 中国 广东 广州 I-C chinamobileltd.com 移动</span><br><span class="line">160.79 ms       AS9808 [CMNET] 中国 广东 广州 chinamobileltd.com 移动</span><br><span class="line">167.55 ms       AS9808 [CMNET] 中国 广东 广州 chinamobileltd.com 移动</span><br><span class="line">165.01 ms       AS56040 [APNIC-AP] 中国 广东 深圳 gd.10086.cn 移动</span><br><span class="line">--------------------自动更新测速节点列表--本脚本原创--------------------</span><br><span class="line">位置             上传速度        下载速度        延迟     丢包率</span><br><span class="line">Speedtest.net    917.94 Mbps     927.34 Mbps     0.23     0.0%</span><br><span class="line">洛杉矶           854.26 Mbps     946.30 Mbps     0.88     0.0%</span><br><span class="line">日本东京         388.84 Mbps     667.46 Mbps     107.15   0.0%</span><br><span class="line">联通上海5G       10.56 Mbps      538.27 Mbps     179.19   0.0%</span><br><span class="line">联通Beijing      243.37 Mbps     680.30 Mbps     168.07   NULL</span><br><span class="line">电信Suzhou5G     260.20 Mbps     718.30 Mbps     138.34   NULL</span><br><span class="line">电信浙江         39.87 Mbps      538.90 Mbps     137.61   NULL</span><br><span class="line">移动Fujian       225.30 Mbps     572.29 Mbps     208.06   NULL</span><br><span class="line">------------------------------------------------------------------------</span><br><span class="line"> 总共花费      : 8 分 17 秒</span><br><span class="line"> 时间          : Tue Mar 11 03:00:47 EDT 2025</span><br><span class="line">------------------------------------------------------------------------</span><br><span class="line">  短链:</span><br><span class="line">    https://paste.spiritlhl.net/#/show/SLNSc.txt</span><br><span class="line">    http://hpaste.spiritlhl.net/#/show/SLNSc.txt</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">本次测评的服务器是Sakura Clouds的LAX2 VPS-2C1G KVM VPS，月付&#92;$1.00USD。</summary>
    
    
    
    <category term="VPS测评" scheme="https://blog.032802.xyz/categories/VPS%E6%B5%8B%E8%AF%84/"/>
    
    
    <category term="VPS" scheme="https://blog.032802.xyz/tags/VPS/"/>
    
    <category term="VPS测评" scheme="https://blog.032802.xyz/tags/VPS%E6%B5%8B%E8%AF%84/"/>
    
    <category term="美国VPS" scheme="https://blog.032802.xyz/tags/%E7%BE%8E%E5%9B%BDVPS/"/>
    
    <category term="Sakura Clouds" scheme="https://blog.032802.xyz/tags/Sakura-Clouds/"/>
    
  </entry>
  
  <entry>
    <title>【VPS测评】BandwagonHost - MINIBOX-10-512</title>
    <link href="https://blog.032802.xyz/vps-review/bandwagonhost-minibox-10-512.html"/>
    <id>https://blog.032802.xyz/vps-review/bandwagonhost-minibox-10-512.html</id>
    <published>2025-03-10T11:32:15.000Z</published>
    <updated>2025-03-10T11:32:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>配置信息：</p><ul><li>1 CPU</li><li>512 MB</li><li>10 GB SSD</li><li>500 GB/mo</li><li>1 Gigabit</li><li>Los Angeles</li></ul><p>购买链接：活动机型，现已停售。</p><h2 id="测试">2025-03-10测试</h2><h3 id="itdog">ITDOG</h3><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuaXRkb2cuY24vcGluZy8=">https://www.itdog.cn/ping/<i class="fa fa-external-link-alt"></i></span>：</p><p>结果图：<span class="exturl" data-url="aHR0cHM6Ly9pbWcuMDMyODAyLnh5ei92cHMtcmV2aWV3L2JhbmR3YWdvbmhvc3QtbWluaWJveC0xMC01MTIvMjAyNS4wMy4xMC00NS42Mi53ZWJw">ITDOG：45.62.*.*<i class="fa fa-external-link-alt"></i></span></p><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuaXRkb2cuY24vcGluZ19pcHY2Lw==">https://www.itdog.cn/ping_ipv6/<i class="fa fa-external-link-alt"></i></span>：</p><p>结果图：<span class="exturl" data-url="aHR0cHM6Ly9pbWcuMDMyODAyLnh5ei92cHMtcmV2aWV3L2JhbmR3YWdvbmhvc3QtbWluaWJveC0xMC01MTIvMjAyNS4wMy4xMC0yNjA3Xzg3MDBfNTUwMC53ZWJw">ITDOG：2607:8700:5500:*:*:*:*<i class="fa fa-external-link-alt"></i></span></p><h3 id="ipquality">IPQuality</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3h5a3QvSVBRdWFsaXR5">xykt/IPQuality<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash &lt;(curl -Ls IP.Check.Place)</span><br></pre></td></tr></table></figure><p>IP质量体检报告：45.62.*.*：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">########################################################################</span><br><span class="line">                    IP质量体检报告(Lite)：45.62.*.*</span><br><span class="line">                    bash &lt;(curl -sL IP.Check.Place)</span><br><span class="line">                   https://github.com/xykt/IPQuality</span><br><span class="line">        报告时间：2025-03-10 19:32:54 CST  脚本版本：v2025-01-24</span><br><span class="line">########################################################################</span><br><span class="line">一、基础信息（IPinfo 数据库）</span><br><span class="line">自治系统号：            ASAS25820</span><br><span class="line">组织：                  IT7 Networks Inc</span><br><span class="line">坐标：                  118°15′60″W, 34°3′21″N</span><br><span class="line">地图：                  https://check.place/34.0559,-118.2666,12,cn</span><br><span class="line">城市：                  Los Angeles, 90017</span><br><span class="line">使用地：                [US]United States of America, Americas</span><br><span class="line">注册地：                [CA]Canada</span><br><span class="line">IP类型：                 广播IP</span><br><span class="line">二、IP类型属性</span><br><span class="line">数据库：      IPinfo      ipapi    IP2LOCATION</span><br><span class="line">使用类型：     机房        家宽        机房</span><br><span class="line">公司类型：     机房        家宽</span><br><span class="line">三、风险评分</span><br><span class="line">风险等级：      极低         低       中等       高         极高</span><br><span class="line">SCAMALYTICS：  0|低风险</span><br><span class="line">ipapi：           0.39%|低风险</span><br><span class="line">Cloudflare：   0|低风险</span><br><span class="line">DB-IP：         |低风险</span><br><span class="line">四、风险因子</span><br><span class="line">库： IP2LOCATION ipapi SCAMALYTICS IPinfo IPWHOIS</span><br><span class="line">地区：    [US]    [US]    [US]    [US]    [US]</span><br><span class="line">代理：     否      否      否      否      是</span><br><span class="line">Tor：      否      否      否      否      否</span><br><span class="line">VPN：      是      是      否      否      否</span><br><span class="line">服务器：   是      否      否      是      是</span><br><span class="line">滥用：     否      否      无      无      无</span><br><span class="line">机器人：   否      否      否      无      无</span><br><span class="line">五、流媒体及AI服务解锁检测</span><br><span class="line">服务商：  TikTok   Disney+  Netflix Youtube  AmazonPV  Spotify  ChatGPT</span><br><span class="line">状态：     解锁     屏蔽    仅自制    解锁     解锁     屏蔽     解锁</span><br><span class="line">地区：     [US]              [US]     [US]     [US]              [US]</span><br><span class="line">方式：     原生              原生     原生     原生               DNS</span><br><span class="line">六、邮局连通性及黑名单检测</span><br><span class="line">本地25端口：阻断</span><br><span class="line">IP地址黑名单数据库：  有效 439   正常 409   已标记 30   黑名单 0</span><br><span class="line">========================================================================</span><br><span class="line">今日IP检测量：896；总检测量：444941。感谢使用xy系列脚本！</span><br></pre></td></tr></table></figure><p>IP质量体检报告：2607:8700:5500:*:*:*:*：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">########################################################################</span><br><span class="line">             IP质量体检报告(Lite)：2607:8700:5500:*:*:*:*:*</span><br><span class="line">                    bash &lt;(curl -sL IP.Check.Place)</span><br><span class="line">                   https://github.com/xykt/IPQuality</span><br><span class="line">        报告时间：2025-03-10 19:32:54 CST  脚本版本：v2025-01-24</span><br><span class="line">########################################################################</span><br><span class="line">一、基础信息（IPinfo 数据库）</span><br><span class="line">自治系统号：            ASAS25820</span><br><span class="line">组织：                  IT7 Networks Inc</span><br><span class="line">坐标：                  118°15′60″W, 34°3′21″N</span><br><span class="line">地图：                  https://check.place/34.0559,-118.2666,12,cn</span><br><span class="line">城市：                  Los Angeles, 90017</span><br><span class="line">使用地：                [US]United States of America, Americas</span><br><span class="line">注册地：                [CA]Canada</span><br><span class="line">IP类型：                 广播IP</span><br><span class="line">二、IP类型属性</span><br><span class="line">数据库：      IPinfo      ipapi    IP2LOCATION</span><br><span class="line">使用类型：     机房        家宽        机房</span><br><span class="line">公司类型：     机房        家宽</span><br><span class="line">三、风险评分</span><br><span class="line">风险等级：      极低         低       中等       高         极高</span><br><span class="line">SCAMALYTICS：  0|低风险</span><br><span class="line">ipapi：    0.00%|极低风险</span><br><span class="line">Cloudflare：   0|低风险</span><br><span class="line">DB-IP：         |低风险</span><br><span class="line">四、风险因子</span><br><span class="line">库： IP2LOCATION ipapi SCAMALYTICS IPinfo IPWHOIS</span><br><span class="line">地区：    [US]    [US]    [US]    [US]    [US]</span><br><span class="line">代理：     否      否      否      否      否</span><br><span class="line">Tor：      否      否      否      否      否</span><br><span class="line">VPN：      是      否      否      否      否</span><br><span class="line">服务器：   是      否      否      是      否</span><br><span class="line">滥用：     否      否      无      无      无</span><br><span class="line">机器人：   否      否      否      无      无</span><br><span class="line">五、流媒体及AI服务解锁检测</span><br><span class="line">服务商：  TikTok   Disney+  Netflix Youtube  AmazonPV  Spotify  ChatGPT</span><br><span class="line">状态：     失败     屏蔽    仅自制    解锁     屏蔽     屏蔽     失败</span><br><span class="line">地区：                       [CA]     [US]</span><br><span class="line">方式：                       原生     原生</span><br><span class="line">六、邮局连通性及黑名单检测</span><br><span class="line">本地25端口：阻断</span><br><span class="line">========================================================================</span><br><span class="line">今日IP检测量：897；总检测量：444942。感谢使用xy系列脚本！</span><br></pre></td></tr></table></figure><h3 id="融合怪">融合怪</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NwaXJpdExITFMvZWNz">spiritLHLS/ecs<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -L https://gitlab.com/spiritysdx/za/-/raw/main/ecs.sh -o ecs.sh &amp;&amp; <span class="built_in">chmod</span> +x ecs.sh &amp;&amp; bash ecs.sh -m 1</span><br></pre></td></tr></table></figure><p><span class="exturl" data-url="aHR0cHM6Ly9wYXN0ZS5zcGlyaXRsaGwubmV0LyMvc2hvdy9qZzZjbi50eHQ=">https://paste.spiritlhl.net/#/show/jg6cn.txt<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br></pre></td><td class="code"><pre><span class="line">--------------------- A Bench Script By spiritlhl ----------------------</span><br><span class="line">                   测评频道: https://t.me/vps_reviews</span><br><span class="line">VPS融合怪版本：2025.02.12</span><br><span class="line">Shell项目地址：https://github.com/spiritLHLS/ecs</span><br><span class="line">Go项目地址：https://github.com/oneclickvirt/ecs</span><br><span class="line">---------------------基础信息查询--感谢所有开源项目---------------------</span><br><span class="line"> CPU 型号          : AMD EPYC-Genoa Processor</span><br><span class="line"> CPU 核心数        : 1</span><br><span class="line"> CPU 频率          : 2345.594 MHz</span><br><span class="line"> CPU 缓存          : L1: 32.00 KB / L2: 1.00 MB / L3: 32.00 MB</span><br><span class="line"> AES-NI指令集      : ✔ Enabled</span><br><span class="line"> VM-x/AMD-V支持    : ❌ Disabled</span><br><span class="line"> 内存              : 122.80 MiB / 583.02 MiB</span><br><span class="line"> Swap              : 2.02 MiB / 321.00 MiB</span><br><span class="line"> 硬盘空间          : 1.51 GiB / 10.29 GiB</span><br><span class="line"> 启动盘路径        : /dev/sda2</span><br><span class="line"> 系统在线时间      : 0 days, 0 hour 6 min</span><br><span class="line"> 负载              : 0.54, 0.37, 0.15</span><br><span class="line"> 系统              : Debian GNU/Linux 12 (bookworm) (x86_64)</span><br><span class="line"> 架构              : x86_64 (64 Bit)</span><br><span class="line"> 内核              : 6.1.0-9-amd64</span><br><span class="line"> TCP加速方式       : bbr</span><br><span class="line"> 虚拟化架构        : KVM</span><br><span class="line"> NAT类型           : Full Cone</span><br><span class="line"> IPV4 ASN          : AS25820 IT7 Networks Inc</span><br><span class="line"> IPV4 位置         : Los Angeles / California / US</span><br><span class="line"> IPV6 ASN          : AS25820 IT7 Networks Inc</span><br><span class="line"> IPV6 位置         : Los Angeles / California / United States</span><br><span class="line"> IPV6 子网掩码     : 128</span><br><span class="line">----------------------CPU测试--通过sysbench测试-------------------------</span><br><span class="line"> -&gt; CPU 测试中 (Fast Mode, 1-Pass @ 5sec)</span><br><span class="line"> 1 线程测试(单核)得分:          1592 Scores</span><br><span class="line">---------------------内存测试--感谢lemonbench开源-----------------------</span><br><span class="line"> -&gt; 内存测试 Test (Fast Mode, 1-Pass @ 5sec)</span><br><span class="line"> 单线程读测试:          39867.44 MB/s</span><br><span class="line"> 单线程写测试:          17976.18 MB/s</span><br><span class="line">------------------磁盘dd读写测试--感谢lemonbench开源--------------------</span><br><span class="line"> -&gt; 磁盘IO测试中 (4K Block/1M Block, Direct Mode)</span><br><span class="line"> 测试操作               写速度                                  读速度</span><br><span class="line"> 100MB-4K Block         19.3 MB/s (4706 IOPS, 5.44s)            22.2 MB/s (5419 IOPS, 4.72s)</span><br><span class="line"> 1GB-1M Block           897 MB/s (855 IOPS, 1.17s)              1.3 GB/s (1232 IOPS, 0.81s)</span><br><span class="line">---------------------磁盘fio读写测试--感谢yabs开源----------------------</span><br><span class="line">Block Size | 4k            (IOPS) | 64k           (IOPS)</span><br><span class="line">  ------   | ---            ----  | ----           ----</span><br><span class="line">Read       | 140.32 MB/s  (35.0k) | 217.90 MB/s   (3.4k)</span><br><span class="line">Write      | 140.69 MB/s  (35.1k) | 219.04 MB/s   (3.4k)</span><br><span class="line">Total      | 281.02 MB/s  (70.2k) | 436.95 MB/s   (6.8k)</span><br><span class="line">           |                      |</span><br><span class="line">Block Size | 512k          (IOPS) | 1m            (IOPS)</span><br><span class="line">  ------   | ---            ----  | ----           ----</span><br><span class="line">Read       | 549.30 MB/s   (1.0k) | 843.19 MB/s    (823)</span><br><span class="line">Write      | 578.49 MB/s   (1.1k) | 899.35 MB/s    (878)</span><br><span class="line">Total      | 1.12 GB/s     (2.2k) | 1.74 GB/s     (1.7k)</span><br><span class="line">------------流媒体解锁--基于oneclickvirt/CommonMediaTests开源-----------</span><br><span class="line">以下测试的解锁地区是准确的，但是不是完整解锁的判断可能有误，这方面仅作参考使用</span><br><span class="line">----------------Netflix-----------------</span><br><span class="line">[IPV4]</span><br><span class="line">您的出口IP可以使用Netflix，但仅可看Netflix自制剧</span><br><span class="line">NF所识别的IP地域信息：美国</span><br><span class="line">[IPV6]</span><br><span class="line">您的出口IP可以使用Netflix，但仅可看Netflix自制剧</span><br><span class="line">NF所识别的IP地域信息：加拿大</span><br><span class="line">----------------Youtube-----------------</span><br><span class="line">[IPV4]</span><br><span class="line">连接方式: Youtube Video Server</span><br><span class="line">视频缓存节点地域: 美国  洛杉机(LAX17S56)</span><br><span class="line">[IPV6]</span><br><span class="line">连接方式: Youtube Video Server</span><br><span class="line">视频缓存节点地域: 美国  洛杉机(LAX31S13)</span><br><span class="line">---------------DisneyPlus---------------</span><br><span class="line">[IPV4]</span><br><span class="line">当前IPv4出口所在地区即将开通DisneyPlus</span><br><span class="line">[IPV6]</span><br><span class="line">当前IPv4出口所在地区即将开通DisneyPlus</span><br><span class="line">解锁Netflix，Youtube，DisneyPlus上面和下面进行比较，不同之处自行判断</span><br><span class="line">----------------流媒体解锁--感谢RegionRestrictionCheck开源--------------</span><br><span class="line"> 以下为IPV4网络测试，若无IPV4网络则无输出</span><br><span class="line">============[ Multination ]============</span><br><span class="line"> Dazn:                                  Failed (Error: )</span><br><span class="line"> Disney+:                               No (IP Banned By Disney+ 1)</span><br><span class="line"> Netflix:                               Originals Only</span><br><span class="line"> YouTube Premium:                       Yes (Region: US)</span><br><span class="line"> Amazon Prime Video:                    Yes (Region: US)</span><br><span class="line"> TVBAnywhere+:                          Yes</span><br><span class="line"> Spotify Registration:                  No</span><br><span class="line"> OneTrust Region:                       US [California]</span><br><span class="line"> iQyi Oversea Region:                   US</span><br><span class="line"> Bing Region:                           US (Risky)</span><br><span class="line"> Apple Region:                          US</span><br><span class="line"> YouTube CDN:                           Los Angeles, CA</span><br><span class="line"> Netflix Preferred CDN:                 Los Angeles, CA</span><br><span class="line"> ChatGPT:                               Yes</span><br><span class="line"> Google Gemini:                         Yes (Region: USA)</span><br><span class="line"> Claude:                                Yes</span><br><span class="line"> Wikipedia Editability:                 No</span><br><span class="line"> Google Play Store:                     United States</span><br><span class="line"> Google Search CAPTCHA Free:            Yes</span><br><span class="line"> Steam Currency:                        USD</span><br><span class="line"> ---Forum---</span><br><span class="line"> Reddit:                                No</span><br><span class="line">=======================================</span><br><span class="line"> 以下为IPV6网络测试，若无IPV6网络则无输出</span><br><span class="line">============[ Multination ]============</span><br><span class="line"> Dazn:                                  IPv6 Is Not Currently Supported</span><br><span class="line"> Disney+:                               IPv6 Is Not Currently Supported</span><br><span class="line"> Netflix:                               Originals Only</span><br><span class="line"> YouTube Premium:                       Yes (Region: US)</span><br><span class="line"> Amazon Prime Video:                    IPv6 Is Not Currently Supported</span><br><span class="line"> TVBAnywhere+:                          IPv6 Is Not Currently Supported</span><br><span class="line"> Spotify Registration:                  No</span><br><span class="line"> OneTrust Region:                       US [California]</span><br><span class="line"> iQyi Oversea Region:                   IPv6 Is Not Currently Supported</span><br><span class="line"> Bing Region:                           US (Risky)</span><br><span class="line"> Apple Region:                          US</span><br><span class="line"> YouTube CDN:                           Los Angeles, CA</span><br><span class="line"> Netflix Preferred CDN:                 Los Angeles, CA</span><br><span class="line"> ChatGPT:                               Failed (Network Connection)</span><br><span class="line"> Google Gemini:                         Yes (Region: USA)</span><br><span class="line"> Claude:                                Yes</span><br><span class="line"> Wikipedia Editability:                 No</span><br><span class="line"> Google Play Store:                     United States</span><br><span class="line"> Google Search CAPTCHA Free:            Yes</span><br><span class="line"> Steam Currency:                        IPv6 Is Not Currently Supported</span><br><span class="line"> ---Forum---</span><br><span class="line"> Reddit:                                IPv6 Is Not Currently Supported</span><br><span class="line">=======================================</span><br><span class="line">---------------TikTok解锁--感谢lmc999的源脚本及fscarmen PR--------------</span><br><span class="line"> Tiktok Region:         【US】(可能为IDC IP)</span><br><span class="line">-------------IP质量检测--基于oneclickvirt/securityCheck使用-------------</span><br><span class="line">数据仅作参考，不代表100%准确，如果和实际情况不一致请手动查询多个数据库比对</span><br><span class="line">以下为各数据库编号，输出结果后将自带数据库来源对应的编号</span><br><span class="line">ipinfo数据库  [0] | scamalytics数据库 [1] | virustotal数据库   [2] | abuseipdb数据库   [3] | ip2location数据库    [4]</span><br><span class="line">ip-api数据库  [5] | ipwhois数据库     [6] | ipregistry数据库   [7] | ipdata数据库      [8] | db-ip数据库          [9]</span><br><span class="line">ipapiis数据库 [A] | ipapicom数据库    [B] | bigdatacloud数据库 [C] | cheervision数据库 [D] | ipqualityscore数据库 [E]</span><br><span class="line">IPV4:</span><br><span class="line">安全得分:</span><br><span class="line">声誉(越高越好): 0 [2]</span><br><span class="line">信任得分(越高越好): 92 [8]</span><br><span class="line">VPN得分(越低越好): 4 [8]</span><br><span class="line">代理得分(越低越好): 16 [8]</span><br><span class="line">社区投票-无害: 0 [2]</span><br><span class="line">社区投票-恶意: 0 [2]</span><br><span class="line">威胁得分(越低越好): 5 [8]</span><br><span class="line">欺诈得分(越低越好): 0 [1] 65 [E]</span><br><span class="line">滥用得分(越低越好): 0 [3]</span><br><span class="line">ASN滥用得分(越低越好): 0.0022 (Low) [A]</span><br><span class="line">公司滥用得分(越低越好): 0 (Very Low) [A]</span><br><span class="line">威胁级别: low [9 B]</span><br><span class="line">黑名单记录统计:(有多少黑名单网站有记录):</span><br><span class="line">无害记录数: 0 [2]  恶意记录数: 0 [2]  可疑记录数: 0 [2]  无记录数: 94 [2]</span><br><span class="line">安全信息:</span><br><span class="line">使用类型: isp [A] unknown [C] DataCenter/WebHosting/Transit [3] hosting [0 7 9]</span><br><span class="line">公司类型: isp [A] business [7] hosting [0]</span><br><span class="line">是否云提供商: Yes [7 D]</span><br><span class="line">是否数据中心: Yes [0 5] No [1 6 8 A C]</span><br><span class="line">是否移动设备: No [5 A C] Yes [E]</span><br><span class="line">是否代理: No [0 1 4 5 6 7 8 9 A B C D] Yes [E]</span><br><span class="line">是否VPN: Yes [7 D E] No [0 1 6 A C]</span><br><span class="line">是否TorExit: No [1 7 D]</span><br><span class="line">是否Tor出口: No [1 7 D]</span><br><span class="line">是否网络爬虫: No [9 A B E]</span><br><span class="line">是否匿名: No [1 6 8] Yes [7 D]</span><br><span class="line">是否攻击者: No [7 8 D]</span><br><span class="line">是否滥用者: No [7 8 A C D E]</span><br><span class="line">是否威胁: No [7 8 C D]</span><br><span class="line">是否中继: No [0 7 8 C D]</span><br><span class="line">是否Bogon: No [7 8 A C D]</span><br><span class="line">是否机器人: No [E]</span><br><span class="line">DNS-黑名单: 313(Total_Check) 0(Clean) 0(Blacklisted) 0(Other)</span><br><span class="line">IPV6:</span><br><span class="line">安全得分:</span><br><span class="line">欺诈得分(越低越好): 0 [1]</span><br><span class="line">滥用得分(越低越好): 0 [3]</span><br><span class="line">ASN滥用得分(越低越好): 0.0022 (Low) [A]</span><br><span class="line">公司滥用得分(越低越好): 0 (Very Low) [A]</span><br><span class="line">威胁级别: low [B]</span><br><span class="line">安全信息:</span><br><span class="line">使用类型: isp [A] DataCenter/WebHosting/Transit [3]</span><br><span class="line">公司类型: isp [A]</span><br><span class="line">是否云提供商: Yes [D]</span><br><span class="line">是否数据中心: No [1 A]</span><br><span class="line">是否移动设备: No [A]</span><br><span class="line">是否代理: No [1 A B D]</span><br><span class="line">是否VPN: Yes [D] No [1 A]</span><br><span class="line">是否TorExit: No [1 D]</span><br><span class="line">是否Tor出口: No [1 D]</span><br><span class="line">是否网络爬虫: No [A B]</span><br><span class="line">是否匿名: No [1] Yes [D]</span><br><span class="line">是否攻击者: No [D]</span><br><span class="line">是否滥用者: No [A D]</span><br><span class="line">是否威胁: No [D]</span><br><span class="line">是否中继: No [D]</span><br><span class="line">是否Bogon: No [A D]</span><br><span class="line">DNS-黑名单: 313(Total_Check) 0(Clean) 0(Blacklisted) 313(Other)</span><br><span class="line">Google搜索可行性：NO</span><br><span class="line">-------------邮件端口检测--基于oneclickvirt/portchecker开源-------------</span><br><span class="line">Platform  SMTP  SMTPS POP3  POP3S IMAP  IMAPS</span><br><span class="line">LocalPort ✔     ✔     ✔     ✔     ✔     ✔</span><br><span class="line">QQ        ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">163       ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">Sohu      ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">Yandex    ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">Gmail     ✔     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">Outlook   ✔     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Office365 ✔     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Yahoo     ✔     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">MailCOM   ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">MailRU    ✔     ✔     ✘     ✘     ✔     ✘</span><br><span class="line">AOL       ✔     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">GMX       ✔     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Sina      ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">Apple     ✘     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">FastMail  ✘     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">ProtonMail✘     ✘     ✘     ✘     ✘     ✘</span><br><span class="line">MXRoute   ✔     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Namecrane ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">XYAMail   ✘     ✘     ✘     ✘     ✘     ✘</span><br><span class="line">ZohoMail  ✘     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">Inbox_eu  ✔     ✔     ✔     ✘     ✘     ✘</span><br><span class="line">Free_fr   ✘     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">----------------三网回程--基于oneclickvirt/backtrace开源----------------</span><br><span class="line">北京电信 219.141.140.10  电信CN2GIA [精品线路]</span><br><span class="line">北京联通 202.106.195.68  电信CN2GIA [精品线路] 联通4837   [普通线路]</span><br><span class="line">北京移动 221.179.155.161 电信CN2GIA [精品线路]</span><br><span class="line">上海电信 202.96.209.133  电信CN2GIA [精品线路]</span><br><span class="line">上海联通 210.22.97.1     电信CN2GIA [精品线路] 联通4837   [普通线路]</span><br><span class="line">上海移动 211.136.112.200 电信CN2GIA [精品线路]</span><br><span class="line">广州电信 58.60.188.222   电信CN2GIA [精品线路]</span><br><span class="line">广州联通 210.21.196.6    电信CN2GIA [精品线路] 联通4837   [普通线路]</span><br><span class="line">广州移动 120.196.165.24  电信CN2GT  [优质线路] 电信163    [普通线路]</span><br><span class="line">成都电信 61.139.2.69     电信CN2GIA [精品线路]</span><br><span class="line">成都联通 119.6.6.6       电信CN2GIA [精品线路] 联通4837   [普通线路]</span><br><span class="line">成都移动 211.137.96.205  电信CN2GIA [精品线路]</span><br><span class="line">准确线路自行查看详细路由，本测试结果仅作参考</span><br><span class="line">同一目标地址多个线路时，可能检测已越过汇聚层，除了第一个线路外，后续信息可能无效</span><br><span class="line">---------------------回程路由--感谢fscarmen开源及PR---------------------</span><br><span class="line">依次测试电信/联通/移动经过的地区及线路，核心程序来自nexttrace，请知悉!</span><br><span class="line">广州电信 58.60.188.222</span><br><span class="line">21.93 ms        * RFC1918</span><br><span class="line">5.77 ms         * RFC1918</span><br><span class="line">0.47 ms         AS25820 美国 加利福尼亚州 洛杉矶 it7.net</span><br><span class="line">149.42 ms       * [CN2-BackBone] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">158.95 ms       * [CN2-BackBone] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">157.51 ms       * [CN2-BackBone] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">155.64 ms       AS4134 中国 广东 佛山市 www.chinatelecom.com.cn 电信</span><br><span class="line">153.81 ms       AS4134 中国 广东 深圳 福田区 www.chinatelecom.com.cn 电信</span><br><span class="line">广州联通 210.21.196.6</span><br><span class="line">26.90 ms        * RFC1918</span><br><span class="line">7.69 ms         * RFC1918</span><br><span class="line">0.68 ms         AS25820 美国 加利福尼亚州 洛杉矶 it7.net</span><br><span class="line">154.21 ms       * [CN2-BackBone] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">155.60 ms       * [CN2-Global] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">170.17 ms       * [CN2-BackBone] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">196.85 ms       AS4837 [CU169-BACKBONE] 中国 广东 广州 chinaunicom.cn 联通</span><br><span class="line">298.68 ms       AS4837 [CU169-BACKBONE] 中国 广东 广州 chinaunicom.cn</span><br><span class="line">196.33 ms       AS17816 [APNIC-AP] 中国 广东 深圳 chinaunicom.cn 联通</span><br><span class="line">176.50 ms       AS17623 [APNIC-AP] 中国 广东 深圳 chinaunicom.cn 联通</span><br><span class="line">195.32 ms       AS17623 中国 广东 深圳 宝安区 chinaunicom.cn 联通</span><br><span class="line">广州移动 120.196.165.24</span><br><span class="line">31.58 ms        * RFC1918</span><br><span class="line">7.17 ms         * RFC1918</span><br><span class="line">0.61 ms         AS25820 美国 加利福尼亚州 洛杉矶 it7.net</span><br><span class="line">150.95 ms       * [CN2-BackBone] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">157.08 ms       * [CN2-BackBone] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">153.69 ms       AS4134 [CHINANET-BB] 中国 广东 广州 www.chinatelecom.com.cn 电信</span><br><span class="line">160.06 ms       AS9808 [CMNET] 中国 广东 广州 chinamobileltd.com 移动</span><br><span class="line">161.91 ms       AS9808 [CMNET] 中国 广东 广州 chinamobileltd.com 移动</span><br><span class="line">160.62 ms       AS56040 [APNIC-AP] 中国 广东 深圳 gd.10086.cn 移动</span><br><span class="line">--------------------自动更新测速节点列表--本脚本原创--------------------</span><br><span class="line">位置             上传速度        下载速度        延迟     丢包率</span><br><span class="line">Speedtest.net    4208.35 Mbps    6446.98 Mbps    0.50     0.0%</span><br><span class="line">洛杉矶           135.43 Mbps     2134.82 Mbps    0.95     0.0%</span><br><span class="line">日本东京         127.49 Mbps     457.99 Mbps     105.08   0.0%</span><br><span class="line">联通上海5G       1211.06 Mbps    545.92 Mbps     151.74   0.0%</span><br><span class="line">电信Suzhou5G     1376.62 Mbps    3015.86 Mbps    135.66   NULL</span><br><span class="line">电信浙江         654.16 Mbps     1083.04 Mbps    133.66   NULL</span><br><span class="line">移动Fujian       120.01 Mbps     676.17 Mbps     196.56   NULL</span><br><span class="line">------------------------------------------------------------------------</span><br><span class="line"> 总共花费      : 5 分 52 秒</span><br><span class="line"> 时间          : Mon Mar 10 04:40:16 PDT 2025</span><br><span class="line">------------------------------------------------------------------------</span><br><span class="line">  短链:</span><br><span class="line">    https://paste.spiritlhl.net/#/show/jg6cn.txt</span><br><span class="line">    http://hpaste.spiritlhl.net/#/show/jg6cn.txt</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">本次测评的服务器是BandwagonHost的MINIBOX-10-512，年付&#92;$27.04USD。</summary>
    
    
    
    <category term="VPS测评" scheme="https://blog.032802.xyz/categories/VPS%E6%B5%8B%E8%AF%84/"/>
    
    
    <category term="BandwagonHost" scheme="https://blog.032802.xyz/tags/BandwagonHost/"/>
    
    <category term="VPS" scheme="https://blog.032802.xyz/tags/VPS/"/>
    
    <category term="VPS测评" scheme="https://blog.032802.xyz/tags/VPS%E6%B5%8B%E8%AF%84/"/>
    
    <category term="美国VPS" scheme="https://blog.032802.xyz/tags/%E7%BE%8E%E5%9B%BDVPS/"/>
    
  </entry>
  
  <entry>
    <title>【VPS测评】BandwagonHost - SPECIAL 10G KVM PROMO V5 - LOS ANGELES - CN2 GIA LIMITED EDITION</title>
    <link href="https://blog.032802.xyz/vps-review/bandwagonhost-special-10g-kvm-promo-v5-los-angeles-cn2-gia-limited-edition.html"/>
    <id>https://blog.032802.xyz/vps-review/bandwagonhost-special-10g-kvm-promo-v5-los-angeles-cn2-gia-limited-edition.html</id>
    <published>2025-03-10T11:04:11.000Z</published>
    <updated>2025-03-10T11:04:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>配置信息：</p><ul><li>1 CPU</li><li>512 MB</li><li>10 GB SSD</li><li>500 GB/mo</li><li>1 Gigabit</li></ul><p>购买链接：活动机型，现已停售。</p><h2 id="测试">2025-03-10测试</h2><h3 id="itdog">ITDOG</h3><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuaXRkb2cuY24vcGluZy8=">https://www.itdog.cn/ping/<i class="fa fa-external-link-alt"></i></span>：</p><p>结果图：<span class="exturl" data-url="aHR0cHM6Ly9pbWcuMDMyODAyLnh5ei92cHMtcmV2aWV3L2JhbmR3YWdvbmhvc3Qtc3BlY2lhbC0xMGcta3ZtLXByb21vLXY1LWxvcy1hbmdlbGVzLWNuMi1naWEtbGltaXRlZC1lZGl0aW9uLzIwMjUuMDMuMTAtMTM4LjEyOC53ZWJw">ITDOG：138.128.*.*<i class="fa fa-external-link-alt"></i></span></p><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuaXRkb2cuY24vcGluZ19pcHY2Lw==">https://www.itdog.cn/ping_ipv6/<i class="fa fa-external-link-alt"></i></span>：</p><p>结果图：<span class="exturl" data-url="aHR0cHM6Ly9pbWcuMDMyODAyLnh5ei92cHMtcmV2aWV3L2JhbmR3YWdvbmhvc3Qtc3BlY2lhbC0xMGcta3ZtLXByb21vLXY1LWxvcy1hbmdlbGVzLWNuMi1naWEtbGltaXRlZC1lZGl0aW9uLzIwMjUuMDMuMTAtMjYwN184NzAwXzU1MDAud2VicA==">ITDOG：2607:8700:5500:*:*:*:*<i class="fa fa-external-link-alt"></i></span></p><h3 id="ipquality">IPQuality</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3h5a3QvSVBRdWFsaXR5">xykt/IPQuality<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash &lt;(curl -Ls IP.Check.Place)</span><br></pre></td></tr></table></figure><p>IP质量体检报告：138.128.*.*：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">########################################################################</span><br><span class="line">                   IP质量体检报告(Lite)：138.128.*.*</span><br><span class="line">                    bash &lt;(curl -sL IP.Check.Place)</span><br><span class="line">                   https://github.com/xykt/IPQuality</span><br><span class="line">        报告时间：2025-03-10 19:11:00 CST  脚本版本：v2025-01-24</span><br><span class="line">########################################################################</span><br><span class="line">一、基础信息（IPinfo 数据库）</span><br><span class="line">自治系统号：            ASAS25820</span><br><span class="line">组织：                  IT7 Networks Inc</span><br><span class="line">坐标：                  121°47′15″W, 37°13′58″N</span><br><span class="line">地图：                  https://check.place/37.2329,-121.7875,12,cn</span><br><span class="line">城市：                  San Jose, 95119</span><br><span class="line">使用地：                [US]United States of America, Americas</span><br><span class="line">注册地：                [CA]Canada</span><br><span class="line">IP类型：                 广播IP</span><br><span class="line">二、IP类型属性</span><br><span class="line">数据库：      IPinfo      ipapi    IP2LOCATION</span><br><span class="line">使用类型：     机房        家宽        机房</span><br><span class="line">公司类型：     机房        家宽</span><br><span class="line">三、风险评分</span><br><span class="line">风险等级：      极低         低       中等       高         极高</span><br><span class="line">SCAMALYTICS：  0|低风险</span><br><span class="line">ipapi：     0.10%|低风险</span><br><span class="line">Cloudflare：   0|低风险</span><br><span class="line">DB-IP：         |低风险</span><br><span class="line">四、风险因子</span><br><span class="line">库： IP2LOCATION ipapi SCAMALYTICS IPinfo IPWHOIS</span><br><span class="line">地区：    [US]    [US]    [US]    [US]    [US]</span><br><span class="line">代理：     否      否      否      否      否</span><br><span class="line">Tor：      否      否      否      否      否</span><br><span class="line">VPN：      是      是      否      否      是</span><br><span class="line">服务器：   是      否      否      是      是</span><br><span class="line">滥用：     否      否      无      无      无</span><br><span class="line">机器人：   否      否      否      无      无</span><br><span class="line">五、流媒体及AI服务解锁检测</span><br><span class="line">服务商：  TikTok   Disney+  Netflix Youtube  AmazonPV  Spotify  ChatGPT</span><br><span class="line">状态：     解锁     屏蔽    仅自制    解锁     解锁     屏蔽     解锁</span><br><span class="line">地区：     [US]              [US]     [US]     [US]              [US]</span><br><span class="line">方式：     原生              原生     原生     原生               DNS</span><br><span class="line">六、邮局连通性及黑名单检测</span><br><span class="line">本地25端口：阻断</span><br><span class="line">IP地址黑名单数据库：  有效 439   正常 408   已标记 31   黑名单 0</span><br><span class="line">========================================================================</span><br><span class="line">今日IP检测量：862；总检测量：444907。感谢使用xy系列脚本！</span><br></pre></td></tr></table></figure><p>IP质量体检报告：2607:8700:5500:*:*:*:*：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">########################################################################</span><br><span class="line">             IP质量体检报告(Lite)：2607:8700:5500:*:*:*:*:*</span><br><span class="line">                    bash &lt;(curl -sL IP.Check.Place)</span><br><span class="line">                   https://github.com/xykt/IPQuality</span><br><span class="line">        报告时间：2025-03-10 19:11:00 CST  脚本版本：v2025-01-24</span><br><span class="line">########################################################################</span><br><span class="line">一、基础信息（IPinfo 数据库）</span><br><span class="line">自治系统号：            ASAS25820</span><br><span class="line">组织：                  IT7 Networks Inc</span><br><span class="line">坐标：                  118°15′60″W, 34°3′21″N</span><br><span class="line">地图：                  https://check.place/34.0559,-118.2666,12,cn</span><br><span class="line">城市：                  Los Angeles, 90017</span><br><span class="line">使用地：                [US]United States of America, Americas</span><br><span class="line">注册地：                [CA]Canada</span><br><span class="line">IP类型：                 广播IP</span><br><span class="line">二、IP类型属性</span><br><span class="line">数据库：      IPinfo      ipapi    IP2LOCATION</span><br><span class="line">使用类型：     机房        家宽        机房</span><br><span class="line">公司类型：     机房        家宽</span><br><span class="line">三、风险评分</span><br><span class="line">风险等级：      极低         低       中等       高         极高</span><br><span class="line">SCAMALYTICS：  0|低风险</span><br><span class="line">ipapi：    0.00%|极低风险</span><br><span class="line">Cloudflare：   0|低风险</span><br><span class="line">DB-IP：         |低风险</span><br><span class="line">四、风险因子</span><br><span class="line">库： IP2LOCATION ipapi SCAMALYTICS IPinfo IPWHOIS</span><br><span class="line">地区：    [US]    [US]    [US]    [US]    [US]</span><br><span class="line">代理：     否      否      否      否      否</span><br><span class="line">Tor：      否      否      否      否      否</span><br><span class="line">VPN：      是      否      否      否      否</span><br><span class="line">服务器：   是      否      否      是      否</span><br><span class="line">滥用：     否      否      无      无      无</span><br><span class="line">机器人：   否      否      否      无      无</span><br><span class="line">五、流媒体及AI服务解锁检测</span><br><span class="line">服务商：  TikTok   Disney+  Netflix Youtube  AmazonPV  Spotify  ChatGPT</span><br><span class="line">状态：     失败     屏蔽    仅自制    解锁     屏蔽     屏蔽     失败</span><br><span class="line">地区：                       [CA]     [US]</span><br><span class="line">方式：                       原生     原生</span><br><span class="line">六、邮局连通性及黑名单检测</span><br><span class="line">本地25端口：阻断</span><br><span class="line">========================================================================</span><br><span class="line">今日IP检测量：863；总检测量：444908。感谢使用xy系列脚本！</span><br></pre></td></tr></table></figure><h3 id="融合怪">融合怪</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NwaXJpdExITFMvZWNz">spiritLHLS/ecs<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -L https://gitlab.com/spiritysdx/za/-/raw/main/ecs.sh -o ecs.sh &amp;&amp; <span class="built_in">chmod</span> +x ecs.sh &amp;&amp; bash ecs.sh -m 1</span><br></pre></td></tr></table></figure><p><span class="exturl" data-url="aHR0cHM6Ly9wYXN0ZS5zcGlyaXRsaGwubmV0LyMvc2hvdy9vR2c0SS50eHQ=">https://paste.spiritlhl.net/#/show/oGg4I.txt<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br></pre></td><td class="code"><pre><span class="line">--------------------- A Bench Script By spiritlhl ----------------------</span><br><span class="line">                   测评频道: https://t.me/vps_reviews</span><br><span class="line">VPS融合怪版本：2025.02.12</span><br><span class="line">Shell项目地址：https://github.com/spiritLHLS/ecs</span><br><span class="line">Go项目地址：https://github.com/oneclickvirt/ecs</span><br><span class="line">---------------------基础信息查询--感谢所有开源项目---------------------</span><br><span class="line"> CPU 型号          : Intel Xeon Processor (SierraForest)</span><br><span class="line"> CPU 核心数        : 1</span><br><span class="line"> CPU 频率          : 2599.988 MHz</span><br><span class="line"> CPU 缓存          : L1: 32.00 KB / L2: 4.00 MB / L3: 16.00 MB</span><br><span class="line"> AES-NI指令集      : ✔ Enabled</span><br><span class="line"> VM-x/AMD-V支持    : ❌ Disabled</span><br><span class="line"> 内存              : 108.45 MiB / 521.02 MiB</span><br><span class="line"> Swap              : 268 KiB / 289.00 MiB</span><br><span class="line"> 硬盘空间          : 1.48 GiB / 9.31 GiB</span><br><span class="line"> 启动盘路径        : /dev/sda2</span><br><span class="line"> 系统在线时间      : 0 days, 0 hour 3 min</span><br><span class="line"> 负载              : 0.93, 0.63, 0.26</span><br><span class="line"> 系统              : Debian GNU/Linux 12 (bookworm) (x86_64)</span><br><span class="line"> 架构              : x86_64 (64 Bit)</span><br><span class="line"> 内核              : 6.1.0-9-amd64</span><br><span class="line"> TCP加速方式       : bbr</span><br><span class="line"> 虚拟化架构        : KVM</span><br><span class="line"> NAT类型           : Full Cone</span><br><span class="line"> IPV4 ASN          : AS25820 IT7 Networks Inc</span><br><span class="line"> IPV4 位置         : San Jose / California / US</span><br><span class="line"> IPV6 ASN          : AS25820 IT7 Networks Inc</span><br><span class="line"> IPV6 位置         : Los Angeles / California / United States</span><br><span class="line"> IPV6 子网掩码     : 128</span><br><span class="line">----------------------CPU测试--通过sysbench测试-------------------------</span><br><span class="line"> -&gt; CPU 测试中 (Fast Mode, 1-Pass @ 5sec)</span><br><span class="line"> 1 线程测试(单核)得分:          937 Scores</span><br><span class="line">---------------------内存测试--感谢lemonbench开源-----------------------</span><br><span class="line"> -&gt; 内存测试 Test (Fast Mode, 1-Pass @ 5sec)</span><br><span class="line"> 单线程读测试:          19513.67 MB/s</span><br><span class="line"> 单线程写测试:          17363.22 MB/s</span><br><span class="line">------------------磁盘dd读写测试--感谢lemonbench开源--------------------</span><br><span class="line"> -&gt; 磁盘IO测试中 (4K Block/1M Block, Direct Mode)</span><br><span class="line"> 测试操作               写速度                                  读速度</span><br><span class="line"> 100MB-4K Block         18.4 MB/s (4484 IOPS, 5.71s)            20.5 MB/s (5003 IOPS, 5.12s)</span><br><span class="line"> 1GB-1M Block           746 MB/s (711 IOPS, 1.41s)              888 MB/s (846 IOPS, 1.18s)</span><br><span class="line">---------------------磁盘fio读写测试--感谢yabs开源----------------------</span><br><span class="line">Block Size | 4k            (IOPS) | 64k           (IOPS)</span><br><span class="line">  ------   | ---            ----  | ----           ----</span><br><span class="line">Read       | 65.52 MB/s   (16.3k) | 544.70 MB/s   (8.5k)</span><br><span class="line">Write      | 65.66 MB/s   (16.4k) | 547.56 MB/s   (8.5k)</span><br><span class="line">Total      | 131.18 MB/s  (32.7k) | 1.09 GB/s    (17.0k)</span><br><span class="line">           |                      |</span><br><span class="line">Block Size | 512k          (IOPS) | 1m            (IOPS)</span><br><span class="line">  ------   | ---            ----  | ----           ----</span><br><span class="line">Read       | 755.78 MB/s   (1.4k) | 874.43 MB/s    (853)</span><br><span class="line">Write      | 795.93 MB/s   (1.5k) | 932.67 MB/s    (910)</span><br><span class="line">Total      | 1.55 GB/s     (3.0k) | 1.80 GB/s     (1.7k)</span><br><span class="line">------------流媒体解锁--基于oneclickvirt/CommonMediaTests开源-----------</span><br><span class="line">以下测试的解锁地区是准确的，但是不是完整解锁的判断可能有误，这方面仅作参考使用</span><br><span class="line">----------------Netflix-----------------</span><br><span class="line">[IPV4]</span><br><span class="line">您的出口IP可以使用Netflix，但仅可看Netflix自制剧</span><br><span class="line">NF所识别的IP地域信息：美国</span><br><span class="line">[IPV6]</span><br><span class="line">您的出口IP可以使用Netflix，但仅可看Netflix自制剧</span><br><span class="line">NF所识别的IP地域信息：加拿大</span><br><span class="line">----------------Youtube-----------------</span><br><span class="line">[IPV4]</span><br><span class="line">连接方式: Google Global CacheCDN (ISP Cooperation)</span><br><span class="line">ISP运营商: FCIXUS</span><br><span class="line">视频缓存节点地域: SJC(SJC1)</span><br><span class="line">[IPV6]</span><br><span class="line">连接方式: Youtube Video Server</span><br><span class="line">视频缓存节点地域: 美国  洛杉机(LAX31S13)</span><br><span class="line">---------------DisneyPlus---------------</span><br><span class="line">[IPV4]</span><br><span class="line">当前IPv4出口所在地区即将开通DisneyPlus</span><br><span class="line">[IPV6]</span><br><span class="line">当前IPv4出口所在地区即将开通DisneyPlus</span><br><span class="line">解锁Netflix，Youtube，DisneyPlus上面和下面进行比较，不同之处自行判断</span><br><span class="line">----------------流媒体解锁--感谢RegionRestrictionCheck开源--------------</span><br><span class="line"> 以下为IPV4网络测试，若无IPV4网络则无输出</span><br><span class="line">============[ Multination ]============</span><br><span class="line"> Dazn:                                  Failed (Error: )</span><br><span class="line"> Disney+:                               No (IP Banned By Disney+ 1)</span><br><span class="line"> Netflix:                               Originals Only</span><br><span class="line"> YouTube Premium:                       Yes (Region: US)</span><br><span class="line"> Amazon Prime Video:                    Yes (Region: US)</span><br><span class="line"> TVBAnywhere+:                          Yes</span><br><span class="line"> Spotify Registration:                  No</span><br><span class="line"> OneTrust Region:                       US [California]</span><br><span class="line"> iQyi Oversea Region:                   US</span><br><span class="line"> Bing Region:                           US (Risky)</span><br><span class="line"> Apple Region:                          US</span><br><span class="line"> YouTube CDN:                           [FCIXUS] in [San Jose, CA]</span><br><span class="line"> Netflix Preferred CDN:                 San Jose, CA</span><br><span class="line"> ChatGPT:                               Yes</span><br><span class="line"> Google Gemini:                         Yes (Region: USA)</span><br><span class="line"> Claude:                                Yes</span><br><span class="line"> Wikipedia Editability:                 No</span><br><span class="line"> Google Play Store:                     United States</span><br><span class="line"> Google Search CAPTCHA Free:            Yes</span><br><span class="line"> Steam Currency:                        USD</span><br><span class="line"> ---Forum---</span><br><span class="line"> Reddit:                                No</span><br><span class="line">=======================================</span><br><span class="line"> 以下为IPV6网络测试，若无IPV6网络则无输出</span><br><span class="line">============[ Multination ]============</span><br><span class="line"> Dazn:                                  IPv6 Is Not Currently Supported</span><br><span class="line"> Disney+:                               IPv6 Is Not Currently Supported</span><br><span class="line"> Netflix:                               Originals Only</span><br><span class="line"> YouTube Premium:                       Yes (Region: US)</span><br><span class="line"> Amazon Prime Video:                    IPv6 Is Not Currently Supported</span><br><span class="line"> TVBAnywhere+:                          IPv6 Is Not Currently Supported</span><br><span class="line"> Spotify Registration:                  No</span><br><span class="line"> OneTrust Region:                       US [California]</span><br><span class="line"> iQyi Oversea Region:                   IPv6 Is Not Currently Supported</span><br><span class="line"> Bing Region:                           US (Risky)</span><br><span class="line"> Apple Region:                          US</span><br><span class="line"> YouTube CDN:                           Los Angeles, CA</span><br><span class="line"> Netflix Preferred CDN:                 Los Angeles, CA</span><br><span class="line"> ChatGPT:                               Failed (Network Connection)</span><br><span class="line"> Google Gemini:                         Yes (Region: USA)</span><br><span class="line"> Claude:                                Yes</span><br><span class="line"> Wikipedia Editability:                 No</span><br><span class="line"> Google Play Store:                     United States</span><br><span class="line"> Google Search CAPTCHA Free:            Yes</span><br><span class="line"> Steam Currency:                        IPv6 Is Not Currently Supported</span><br><span class="line"> ---Forum---</span><br><span class="line"> Reddit:                                IPv6 Is Not Currently Supported</span><br><span class="line">=======================================</span><br><span class="line">---------------TikTok解锁--感谢lmc999的源脚本及fscarmen PR--------------</span><br><span class="line"> Tiktok Region:         【US】</span><br><span class="line">-------------IP质量检测--基于oneclickvirt/securityCheck使用-------------</span><br><span class="line">数据仅作参考，不代表100%准确，如果和实际情况不一致请手动查询多个数据库比对</span><br><span class="line">以下为各数据库编号，输出结果后将自带数据库来源对应的编号</span><br><span class="line">ipinfo数据库  [0] | scamalytics数据库 [1] | virustotal数据库   [2] | abuseipdb数据库   [3] | ip2location数据库    [4]</span><br><span class="line">ip-api数据库  [5] | ipwhois数据库     [6] | ipregistry数据库   [7] | ipdata数据库      [8] | db-ip数据库          [9]</span><br><span class="line">ipapiis数据库 [A] | ipapicom数据库    [B] | bigdatacloud数据库 [C] | cheervision数据库 [D] | ipqualityscore数据库 [E]</span><br><span class="line">IPV4:</span><br><span class="line">安全得分:</span><br><span class="line">声誉(越高越好): 0 [2]</span><br><span class="line">信任得分(越高越好): 0 [8]</span><br><span class="line">VPN得分(越低越好): 100 [8]</span><br><span class="line">代理得分(越低越好): 100 [8]</span><br><span class="line">社区投票-无害: 0 [2]</span><br><span class="line">社区投票-恶意: 0 [2]</span><br><span class="line">威胁得分(越低越好): 100 [8]</span><br><span class="line">欺诈得分(越低越好): 0 [1] 65 [E]</span><br><span class="line">滥用得分(越低越好): 0 [3]</span><br><span class="line">ASN滥用得分(越低越好): 0.0022 (Low) [A]</span><br><span class="line">公司滥用得分(越低越好): 0.001 (Low) [A]</span><br><span class="line">威胁级别: low [9 B]</span><br><span class="line">黑名单记录统计:(有多少黑名单网站有记录):</span><br><span class="line">无害记录数: 0 [2]  恶意记录数: 0 [2]  可疑记录数: 0 [2]  无记录数: 94 [2]</span><br><span class="line">安全信息:</span><br><span class="line">使用类型: isp [A] DataCenter/WebHosting/Transit [3] business [8] hosting [0 7 9] unknown [C]</span><br><span class="line">公司类型: isp [A] hosting [0] business [7]</span><br><span class="line">是否云提供商: Yes [7 D]</span><br><span class="line">是否数据中心: Yes [0 5 6] No [1 8 A C]</span><br><span class="line">是否移动设备: No [5 A C] Yes [E]</span><br><span class="line">是否代理: Yes [5 E] No [0 1 4 6 7 8 9 A B C D]</span><br><span class="line">是否VPN: No [0 1 C] Yes [6 7 A D E]</span><br><span class="line">是否Tor: No [0 1 3 6 7 8 A B C D E]</span><br><span class="line">是否Tor出口: No [1 7 D]</span><br><span class="line">是否网络爬虫: No [9 A B E]</span><br><span class="line">是否匿名: No [1 8] Yes [6 7 D]</span><br><span class="line">是否攻击者: No [7 8 D]</span><br><span class="line">是否滥用者: No [7 8 A C D E]</span><br><span class="line">是否威胁: No [7 8 C D]</span><br><span class="line">是否中继: No [0 7 8 C D]</span><br><span class="line">是否Bogon: No [7 8 A C D]</span><br><span class="line">是否机器人: No [E]</span><br><span class="line">DNS-黑名单: 313(Total_Check) 0(Clean) 6(Blacklisted) 17(Other)</span><br><span class="line">IPV6:</span><br><span class="line">安全得分:</span><br><span class="line">欺诈得分(越低越好): 0 [1]</span><br><span class="line">滥用得分(越低越好): 0 [3]</span><br><span class="line">ASN滥用得分(越低越好): 0.0022 (Low) [A]</span><br><span class="line">公司滥用得分(越低越好): 0 (Very Low) [A]</span><br><span class="line">威胁级别: low [B]</span><br><span class="line">安全信息:</span><br><span class="line">使用类型: isp [A] DataCenter/WebHosting/Transit [3]</span><br><span class="line">公司类型: isp [A]</span><br><span class="line">是否云提供商: Yes [D]</span><br><span class="line">是否数据中心: No [1 A]</span><br><span class="line">是否移动设备: No [A]</span><br><span class="line">是否代理: No [1 A B D]</span><br><span class="line">是否VPN: Yes [D] No [1 A]</span><br><span class="line">是否TorExit: No [1 D]</span><br><span class="line">是否Tor出口: No [1 D]</span><br><span class="line">是否网络爬虫: No [A B]</span><br><span class="line">是否匿名: No [1] Yes [D]</span><br><span class="line">是否攻击者: No [D]</span><br><span class="line">是否滥用者: No [A D]</span><br><span class="line">是否威胁: No [D]</span><br><span class="line">是否中继: No [D]</span><br><span class="line">是否Bogon: No [A D]</span><br><span class="line">DNS-黑名单: 313(Total_Check) 0(Clean) 0(Blacklisted) 313(Other)</span><br><span class="line">Google搜索可行性：NO</span><br><span class="line">-------------邮件端口检测--基于oneclickvirt/portchecker开源-------------</span><br><span class="line">Platform  SMTP  SMTPS POP3  POP3S IMAP  IMAPS</span><br><span class="line">LocalPort ✔     ✔     ✔     ✔     ✔     ✔</span><br><span class="line">QQ        ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">163       ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">Sohu      ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">Yandex    ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">Gmail     ✔     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">Outlook   ✔     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Office365 ✔     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Yahoo     ✔     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">MailCOM   ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">MailRU    ✔     ✔     ✘     ✘     ✔     ✘</span><br><span class="line">AOL       ✔     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">GMX       ✔     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Sina      ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">Apple     ✘     ✘     ✘     ✘     ✘     ✘</span><br><span class="line">FastMail  ✘     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">ProtonMail✘     ✘     ✘     ✘     ✘     ✘</span><br><span class="line">MXRoute   ✔     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Namecrane ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">XYAMail   ✘     ✘     ✘     ✘     ✘     ✘</span><br><span class="line">ZohoMail  ✘     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">Inbox_eu  ✔     ✔     ✔     ✘     ✘     ✘</span><br><span class="line">Free_fr   ✘     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">----------------三网回程--基于oneclickvirt/backtrace开源----------------</span><br><span class="line">北京电信 219.141.140.10  电信CN2GIA [精品线路]</span><br><span class="line">北京联通 202.106.195.68  电信CN2GIA [精品线路]</span><br><span class="line">北京移动 221.179.155.161 电信CN2GIA [精品线路]</span><br><span class="line">上海电信 202.96.209.133  电信CN2GIA [精品线路]</span><br><span class="line">上海联通 210.22.97.1     电信CN2GIA [精品线路] 联通4837   [普通线路]</span><br><span class="line">上海移动 211.136.112.200 电信CN2GIA [精品线路]</span><br><span class="line">广州电信 58.60.188.222   电信CN2GIA [精品线路]</span><br><span class="line">广州联通 210.21.196.6    电信CN2GIA [精品线路] 联通4837   [普通线路]</span><br><span class="line">广州移动 120.196.165.24  电信CN2GIA [精品线路]</span><br><span class="line">成都电信 61.139.2.69     电信CN2GIA [精品线路]</span><br><span class="line">成都联通 119.6.6.6       电信CN2GIA [精品线路] 联通4837   [普通线路]</span><br><span class="line">成都移动 211.137.96.205  电信CN2GIA [精品线路]</span><br><span class="line">准确线路自行查看详细路由，本测试结果仅作参考</span><br><span class="line">同一目标地址多个线路时，可能检测已越过汇聚层，除了第一个线路外，后续信息可能无效</span><br><span class="line">---------------------回程路由--感谢fscarmen开源及PR---------------------</span><br><span class="line">依次测试电信/联通/移动经过的地区及线路，核心程序来自nexttrace，请知悉!</span><br><span class="line">广州电信 58.60.188.222</span><br><span class="line">13.85 ms        * RFC1918</span><br><span class="line">19.49 ms        * RFC1918</span><br><span class="line">1.19 ms         AS4134 [CHINANET-US] 美国 加利福尼亚 洛杉矶 www.chinatelecom.com.cn 电信</span><br><span class="line">159.20 ms       * [CN2-BackBone] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">163.82 ms       * [CN2-BackBone] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">165.34 ms       * [CN2-BackBone] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">166.10 ms       AS4134 中国 广东 佛山市 www.chinatelecom.com.cn 电信</span><br><span class="line">163.80 ms       AS4134 中国 广东 深圳 福田区 www.chinatelecom.com.cn 电信</span><br><span class="line">广州联通 210.21.196.6</span><br><span class="line">14.88 ms        * RFC1918</span><br><span class="line">33.74 ms        * RFC1918</span><br><span class="line">1.18 ms         AS4134 [CHINANET-US] 美国 加利福尼亚 洛杉矶 www.chinatelecom.com.cn 电信</span><br><span class="line">159.30 ms       * [CN2-BackBone] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">160.94 ms       * [CN2-Global] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">161.67 ms       * [CN2-BackBone] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">198.82 ms       AS4837 [CU169-BACKBONE] 中国 广东 广州 chinaunicom.cn 联通</span><br><span class="line">208.51 ms       AS17816 [APNIC-AP] 中国 广东 深圳 chinaunicom.cn 联通</span><br><span class="line">209.47 ms       AS17623 [APNIC-AP] 中国 广东 深圳 chinaunicom.cn 联通</span><br><span class="line">181.70 ms       AS17623 中国 广东 深圳 宝安区 chinaunicom.cn 联通</span><br><span class="line">广州移动 120.196.165.24</span><br><span class="line">17.27 ms        * RFC1918</span><br><span class="line">13.46 ms        * RFC1918</span><br><span class="line">1.18 ms         AS4134 [CHINANET-US] 美国 加利福尼亚 洛杉矶 www.chinatelecom.com.cn 电信</span><br><span class="line">159.24 ms       * [CN2-BackBone] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">262.15 ms       * [CN2-BackBone] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">162.69 ms       AS4134 [CHINANET-BB] 中国 广东 广州 www.chinatelecom.com.cn 电信</span><br><span class="line">251.42 ms       AS9808 [CMNET] 中国 北京 北京 chinamobileltd.com</span><br><span class="line">194.37 ms       AS9808 [CMNET] 中国 广东 广州 chinamobileltd.com 移动</span><br><span class="line">195.84 ms       AS9808 [CMNET] 中国 广东 广州 chinamobileltd.com 移动</span><br><span class="line">194.03 ms       AS56040 [APNIC-AP] 中国 广东 深圳 gd.10086.cn 移动</span><br><span class="line">--------------------自动更新测速节点列表--本脚本原创--------------------</span><br><span class="line">位置             上传速度        下载速度        延迟     丢包率</span><br><span class="line">Speedtest.net    6074.96 Mbps    6313.38 Mbps    1.55     0.0%</span><br><span class="line">洛杉矶           2557.37 Mbps    3989.14 Mbps    9.42     0.0%</span><br><span class="line">日本东京         720.59 Mbps     2686.68 Mbps    113.46   0.0%</span><br><span class="line">联通上海5G       1151.82 Mbps    3436.89 Mbps    176.92   0.0%</span><br><span class="line">电信Zhenjiang5G  667.51 Mbps     3503.46 Mbps    141.84   NULL</span><br><span class="line">电信浙江         621.76 Mbps     1967.82 Mbps    149.15   NULL</span><br><span class="line">移动Fujian       381.28 Mbps     1678.87 Mbps    225.85   NULL</span><br><span class="line">------------------------------------------------------------------------</span><br><span class="line"> 总共花费      : 6 分 34 秒</span><br><span class="line"> 时间          : Mon Mar 10 04:19:16 PDT 2025</span><br><span class="line">------------------------------------------------------------------------</span><br><span class="line">  短链:</span><br><span class="line">    https://paste.spiritlhl.net/#/show/oGg4I.txt</span><br><span class="line">    http://hpaste.spiritlhl.net/#/show/oGg4I.txt</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">本次测评的服务器是BandwagonHost的SPECIAL 10G KVM PROMO V5 - LOS ANGELES - CN2 GIA LIMITED EDITION，年付&#92;$46.60USD。</summary>
    
    
    
    <category term="VPS测评" scheme="https://blog.032802.xyz/categories/VPS%E6%B5%8B%E8%AF%84/"/>
    
    
    <category term="BandwagonHost" scheme="https://blog.032802.xyz/tags/BandwagonHost/"/>
    
    <category term="VPS" scheme="https://blog.032802.xyz/tags/VPS/"/>
    
    <category term="VPS测评" scheme="https://blog.032802.xyz/tags/VPS%E6%B5%8B%E8%AF%84/"/>
    
    <category term="美国VPS" scheme="https://blog.032802.xyz/tags/%E7%BE%8E%E5%9B%BDVPS/"/>
    
  </entry>
  
  <entry>
    <title>【VPS测评】BandwagonHost - The DC6 Plan</title>
    <link href="https://blog.032802.xyz/vps-review/bandwagonhost-the-dc6-plan.html"/>
    <id>https://blog.032802.xyz/vps-review/bandwagonhost-the-dc6-plan.html</id>
    <published>2025-03-10T10:58:07.000Z</published>
    <updated>2025-03-10T10:58:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>配置信息：</p><ul><li>1 CPU</li><li>1 GB</li><li>20 GB SSD</li><li>1000 GB/mo</li><li>2.5 Gigabit</li><li>Los Angeles</li></ul><p>购买链接：活动机型，现已停售。</p><h2 id="测试">2025-03-10测试</h2><h3 id="itdog">ITDOG</h3><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuaXRkb2cuY24vcGluZy8=">https://www.itdog.cn/ping/<i class="fa fa-external-link-alt"></i></span>：</p><p>结果图：<span class="exturl" data-url="aHR0cHM6Ly9pbWcuMDMyODAyLnh5ei92cHMtcmV2aWV3L2JhbmR3YWdvbmhvc3QtdGhlLWRjNi1wbGFuLzIwMjUuMDMuMTAtNzQuMjExLndlYnA=">ITDOG：74.211.*.*<i class="fa fa-external-link-alt"></i></span></p><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuaXRkb2cuY24vcGluZ19pcHY2Lw==">https://www.itdog.cn/ping_ipv6/<i class="fa fa-external-link-alt"></i></span>：</p><p>结果图：<span class="exturl" data-url="aHR0cHM6Ly9pbWcuMDMyODAyLnh5ei92cHMtcmV2aWV3L2JhbmR3YWdvbmhvc3QtdGhlLWRjNi1wbGFuLzIwMjUuMDMuMTAtMjYwN184NzAwXzU1MDAud2VicA==">ITDOG：2607:8700:5500:*:*:*:*<i class="fa fa-external-link-alt"></i></span></p><h3 id="ipquality">IPQuality</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3h5a3QvSVBRdWFsaXR5">xykt/IPQuality<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash &lt;(curl -Ls IP.Check.Place)</span><br></pre></td></tr></table></figure><p>IP质量体检报告：74.211.*.*：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">########################################################################</span><br><span class="line">                    IP质量体检报告(Lite)：74.211.*.*</span><br><span class="line">                    bash &lt;(curl -sL IP.Check.Place)</span><br><span class="line">                   https://github.com/xykt/IPQuality</span><br><span class="line">        报告时间：2025-03-10 18:56:49 CST  脚本版本：v2025-01-24</span><br><span class="line">########################################################################</span><br><span class="line">一、基础信息（IPinfo 数据库）</span><br><span class="line">自治系统号：            ASAS25820</span><br><span class="line">组织：                  IT7 Networks Inc</span><br><span class="line">坐标：                  118°15′60″W, 34°3′21″N</span><br><span class="line">地图：                  https://check.place/34.0559,-118.2666,12,cn</span><br><span class="line">城市：                  Los Angeles, 90017</span><br><span class="line">使用地：                [US]United States of America, Americas</span><br><span class="line">注册地：                [CA]Canada</span><br><span class="line">IP类型：                 广播IP</span><br><span class="line">二、IP类型属性</span><br><span class="line">数据库：      IPinfo      ipapi    IP2LOCATION</span><br><span class="line">使用类型：     机房        家宽        机房</span><br><span class="line">公司类型：     机房        家宽</span><br><span class="line">三、风险评分</span><br><span class="line">风险等级：      极低         低       中等       高         极高</span><br><span class="line">SCAMALYTICS：  0|低风险</span><br><span class="line">ipapi：       0.18%|低风险</span><br><span class="line">Cloudflare：   0|低风险</span><br><span class="line">DB-IP：         |低风险</span><br><span class="line">四、风险因子</span><br><span class="line">库： IP2LOCATION ipapi SCAMALYTICS IPinfo IPWHOIS</span><br><span class="line">地区：    [US]    [US]    [US]    [US]    [US]</span><br><span class="line">代理：     否      否      否      否      否</span><br><span class="line">Tor：      否      否      否      否      否</span><br><span class="line">VPN：      是      是      否      否      是</span><br><span class="line">服务器：   是      是      否      是      是</span><br><span class="line">滥用：     否      否      无      无      无</span><br><span class="line">机器人：   否      否      否      无      无</span><br><span class="line">五、流媒体及AI服务解锁检测</span><br><span class="line">服务商：  TikTok   Disney+  Netflix Youtube  AmazonPV  Spotify  ChatGPT</span><br><span class="line">状态：     解锁     屏蔽    仅自制    解锁     解锁     屏蔽     解锁</span><br><span class="line">地区：     [US]              [US]     [US]     [US]              [US]</span><br><span class="line">方式：     原生              原生     原生     原生               DNS</span><br><span class="line">六、邮局连通性及黑名单检测</span><br><span class="line">本地25端口：阻断</span><br><span class="line">IP地址黑名单数据库：  有效 439   正常 409   已标记 30   黑名单 0</span><br><span class="line">========================================================================</span><br><span class="line">今日IP检测量：841；总检测量：444886。感谢使用xy系列脚本！</span><br></pre></td></tr></table></figure><p>IP质量体检报告：2607:8700:5500:*:*:*:*：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">########################################################################</span><br><span class="line">             IP质量体检报告(Lite)：2607:8700:5500:*:*:*:*:*</span><br><span class="line">                    bash &lt;(curl -sL IP.Check.Place)</span><br><span class="line">                   https://github.com/xykt/IPQuality</span><br><span class="line">        报告时间：2025-03-10 18:56:49 CST  脚本版本：v2025-01-24</span><br><span class="line">########################################################################</span><br><span class="line">一、基础信息（IPinfo 数据库）</span><br><span class="line">自治系统号：            ASAS25820</span><br><span class="line">组织：                  IT7 Networks Inc</span><br><span class="line">坐标：                  118°15′60″W, 34°3′21″N</span><br><span class="line">地图：                  https://check.place/34.0559,-118.2666,12,cn</span><br><span class="line">城市：                  Los Angeles, 90017</span><br><span class="line">使用地：                [US]United States of America, Americas</span><br><span class="line">注册地：                [CA]Canada</span><br><span class="line">IP类型：                 广播IP</span><br><span class="line">二、IP类型属性</span><br><span class="line">数据库：      IPinfo      ipapi    IP2LOCATION</span><br><span class="line">使用类型：     机房        家宽        机房</span><br><span class="line">公司类型：     机房        家宽</span><br><span class="line">三、风险评分</span><br><span class="line">风险等级：      极低         低       中等       高         极高</span><br><span class="line">SCAMALYTICS：  0|低风险</span><br><span class="line">ipapi：    0.00%|极低风险</span><br><span class="line">Cloudflare：   0|低风险</span><br><span class="line">DB-IP：         |低风险</span><br><span class="line">四、风险因子</span><br><span class="line">库： IP2LOCATION ipapi SCAMALYTICS IPinfo IPWHOIS</span><br><span class="line">地区：    [US]    [US]    [US]    [US]    [US]</span><br><span class="line">代理：     否      否      否      否      否</span><br><span class="line">Tor：      否      否      否      否      否</span><br><span class="line">VPN：      是      否      否      否      否</span><br><span class="line">服务器：   是      否      否      是      否</span><br><span class="line">滥用：     否      否      无      无      无</span><br><span class="line">机器人：   否      否      否      无      无</span><br><span class="line">五、流媒体及AI服务解锁检测</span><br><span class="line">服务商：  TikTok   Disney+  Netflix Youtube  AmazonPV  Spotify  ChatGPT</span><br><span class="line">状态：     失败     屏蔽    仅自制    解锁     屏蔽     屏蔽     失败</span><br><span class="line">地区：                       [CA]     [US]</span><br><span class="line">方式：                       原生     原生</span><br><span class="line">六、邮局连通性及黑名单检测</span><br><span class="line">本地25端口：阻断</span><br><span class="line">========================================================================</span><br><span class="line">今日IP检测量：842；总检测量：444887。感谢使用xy系列脚本！</span><br></pre></td></tr></table></figure><h3 id="融合怪">融合怪</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NwaXJpdExITFMvZWNz">spiritLHLS/ecs<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -L https://gitlab.com/spiritysdx/za/-/raw/main/ecs.sh -o ecs.sh &amp;&amp; <span class="built_in">chmod</span> +x ecs.sh &amp;&amp; bash ecs.sh -m 1</span><br></pre></td></tr></table></figure><p><span class="exturl" data-url="aHR0cHM6Ly9wYXN0ZS5zcGlyaXRsaGwubmV0LyMvc2hvdy9QT1NhMy50eHQ=">https://paste.spiritlhl.net/#/show/POSa3.txt<i class="fa fa-external-link-alt"></i></span>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br></pre></td><td class="code"><pre><span class="line">--------------------- A Bench Script By spiritlhl ----------------------</span><br><span class="line">                   测评频道: https://t.me/vps_reviews</span><br><span class="line">VPS融合怪版本：2025.02.12</span><br><span class="line">Shell项目地址：https://github.com/spiritLHLS/ecs</span><br><span class="line">Go项目地址：https://github.com/oneclickvirt/ecs</span><br><span class="line">---------------------基础信息查询--感谢所有开源项目---------------------</span><br><span class="line"> CPU 型号          : Intel Xeon Processor (SierraForest)</span><br><span class="line"> CPU 核心数        : 1</span><br><span class="line"> CPU 频率          : 2699.988 MHz</span><br><span class="line"> CPU 缓存          : L1: 32.00 KB / L2: 4.00 MB / L3: 16.00 MB</span><br><span class="line"> AES-NI指令集      : ✔ Enabled</span><br><span class="line"> VM-x/AMD-V支持    : ❌ Disabled</span><br><span class="line"> 内存              : 147.90 MiB / 1023.52 MiB</span><br><span class="line"> Swap              : 0 KiB / 545.00 MiB</span><br><span class="line"> 硬盘空间          : 1.73 GiB / 19.15 GiB</span><br><span class="line"> 启动盘路径        : /dev/sda2</span><br><span class="line"> 系统在线时间      : 0 days, 0 hour 9 min</span><br><span class="line"> 负载              : 2.12, 1.16, 0.56</span><br><span class="line"> 系统              : Debian GNU/Linux 12 (bookworm) (x86_64)</span><br><span class="line"> 架构              : x86_64 (64 Bit)</span><br><span class="line"> 内核              : 6.1.0-9-amd64</span><br><span class="line"> TCP加速方式       : bbr</span><br><span class="line"> 虚拟化架构        : KVM</span><br><span class="line"> NAT类型           : Full Cone</span><br><span class="line"> IPV4 ASN          : AS25820 IT7 Networks Inc</span><br><span class="line"> IPV4 位置         : Los Angeles / California / US</span><br><span class="line"> IPV6 ASN          : AS25820 IT7 Networks Inc</span><br><span class="line"> IPV6 位置         : Los Angeles / California / United States</span><br><span class="line"> IPV6 子网掩码     : 128</span><br><span class="line">----------------------CPU测试--通过sysbench测试-------------------------</span><br><span class="line"> -&gt; CPU 测试中 (Fast Mode, 1-Pass @ 5sec)</span><br><span class="line"> 1 线程测试(单核)得分:          788 Scores</span><br><span class="line">---------------------内存测试--感谢lemonbench开源-----------------------</span><br><span class="line"> -&gt; 内存测试 Test (Fast Mode, 1-Pass @ 5sec)</span><br><span class="line"> 单线程读测试:          16377.29 MB/s</span><br><span class="line"> 单线程写测试:          14617.24 MB/s</span><br><span class="line">------------------磁盘dd读写测试--感谢lemonbench开源--------------------</span><br><span class="line"> -&gt; 磁盘IO测试中 (4K Block/1M Block, Direct Mode)</span><br><span class="line"> 测试操作               写速度                                  读速度</span><br><span class="line"> 100MB-4K Block         17.1 MB/s (4182 IOPS, 6.12s)            20.9 MB/s (5092 IOPS, 5.03s)</span><br><span class="line"> 1GB-1M Block           913 MB/s (871 IOPS, 1.15s)              1.4 GB/s (1320 IOPS, 0.76s)</span><br><span class="line">---------------------磁盘fio读写测试--感谢yabs开源----------------------</span><br><span class="line">Block Size | 4k            (IOPS) | 64k           (IOPS)</span><br><span class="line">  ------   | ---            ----  | ----           ----</span><br><span class="line">Read       | 73.29 MB/s   (18.3k) | 956.40 MB/s  (14.9k)</span><br><span class="line">Write      | 73.49 MB/s   (18.3k) | 961.43 MB/s  (15.0k)</span><br><span class="line">Total      | 146.79 MB/s  (36.6k) | 1.91 GB/s    (29.9k)</span><br><span class="line">           |                      |</span><br><span class="line">Block Size | 512k          (IOPS) | 1m            (IOPS)</span><br><span class="line">  ------   | ---            ----  | ----           ----</span><br><span class="line">Read       | 1.19 GB/s     (2.3k) | 1.35 GB/s     (1.3k)</span><br><span class="line">Write      | 1.25 GB/s     (2.4k) | 1.44 GB/s     (1.4k)</span><br><span class="line">Total      | 2.45 GB/s     (4.7k) | 2.79 GB/s     (2.7k)</span><br><span class="line">------------流媒体解锁--基于oneclickvirt/CommonMediaTests开源-----------</span><br><span class="line">以下测试的解锁地区是准确的，但是不是完整解锁的判断可能有误，这方面仅作参考使用</span><br><span class="line">----------------Netflix-----------------</span><br><span class="line">[IPV4]</span><br><span class="line">您的出口IP可以使用Netflix，但仅可看Netflix自制剧</span><br><span class="line">NF所识别的IP地域信息：美国</span><br><span class="line">[IPV6]</span><br><span class="line">您的出口IP可以使用Netflix，但仅可看Netflix自制剧</span><br><span class="line">NF所识别的IP地域信息：加拿大</span><br><span class="line">----------------Youtube-----------------</span><br><span class="line">[IPV4]</span><br><span class="line">连接方式: Youtube Video Server</span><br><span class="line">视频缓存节点地域: 美国  洛杉机(LAX31S13)</span><br><span class="line">[IPV6]</span><br><span class="line">连接方式: Youtube Video Server</span><br><span class="line">视频缓存节点地域: 美国  洛杉机(LAX31S13)</span><br><span class="line">---------------DisneyPlus---------------</span><br><span class="line">[IPV4]</span><br><span class="line">当前IPv4出口所在地区即将开通DisneyPlus</span><br><span class="line">[IPV6]</span><br><span class="line">当前IPv4出口所在地区即将开通DisneyPlus</span><br><span class="line">解锁Netflix，Youtube，DisneyPlus上面和下面进行比较，不同之处自行判断</span><br><span class="line">----------------流媒体解锁--感谢RegionRestrictionCheck开源--------------</span><br><span class="line"> 以下为IPV4网络测试，若无IPV4网络则无输出</span><br><span class="line">============[ Multination ]============</span><br><span class="line"> Dazn:                                  Failed (Error: )</span><br><span class="line"> Disney+:                               No (IP Banned By Disney+ 1)</span><br><span class="line"> Netflix:                               Originals Only</span><br><span class="line"> YouTube Premium:                       Yes (Region: US)</span><br><span class="line"> Amazon Prime Video:                    Yes (Region: US)</span><br><span class="line"> TVBAnywhere+:                          Yes</span><br><span class="line"> Spotify Registration:                  No</span><br><span class="line"> OneTrust Region:                       US [California]</span><br><span class="line"> iQyi Oversea Region:                   US</span><br><span class="line"> Bing Region:                           US (Risky)</span><br><span class="line"> Apple Region:                          US</span><br><span class="line"> YouTube CDN:                           Los Angeles, CA</span><br><span class="line"> Netflix Preferred CDN:                 Los Angeles, CA</span><br><span class="line"> ChatGPT:                               Yes</span><br><span class="line"> Google Gemini:                         Yes (Region: USA)</span><br><span class="line"> Claude:                                Yes</span><br><span class="line"> Wikipedia Editability:                 No</span><br><span class="line"> Google Play Store:                     United States</span><br><span class="line"> Google Search CAPTCHA Free:            Yes</span><br><span class="line"> Steam Currency:                        USD</span><br><span class="line"> ---Forum---</span><br><span class="line"> Reddit:                                No</span><br><span class="line">=======================================</span><br><span class="line"> 以下为IPV6网络测试，若无IPV6网络则无输出</span><br><span class="line">============[ Multination ]============</span><br><span class="line"> Dazn:                                  IPv6 Is Not Currently Supported</span><br><span class="line"> Disney+:                               IPv6 Is Not Currently Supported</span><br><span class="line"> Netflix:                               Originals Only</span><br><span class="line"> YouTube Premium:                       Yes (Region: US)</span><br><span class="line"> Amazon Prime Video:                    IPv6 Is Not Currently Supported</span><br><span class="line"> TVBAnywhere+:                          IPv6 Is Not Currently Supported</span><br><span class="line"> Spotify Registration:                  No</span><br><span class="line"> OneTrust Region:                       US [California]</span><br><span class="line"> iQyi Oversea Region:                   IPv6 Is Not Currently Supported</span><br><span class="line"> Bing Region:                           US (Risky)</span><br><span class="line"> Apple Region:                          US</span><br><span class="line"> YouTube CDN:                           Los Angeles, CA</span><br><span class="line"> Netflix Preferred CDN:                 Los Angeles, CA</span><br><span class="line"> ChatGPT:                               Failed (Network Connection)</span><br><span class="line"> Google Gemini:                         Yes (Region: USA)</span><br><span class="line"> Claude:                                Yes</span><br><span class="line"> Wikipedia Editability:                 No</span><br><span class="line"> Google Play Store:                     United States</span><br><span class="line"> Google Search CAPTCHA Free:            Yes</span><br><span class="line"> Steam Currency:                        IPv6 Is Not Currently Supported</span><br><span class="line"> ---Forum---</span><br><span class="line"> Reddit:                                IPv6 Is Not Currently Supported</span><br><span class="line">=======================================</span><br><span class="line">---------------TikTok解锁--感谢lmc999的源脚本及fscarmen PR--------------</span><br><span class="line"> Tiktok Region:         【US】</span><br><span class="line">-------------IP质量检测--基于oneclickvirt/securityCheck使用-------------</span><br><span class="line">数据仅作参考，不代表100%准确，如果和实际情况不一致请手动查询多个数据库比对</span><br><span class="line">以下为各数据库编号，输出结果后将自带数据库来源对应的编号</span><br><span class="line">ipinfo数据库  [0] | scamalytics数据库 [1] | virustotal数据库   [2] | abuseipdb数据库   [3] | ip2location数据库    [4]</span><br><span class="line">ip-api数据库  [5] | ipwhois数据库     [6] | ipregistry数据库   [7] | ipdata数据库      [8] | db-ip数据库          [9]</span><br><span class="line">ipapiis数据库 [A] | ipapicom数据库    [B] | bigdatacloud数据库 [C] | cheervision数据库 [D] | ipqualityscore数据库 [E]</span><br><span class="line">IPV4:</span><br><span class="line">安全得分:</span><br><span class="line">声誉(越高越好): 0 [2]</span><br><span class="line">信任得分(越高越好): 0 [8]</span><br><span class="line">VPN得分(越低越好): 100 [8]</span><br><span class="line">代理得分(越低越好): 100 [8]</span><br><span class="line">社区投票-无害: 0 [2]</span><br><span class="line">社区投票-恶意: 0 [2]</span><br><span class="line">威胁得分(越低越好): 100 [8]</span><br><span class="line">欺诈得分(越低越好): 0 [1] 80 [E]</span><br><span class="line">滥用得分(越低越好): 0 [3]</span><br><span class="line">ASN滥用得分(越低越好): 0.0022 (Low) [A]</span><br><span class="line">公司滥用得分(越低越好): 0.0018 (Low) [A]</span><br><span class="line">威胁级别: low [9 B]</span><br><span class="line">黑名单记录统计:(有多少黑名单网站有记录):</span><br><span class="line">无害记录数: 0 [2] 恶意记录数: 0 [2]  可疑记录数: 0 [2]  无记录数: 94 [2]</span><br><span class="line">安全信息:</span><br><span class="line">使用类型: hosting - moderate probability [C] business [8] isp [A] DataCenter/WebHosting/Transit [3] corporate [9] hosting [0 7]</span><br><span class="line">公司类型: business [7 A] hosting [0]</span><br><span class="line">是否云提供商: Yes [7 D]</span><br><span class="line">是否数据中心: No [1 8 C] Yes [0 5 6 A]</span><br><span class="line">是否移动设备: Yes [E] No [5 A C]</span><br><span class="line">是否代理: Yes [5 E] No [0 1 4 6 7 8 9 A B C D]</span><br><span class="line">是否VPN: Yes [6 7 A D E] No [0 1 C]</span><br><span class="line">是否TorExit: No [1 7 D]</span><br><span class="line">是否Tor出口: No [1 7 D]</span><br><span class="line">是否网络爬虫: No [9 A B E]</span><br><span class="line">是否匿名: Yes [6 7 D] No [1 8]</span><br><span class="line">是否攻击者: No [7 8 D]</span><br><span class="line">是否滥用者: Yes [E] No [7 8 A C D]</span><br><span class="line">是否威胁: No [7 8 C D]</span><br><span class="line">是否中继: No [0 7 8 C D]</span><br><span class="line">是否Bogon: No [7 8 A C D]</span><br><span class="line">是否机器人: No [E]</span><br><span class="line">DNS-黑名单: 313(Total_Check) 0(Clean) 6(Blacklisted) 13(Other)</span><br><span class="line">IPV6:</span><br><span class="line">安全得分:</span><br><span class="line">欺诈得分(越低越好): 0 [1]</span><br><span class="line">滥用得分(越低越好): 0 [3]</span><br><span class="line">ASN滥用得分(越低越好): 0.0022 (Low) [A]</span><br><span class="line">公司滥用得分(越低越好): 0 (Very Low) [A]</span><br><span class="line">威胁级别: low [B]</span><br><span class="line">安全信息:</span><br><span class="line">使用类型: isp [A] DataCenter/WebHosting/Transit [3]</span><br><span class="line">公司类型: isp [A]</span><br><span class="line">是否云提供商: Yes [D]</span><br><span class="line">是否数据中心: No [1 A]</span><br><span class="line">是否移动设备: No [A]</span><br><span class="line">是否代理: No [1 A B D]</span><br><span class="line">是否VPN: Yes [D] No [1 A]</span><br><span class="line">是否Tor: No [1 3 A B D]</span><br><span class="line">是否Tor出口: No [1 D]</span><br><span class="line">是否网络爬虫: No [A B]</span><br><span class="line">是否匿名: Yes [D] No [1]</span><br><span class="line">是否攻击者: No [D]</span><br><span class="line">是否滥用者: No [A D]</span><br><span class="line">是否威胁: No [D]</span><br><span class="line">是否中继: No [D]</span><br><span class="line">是否Bogon: No [A D]</span><br><span class="line">DNS-黑名单: 313(Total_Check) 0(Clean) 0(Blacklisted) 313(Other)</span><br><span class="line">Google搜索可行性：NO</span><br><span class="line">-------------邮件端口检测--基于oneclickvirt/portchecker开源-------------</span><br><span class="line">Platform  SMTP  SMTPS POP3  POP3S IMAP  IMAPS</span><br><span class="line">LocalPort ✔     ✔     ✔     ✔     ✔     ✔</span><br><span class="line">QQ        ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">163       ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">Sohu      ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">Yandex    ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">Gmail     ✔     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">Outlook   ✔     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Office365 ✔     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Yahoo     ✔     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">MailCOM   ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">MailRU    ✔     ✔     ✘     ✘     ✔     ✘</span><br><span class="line">AOL       ✔     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">GMX       ✔     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Sina      ✔     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Apple     ✘     ✘     ✘     ✘     ✘     ✘</span><br><span class="line">FastMail  ✘     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">ProtonMail✘     ✘     ✘     ✘     ✘     ✘</span><br><span class="line">MXRoute   ✔     ✘     ✔     ✘     ✔     ✘</span><br><span class="line">Namecrane ✔     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">XYAMail   ✘     ✘     ✘     ✘     ✘     ✘</span><br><span class="line">ZohoMail  ✘     ✔     ✘     ✘     ✘     ✘</span><br><span class="line">Inbox_eu  ✔     ✔     ✔     ✘     ✘     ✘</span><br><span class="line">Free_fr   ✘     ✔     ✔     ✘     ✔     ✘</span><br><span class="line">----------------三网回程--基于oneclickvirt/backtrace开源----------------</span><br><span class="line">北京电信 219.141.140.10  电信CN2GIA [精品线路]</span><br><span class="line">北京联通 202.106.195.68  电信CN2GIA [精品线路] 联通4837   [普通线路]</span><br><span class="line">北京移动 221.179.155.161 移动CMIN2  [精品线路]</span><br><span class="line">上海电信 202.96.209.133  电信CN2GIA [精品线路]</span><br><span class="line">上海联通 210.22.97.1     电信CN2GIA [精品线路] 联通4837   [普通线路]</span><br><span class="line">上海移动 211.136.112.200 移动CMIN2  [精品线路]</span><br><span class="line">广州电信 58.60.188.222   电信CN2GIA [精品线路]</span><br><span class="line">广州联通 210.21.196.6    电信CN2GIA [精品线路] 联通4837   [普通线路]</span><br><span class="line">广州移动 120.196.165.24  移动CMIN2  [精品线路]</span><br><span class="line">成都电信 61.139.2.69     电信CN2GIA [精品线路]</span><br><span class="line">成都联通 119.6.6.6       电信CN2GIA [精品线路] 联通4837   [普通线路]</span><br><span class="line">成都移动 211.137.96.205  移动CMIN2  [精品线路]</span><br><span class="line">准确线路自行查看详细路由，本测试结果仅作参考</span><br><span class="line">同一目标地址多个线路时，可能检测已越过汇聚层，除了第一个线路外，后续信息可能无效</span><br><span class="line">---------------------回程路由--感谢fscarmen开源及PR---------------------</span><br><span class="line">依次测试电信/联通/移动经过的地区及线路，核心程序来自nexttrace，请知悉!</span><br><span class="line">广州电信 58.60.188.222</span><br><span class="line">13.62 ms        * RFC1918</span><br><span class="line">20.53 ms        * RFC1918</span><br><span class="line">0.58 ms         AS25820 美国 加利福尼亚州 洛杉矶 it7.net</span><br><span class="line">153.62 ms       * [CN2-Global] 中国 香港 chinatelecom.cn 电信</span><br><span class="line">207.52 ms       * [CN2-BackBone] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">160.66 ms       * [CN2-BackBone] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">209.87 ms       AS4134 中国 广东 佛山市 www.chinatelecom.com.cn 电信</span><br><span class="line">158.38 ms       AS4134 中国 广东 深圳 福田区 www.chinatelecom.com.cn 电信</span><br><span class="line">广州联通 210.21.196.6</span><br><span class="line">12.56 ms        * RFC1918</span><br><span class="line">0.84 ms         AS25820 美国 加利福尼亚州 洛杉矶 it7.net</span><br><span class="line">146.55 ms       * [CN2-BackBone] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">201.67 ms       * [CN2-Global] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">155.22 ms       * [CN2-BackBone] 中国 广东 广州 chinatelecom.cn 电信</span><br><span class="line">158.62 ms       AS4837 [CU169-BACKBONE] 中国 广东 广州 chinaunicom.cn 联通</span><br><span class="line">152.33 ms       AS17816 [UNICOM-GD] 中国 广东 深圳 chinaunicom.cn 联通</span><br><span class="line">160.97 ms       AS17623 [APNIC-AP] 中国 广东 深圳 chinaunicom.cn 联通</span><br><span class="line">158.70 ms       AS17623 中国 广东 深圳 宝安区 chinaunicom.cn 联通</span><br><span class="line">广州移动 120.196.165.24</span><br><span class="line">19.28 ms        * RFC1918</span><br><span class="line">0.74 ms         AS25820 美国 加利福尼亚州 洛杉矶 it7.net</span><br><span class="line">126.57 ms       AS58807 [CMIN2-NET] 美国 加利福尼亚 洛杉矶 cmi.chinamobile.com 移动</span><br><span class="line">126.55 ms       AS58807 [CMIN2-NET] 中国 上海 cmi.chinamobile.com 移动</span><br><span class="line">126.29 ms       AS9808 [CMNET] 中国 上海 chinamobileltd.com 移动</span><br><span class="line">126.58 ms       AS9808 [CMNET] 中国 上海 chinamobileltd.com 移动</span><br><span class="line">127.67 ms       AS9808 [CMNET] 中国 上海 chinamobileltd.com</span><br><span class="line">149.63 ms       AS9808 [CMNET] 中国 北京 chinamobileltd.com 移动</span><br><span class="line">172.02 ms       AS9808 [CMNET] 中国 北京 chinamobileltd.com 移动</span><br><span class="line">225.41 ms       AS9808 [CMNET] 中国 北京 chinamobileltd.com 移动</span><br><span class="line">172.82 ms       AS56040 [APNIC-AP] 中国 广东 深圳 gd.10086.cn 移动</span><br><span class="line">--------------------自动更新测速节点列表--本脚本原创--------------------</span><br><span class="line">位置             上传速度        下载速度        延迟     丢包率</span><br><span class="line">Speedtest.net    6545.87 Mbps    6616.63 Mbps    0.57     0.0%</span><br><span class="line">洛杉矶           1079.10 Mbps    5412.86 Mbps    1.06     0.0%</span><br><span class="line">日本东京         768.72 Mbps     2382.79 Mbps    106.23   0.0%</span><br><span class="line">联通上海5G       2002.72 Mbps    6033.49 Mbps    163.25   0.0%</span><br><span class="line">电信Suzhou5G     1448.74 Mbps    6371.01 Mbps    133.96   NULL</span><br><span class="line">电信Zhenjiang5G  698.06 Mbps     2156.15 Mbps    134.82   NULL</span><br><span class="line">移动Fujian       296.63 Mbps     1493.31 Mbps    220.98   NULL</span><br><span class="line">移动Chengdu      447.03 Mbps     857.25 Mbps     228.25   NULL</span><br><span class="line">------------------------------------------------------------------------</span><br><span class="line"> 总共花费      : 6 分 43 秒</span><br><span class="line"> 时间          : Mon Mar 10 04:07:42 PDT 2025</span><br><span class="line">------------------------------------------------------------------------</span><br><span class="line">  短链:</span><br><span class="line">    https://paste.spiritlhl.net/#/show/POSa3.txt</span><br><span class="line">    http://hpaste.spiritlhl.net/#/show/POSa3.txt</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">本次测评的服务器是BandwagonHost的The DC6 Plan，年付&#92;$49.41USD。</summary>
    
    
    
    <category term="VPS测评" scheme="https://blog.032802.xyz/categories/VPS%E6%B5%8B%E8%AF%84/"/>
    
    
    <category term="BandwagonHost" scheme="https://blog.032802.xyz/tags/BandwagonHost/"/>
    
    <category term="VPS" scheme="https://blog.032802.xyz/tags/VPS/"/>
    
    <category term="VPS测评" scheme="https://blog.032802.xyz/tags/VPS%E6%B5%8B%E8%AF%84/"/>
    
    <category term="美国VPS" scheme="https://blog.032802.xyz/tags/%E7%BE%8E%E5%9B%BDVPS/"/>
    
  </entry>
  
</feed>
